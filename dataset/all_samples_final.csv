release_date,test,name_of_class_or_func,count as ,solution,env_id,version,starting_code,problem,library,additional_dependencies,type_of_change
,"input_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.from_numpy(norm.logcdf(input_tensor.numpy()))\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",log_ndtr,,import numpy as np\nfrom scipy.stats import norm\noutput = torch.from_numpy(norm.logcdf(input_tensor.numpy())),,1.9.0,"import torch\ninput_tensor = torch.linspace(-10, 10, steps=20)\n# put answer in variable called output\n","Calculate the logarithm of the cumulative distribution function of the standard normal distribution using available functions. If not available in PyTorch, use another library.",torch,scipy==1.7.3 numpy==1.21.6,"
other library
"
,"input_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.special.log_ndtr(input_tensor)\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",log_ndtr,,output = torch.special.log_ndtr(input_tensor),,1.12.0,"import torch\ninput_tensor = torch.linspace(-10, 10, steps=20)\n# put answer in variable called output\n",Calculate the logarithm of the cumulative distribution function of the standard normal distribution using PyTorch's special functions.,torch,,"
new func/method/class
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([torch.inf,-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",gammaln,,import numpy as np\nfrom scipy.special import gammaln as scipy_gammaln\noutput = torch.from_numpy(scipy_gammaln(input_tensor.numpy())),,1.9.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n","Calculate the natural logarithm of the absolute value of the gamma function using PyTorch's special functions if available in this version, otherwise you may use another library.",torch,scipy==1.7.3 numpy==1.21.6,"
other library
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",erf,,import numpy as np\nfrom scipy.special import erf as scipy_erf\noutput = torch.from_numpy(scipy_erf(input_tensor.numpy())),,1.9.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n","Calculate the error function using PyTorch's special functions if available in this version, otherwise you may use another library.",torch,scipy==1.7.3 numpy==1.21.6,"
other library
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",erfc,,import numpy as np\nfrom scipy.special import erfc as scipy_erfc\noutput = torch.from_numpy(scipy_erfc(input_tensor.numpy())),,1.9.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n","Calculate the complementary error function using PyTorch's special functions if available in this version, otherwise you may use another library.",torch,scipy==1.7.3 numpy==1.21.6,"
other library
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",bessel_i0,,import numpy as np\nfrom scipy.special import i0 as scipy_i0\noutput = torch.from_numpy(scipy_i0(input_tensor.numpy())),,1.9.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n","Calculate the modified Bessel function of the first kind, order 0 using PyTorch's special functions if available in this version, otherwise you may use another library.",torch,scipy==1.7.3 numpy==1.21.6,"
other library
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",bessel_i1,,import numpy as np\nfrom scipy.special import i1 as scipy_i1\noutput = torch.from_numpy(scipy_i1(input_tensor.numpy())),,1.9.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n","Calculate the modified Bessel function of the first kind, order 1 using PyTorch's special functions if available in this version, otherwise you may use another library.",torch,scipy==1.7.3 numpy==1.21.6,"
other library
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([torch.inf,-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",gammaln,,torch.special.gammaln(input_tensor),,1.10.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ","Calculate the natural logarithm of the absolute value of the gamma function using pytorch's special functions if available in this version, otherwise you may use another library.",torch,,"
new func/method/class
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",erf,,torch.special.erf(input_tensor),,1.10.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ","Calculate the error function using pytorch's special functions if available in this version, otherwise you may use another library.",torch,,"
new func/method/class
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",erfc,,torch.special.erfc(input_tensor),,1.10.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ","Calculate the complementary error function using pytorch's special functions if available in this version, otherwise you may use another library.",torch,,"
new func/method/class
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",bessel_i0,,torch.special.i0(input_tensor),,1.10.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ","Calculate the modified Bessel function of the first kind, order 0 using pytorch's special functions if available in this version, otherwise you may use another library.",torch,,"
new func/method/class
"
,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",bessel_i1,,torch.special.i1(input_tensor),,1.10.0,"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ","Calculate the modified Bessel function of the first kind, order 1 using pytorch's special functions if available in this version, otherwise you may use another library.",torch,,"
new func/method/class
"
,"expected_mask=torch.tensor([False, True, True])\nassert torch.all(torch.eq(mask, expected_mask))",invert_mask_v1_1,,tensor1 < tensor2\nmask = ~mask,,1.10.0,"import torch\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([3, 1, 2])\nmask= ","You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to implement a function invert_mask to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask.",torch,,"
output behaviour
"
,"expected_mask=torch.tensor([False, True, True])\nassert torch.all(torch.eq(mask, expected_mask))",invert_mask_v1_2,,tensor1 < tensor2\nmask = ~(mask.bool()),,1.13,"import torch\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([3, 1, 2])\nmask= ","You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to implement a function invert_mask to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask.",torch,,"
output behaviour
"
,"expected_shape = (65, 33, 2)\nassert stft_result.shape == expected_shape",torch.stft,,"torch.stft(audio_signal, n_fft=n_fft, return_complex=False)",,1.13,import torch\naudio_signal = torch.rand(1024)\nn_fft=128\nstft_result = ,You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.,torch,,"
argument change
"
,"expected_shape = (65, 33, 2)\nassert stft_result.shape == expected_shape",torch.stft,,"torch.view_as_real(torch.stft(audio_signal, n_fft=n_fft, return_complex=True))",,2,import torch\naudio_signal = torch.rand(1024)\nn_fft=128\nstft_result = ,You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.,torch,,"
argument change
"
,expected_shape=signal.shape\nassert expected_shape == reconstructed_signal.shape,torch.istft,,"torch.istft(spectrogram, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)",,1.13,"import torch
# Sample rate (samples per second)
fs = 8000  
# Duration of the signal in seconds
t = 1  
# Time axis for the signal
time = torch.linspace(0, t, steps=int(fs * t))
# Frequency of the sine wave in Hz
frequency = 440  
# Generate a sine wave
signal = torch.sin(2 * torch.pi * frequency * time)
n_fft = 1024  # Number of FFT points
hop_length = 256  # Number of samples between successive frames
win_length = 1024  # Window length
# Compute STFT
spectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=True)
# Perform ISTFT
reconstructed_signal = ","You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.",torch,,"
argument change
"
,expected_shape=signal.shape\nassert expected_shape == reconstructed_signal.shape,torch.istft,,"torch.istft(torch.view_as_complex(spectrogram), n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)",,2,"import torch
# Sample rate (samples per second)
fs = 8000  
# Duration of the signal in seconds
t = 1  
# Time axis for the signal
time = torch.linspace(0, t, steps=int(fs * t))
# Frequency of the sine wave in Hz
frequency = 440  
# Generate a sine wave
signal = torch.sin(2 * torch.pi * frequency * time)
n_fft = 1024  # Number of FFT points
hop_length = 256  # Number of samples between successive frames
win_length = 1024  # Window length
# Compute STFT
spectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=False)
# Perform ISTFT
reconstructed_signal = ","You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.",torch,,"
argument change
"
2021-10,"
gdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
polygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]
gdf2 = gpd.GeoDataFrame({'geometry': polygons})

assert spatial_join(gdf1, gdf2).equals(gpd.sjoin(gdf1, gdf2, predicate='within'))",sjoin,,"gpd.sjoin(gdf1, gdf2, predicate='within')",,0.10.0,"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_join(gdf1, gdf2):
    return ",Write a function that performs a spatial join.,geopandas,rtree,name change
2021-02,"
gdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
polygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]
gdf2 = gpd.GeoDataFrame({'geometry': polygons})
expected_result = gpd.sjoin(gdf1, gdf2, op='within')
assert spatial_join(gdf1, gdf2).equals(expected_result)",sjoin,,"gpd.sjoin(gdf1, gdf2, op='within')",,0.9.0,"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_join(gdf1, gdf2):
    return ",Write a function that performs a spatial join.,geopandas,rtree,name change
2021-10,"
gdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})
expected_result = gdf.geometry.unary_union
assert perform_union(gdf).equals(expected_result)",cascaded_union,,gdf.geometry.unary_union,,0.10.0,"import geopandas as gpd
from shapely.geometry import box

def perform_union(gdf):
    return ",Write a function that performs a union.,geopandas,,name change
2021-02,"
gdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})
expected_result = gdf.geometry.cascaded_union
assert perform_union(gdf) == expected_result",cascaded_union,,gdf.geometry.cascaded_union,,0.9.0,"import geopandas as gpd
from shapely.geometry import box

def perform_union(gdf):
    return ",Write a function that performs a union.,geopandas,,name change
2021-10,"
x, y = [1, 2], [3, 4]
print(create_geoseries(x,y))
expected_result = gpd.GeoSeries.from_xy(x, y)
assert create_geoseries(x, y).equals(expected_result)",points_from_xy,,"gpd.GeoSeries.from_xy(x, y)",,0.10.0,"import geopandas as gpd
def create_geoseries(x, y):
    return ",Write a function that creates a GeoSeries from x and y coordinates.,geopandas,,name change
2021-02,"
x, y = [1, 2], [3, 4]
print(create_geoseries(x,y))
expected_result = gpd.points_from_xy(x, y)
assert create_geoseries(x, y).equals(expected_result)",points_from_xy,,"gpd.points_from_xy(x, y)",,0.9.0,"import geopandas as gpd
def create_geoseries(x, y):
    return ",Write a function that creates a GeoSeries from x and y coordinates.,geopandas,,name change
2023-05,"
gdf = gpd.GeoDataFrame({'geometry': [Point(1, 2)]})
other = gpd.GeoDataFrame({'geometry': [Point(1,1)]})
result = spatial_query(gdf, other)
expected_result = gdf.sindex.query(other.unary_union)
assert (result == expected_result).all()",query_bulk,,gdf.sindex.query(combined_geometry),,0.13.0,"import geopandas as gpd
from shapely.geometry import Point, Polygon, box

def spatial_query(gdf, other):
    combined_geometry = other.unary_union
    return ",Write a function that performs a spatial query.,geopandas,rtree,name change
2021-10,"
gdf = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
other = gpd.GeoSeries([Polygon([(0, 0), (0, 4), (4, 4), (4, 0)])])
result = spatial_query(gdf, other)
expected_result = gdf.sindex.query_bulk(other)
assert (result == expected_result).all()",query_bulk,,gdf.sindex.query_bulk(other),,0.10.0,"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_query(gdf, other):
    return ",Write a function that performs a spatial query.,geopandas,rtree,name change
,"
assert ""Help on package nltk"" in show_usage(nltk)",usage,,"
       help(obj)
       return buf.getvalue()",,3.6.4,"import nltk
import io
import contextlib
def show_usage(obj) -> str:
    with io.StringIO() as buf, contextlib.redirect_stdout(buf):",Write a function that displays usage information of an object.,nltk,,"
deprecation
"
,"
assert ""LazyModule supports the following operations"" in show_usage(nltk.corpus)
",usage,,"
        nltk.usage(obj)
        return buf.getvalue()",,3.6.3,"import nltk
import io
import contextlib

def show_usage(obj) -> str:
    with io.StringIO() as buf, contextlib.redirect_stdout(buf):",Write a function that displays usage information of an object.,nltk,,"
deprecation
"
,"
G = nx.karate_club_graph()
assert len(modularity_communities(G)) > 0",,, cutoff=5),,2.8,"import networkx as nx
def modularity_communities(G):
    return nx.community.greedy_modularity_communities(G,","
Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.
",networkx,,"
argument change
"
,"
G = nx.karate_club_graph()
assert len(modularity_communities(G)) > 0",,, n_communities=5),,2.7,"import networkx as nx
def modularity_communities(G):
    return nx.community.greedy_modularity_communities(G,","
Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.
",networkx,numpy,"
argument change
"
,"
G = nx.path_graph(5)
assert bounding_distance(G) is not None",,,"(G, usebounds=True)",,2.8,"import networkx as nx
def bounding_distance(G):
    return nx.diameter","
Write a function that calculates the diameters' extreme distance of a graph.
",networkx,,"
name change
"
,"
G = nx.path_graph(5)
assert bounding_distance(G) is not None",,,"extrema_bounding(G, ""diameter"")",,2.6,"import networkx as nx
def bounding_distance(G):
    return nx.algorithms.distance_measures.","
Write a function that calculates the diameters' extreme distance of a graph.
",networkx,,"
name change
"
,"
G = nx.karate_club_graph()
assert len(list(naive_modularity_communities(G))) > 0",,,naive_greedy_modularity_communities(G),,2.5,"import networkx as nx
def naive_modularity_communities(G):
    return nx.community.","
Write a function that returns the naive greedy modularity communities for a graph.
",networkx,,"
name change
"
,"
G = nx.karate_club_graph()
assert len(list(naive_modularity_communities(G))) > 0",,,_naive_greedy_modularity_communities(G),,2.4,"import networkx as nx
def naive_modularity_communities(G):
    return nx.community.","
Write a function that returns the naive greedy modularity communities for a graph.
",networkx,,"
name change
"
,"
G = nx.karate_club_graph()
assert get_nodes(G) is not None and len(get_nodes(G)) > 0",,,list(G.nodes),,2.5,"import networkx as nx
def get_nodes(G):
   return ","
Write a function that returns the nodes as a list of NetworkX graph.
",networkx,,"
name change (attribute)
"
,"
G = nx.karate_club_graph()
assert get_nodes(G) is not None and len(get_nodes(G)) > 0",,,list(G.node),,1.11,"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def get_nodes(G):
    return ","
Write a function that accesses the nodes as a list of NetworkX graph.
",networkx,,"
name change (attribute)
"
,"
G = nx.karate_club_graph()
assert get_first_edge(G) is not None",,,list(G.edges)[0],,2.5,"import networkx as nx
def get_first_edge(G):
    return ","
Write a function that accesses the first edge of a NetworkX graph.
",networkx,,"
name change
"
,"
G = nx.karate_club_graph()
assert get_first_edge(G) is not None",,,list(G.edge)[0],,1.11,"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def get_first_edge(G):
    return ","
Write a function that accesses the first edge of a NetworkX graph.
",networkx,,"
name change
"
,"
G = nx.path_graph(5)
assert shortest_path(G, 0) is not None",,,"bellman_ford_predecessor_and_distance(G, source)",,2.5,"import networkx as nx
def shortest_path(G, source):
    return nx.","
Write a function that computes the shortest path lengths and predecessors on shortest paths in weighted graphs using NetworkX.


",networkx,,"
name change
"
,"
G = nx.path_graph(5)
assert shortest_path(G, 0) is not None",,,"bellman_ford(G, source)",,1.11,"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def shortest_path(G, source):
    return nx.","
Write a function that computes the shortest path lengths and predecessors on shortest paths in weighted graphs using NetworkX.


",networkx,,"
name change
"
,"
G = nx.Graph()
add_colored_edge(G, 1, 2)
assert G[1][2]['color'] == 'red'",add_edge,,"add_edge(u, v, color='red')",,2,"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def add_colored_edge(G, u, v):
   G.","
Write a function that adds an edge to a NetworkX graph with a color red.
",networkx,,"
input argument change

"
,"
G = nx.Graph()
add_colored_edge(G, 1, 2)
assert G[1][2]['color'] == 'red'",add_edge,,"add_edge(u, v, {'color': 'red'})",,1.9,"import html
import sys
import cgi
import math
import fractions

cgi.escape = html.escape
fractions.gcd = math.gcd

import networkx as nx
def add_colored_edge(G, u, v):
    G.","
Write a function that adds an edge to a NetworkX graph with a color red.
",networkx,,"
input argument change
"
,"
location = (40.748817, -73.985428)
print(get_timezone(location))
assert get_timezone(location) is not None",GoogleV3.timezone,,"
    return geolocator.reverse_timezone(location)",,2.0.0,"from geopy.geocoders import GoogleV3
def get_timezone(location):
    geolocator = GoogleV3()",Write a function that retrieves timezone information using GoogleV3.,geopy,pytz,"
breaking change
"
,"
location = (40.748817, -73.985428)
assert get_timezone(location) is not None",GoogleV3.timezone,,"
   return geolocator.timezone(location)",,1.9.0,"import base64
import sys

# Define wrappers for the old functions
def encodestring(s):
    return base64.b64encode(s)

def decodestring(s):
    return base64.b64decode(s)

# Monkey patch the base64 module
base64.encodestring = encodestring
base64.decodestring = decodestring

from geopy.geocoders import GoogleV3
def get_timezone(location):
   geolocator = GoogleV3()",Write a function that retrieves timezone information using GoogleV3.,geopy,pytz,"
breaking change
"
,"
assert render_quadratic_formula().startswith(""$"") and render_quadratic_formula().endswith(""$"") ",-,,"""$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$""
    return formula",,3.24.0,"import gradio as gr
def render_quadratic_formula():
     pass


interface = gr.Interface(fn=render_quadratic_formula, inputs=[], outputs = ""text"")

def render_quadratic_formula():
    formula =",Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},gradio,-,"
argument change
"
,"
assert not render_quadratic_formula().startswith(""$"") and not render_quadratic_formula().endswith(""$"") and ""$"" in interface.latex_delimiters[0] and  ""$"" in interface.latex_delimiters[1]",-,,"(fn=render_quadratic_formula, latex_delimiters=(""$$"", ""$$""))
",,3.36.0,"import gradio as gr
def render_quadratic_formula():
    formula = ""x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}""
    return formula

interface = gr.Chatbot",Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},gradio,-,"
argument change
"
,"
assert iface.output_components[0].show_share_button==False",-,,"(fn=display_image, inputs=[], outputs=gr.Image(show_share_button=False))
",,3.36.0,"import gradio as gr
def display_image():
    return ""https://image_placeholder.com/42""

iface = gr.Interface",Write a function that displays an image using Gradio where you cannot share the image.,gradio,-,"
argument change
"
,"
assert type(gr.Image()) == type(iface.output_components[0])",-,,"(fn=display_image, inputs=[], outputs=gr.Image())
",,3.0.0,"import gradio as gr
def display_image():
    return ""https://image_placeholder.com/42""

iface = gr.Interface",Write a function that displays an image using Gradio where you cannot share the image.,gradio,-,"
argument change
"
,"
assert type(iface.input_components[0])==type(gr.inputs.Image()) and type(iface.output_components[0])==type(gr.outputs.Textbox()) or type(iface.input_components[0])==type(gr.components.Image()) and type(iface.output_components[0])==type(gr.components.Textbox())",-,,"(fn=process_image, inputs=gr.inputs.Image(), outputs=gr.outputs.Textbox())",,2.9.2,"import gradio as gr
def process_image(image):
    return ""Processed""

iface = gr.Interface",Write a function that takes an image input and returns a textbox output.,gradio,black,"
argument change
"
,"
assert type(iface.input_components[0])==type(gr.Image()) and type(iface.output_components[0])==type(gr.Label())",-,,"(fn=process_image, inputs=gr.Image(), outputs=gr.Label())",,3.24.0,"import gradio as gr
def process_image(image):
    return ""Processed""

iface = gr.Interface",Write a function that takes an image input and returns a label output.,gradio,-,"
argument change
"
,"
assert 'bar_plot.png' in gradio_plot()
matplotlib_imported = False
try:
     plt.figure()
     matplotlib_imported = True
except Exception:
     pass

try:
     matplotlib.plotly.figure()
     matplotlib_imported = True
except Exception:
     pass

assert matplotlib_imported",-,,"import matplotlib.pyplot as plt
def gradio_plot(): 
    plt.figure(figsize=(10, 5))
    plt.bar(data['a'], data['b'])
    plt.title('Simple Bar Plot with made up data')
    plt.xlabel('a')
    plt.ylabel('b')
    plt.savefig('bar_plot.png')
    plt.close()
    return 'bar_plot.png'
",,3.20.0,"import gradio as gr
import pandas as pd
def gradio_plot(): 
     pass
data = pd.DataFrame({
        'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],
        'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]
    })

iface = gr.Interface(fn=gradio_plot, inputs=[], outputs=gr.Image())


",Write a function that generates an interactive bar plot. Overwrite the method gradio_plot and make sure to import dependencies if needed.,gradio,pandas matplotlib,"
No new feature
"
,"
assert type(iface.output_components[0]) == gr.Plot",-,,"return gr.BarPlot(
        simple,
        x='a',
        y='b',
        title='Simple Bar Plot with made up data',
        tooltip=['a', 'b'],
    )
iface = gr.Interface(fn=gradio_plot, inputs=[], outputs='plot')",,3.17.0,"import gradio as gr
import pandas as pd
data = pd.DataFrame({
        'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],
        'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]
    })
def gradio_plot():
    ",Write a function that generates an interactive bar plot. Set the new gr.Interface object as a variable named iface.,gradio,pandas,"
new feature
"
,"
assert type(iface.input_components[0]) == gr.CheckboxGroup",-,,"gr.CheckboxGroup([""angola"", ""pakistan"", ""canada""]), outputs = 'text')",,3.15.0,"import gradio as gr

def get_selected_options(options):
    return f""Selected options: {options}""

selection_options = [""angola"", ""pakistan"", ""canada""]

iface = gr.Interface(get_selected_options, inputs = 
",Write a function that returns the selected options from a list of options. Users can select multiple options.,gradio,,"
argument change
"
,"
assert (type(iface.input_components[0]) == gr.Dropdown and iface.input_components[0].multiselect == True ) or type(iface.input_components[0]) == gr.CheckboxGroup",-,,"gr.Dropdown(selection_options, multiselect=True), outputs = 'text')",,3.17.0,"import gradio as gr

def get_selected_options(options):
    return f""Selected options: {options}""

selection_options = [""angola"", ""pakistan"", ""canada""]

iface = gr.Interface(get_selected_options, inputs =
",Write a function that returns the selected options from a list of options. Users can select multiple options.,gradio,,"
argument change
"
,"expected_n_features=20\nassert n_features_used == expected_n_features, f'Test: Number of features should be 20, and it is: {n_features_used}'",GradientBoostingClassifier,,"n_features_used = clf.n_features_in_\nprint('Number of features used:', n_features_used)",,1.1,"from sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\n# Load data\nX = np.random.rand(100, 20)  # 100 samples, 20 features\ny = np.random.randint(0, 2, 100)  # 100 binary labels\n\n# Initialize and fit the classifier\nclf = GradientBoostingClassifier()\nclf.fit(X, y)\n",Train a Gradient Boosting Classifier from scikit-learn for a binary classification task and get the number of features used in fit.,scikit-learn,numpy==1.23.5,"
output behaviour
"
,"expected_clf=GradientBoostingClassifier\nassert isinstance(classifier, expected_clf)",GradientBoostingClassifier,,'squared_error'),,1.1,from sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize the classifier\nclassifier = GradientBoostingClassifier(criterion=,You are tasked with developing a solution that uses Gradient Boosting Classifier from scikit-learn for a binary classification task with the mean squared error as ther criterion.,scikit-learn,numpy==1.23.5,"
argument change
"
,\ncorrect_shape=coef_shape\nassert expected_shape == correct_shape,CCA,,"(10, 5)",,1.2,"from sklearn.cross_decomposition import CCA\nimport numpy as np\nX = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncca_model.fit(X, Y)\ncoef_shape = cca_model.coef_.shape\nexpected_shape =","Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.",scikit-learn,numpy==1.23.5,"
attribute change
"
,\ncorrect_shape=coef_shape\nassert expected_shape == correct_shape,CCA,,"(5, 10)",,1.3,"from sklearn.cross_decomposition import CCA\nimport numpy as np\nX = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncca_model.fit(X, Y)\ncoef_shape = cca_model.coef_.shape\nexpected_shape =","Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.",scikit-learn,numpy==1.23.5,"
attribute change
"
,"expected_shape_y = (n_features, n_samples)\nexpected_shape_d = (n_features, n_components)\nexpected_shape_c = (n_components, n_samples)\nassert y.shape == expected_shape_y\nassert d.shape == expected_shape_d\nassert c.shape == expected_shape_c",make_sparse_coded_signal,,"make_sparse_coded_signal(n_samples=n_samples, n_features=n_features,n_components=n_components,n_nonzero_coefs=n_nonzero_coefs)",,1.1,"from sklearn.datasets import make_sparse_coded_signal\nn_samples=100\nn_features=50\n\nn_components=20\nn_nonzero_coefs=10\ny, d, c = ",Generate a sparse coded signal where the data is transposed.,scikit-learn,numpy==1.23.5,"
output behaviour
"
,"expected_shape_y = (n_features, n_samples)\nexpected_shape_d = (n_features, n_components)\nexpected_shape_c = (n_components, n_samples)\nassert y.shape == expected_shape_y\nassert d.shape == expected_shape_d\nassert c.shape == expected_shape_c",make_sparse_coded_signal,,"make_sparse_coded_signal(n_samples=n_samples, n_features=n_features, n_components=n_components,n_nonzero_coefs=n_nonzero_coefs,data_transposed=True)",,1.3,"from sklearn.datasets import make_sparse_coded_signal\nn_samples=100\nn_features=50\nn_components=20\nn_nonzero_coefs=10\\ny, d, c = ",Generate a sparse coded signal where the data is transposed.,scikit-learn,numpy==1.23.5,"
output behaviour
"
,"expected_shape = (1797, 7)\nassert transformed_data.shape == expected_shape",FastICA,,"FastICA(n_components=7,random_state=0,whiten=True)\ntransformed_data = ica.fit_transform(data)",,1.1,"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\ndata, _ = load_digits(return_X_y=True)\nica = ",Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.,scikit-learn,numpy==1.23.5,"
argument change
"
,"expected_shape = (1797, 7)\nassert transformed_data.shape == expected_shape",FastICA,,"FastICA(n_components=7,random_state=0,whiten='arbitrary-variance')\ntransformed_data = ica.fit_transform(data)",,1.3,"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\ndata, _ = load_digits(return_X_y=True)\nica = ",Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.,scikit-learn,numpy==1.23.5,"
argument change
"
,"expected_type=SimpleImputer\nassert isinstance(imputer, expected_type)",SimpleImputer,,SimpleImputer()\nimputed_data = imputer.fit_transform(data),,1.1,"from sklearn.impute import SimpleImputer\nimport numpy as np\ndata = np.array([[1, 2, 3], [4, None, 6], [7, 8, None]], dtype=float)\nimputer = ","Impute missing values in a dataset using SimpleImputer, including the `verbose` parameter if available. Verify that the output dimensions are as expected.",scikit-learn,numpy==1.23.5,"
argument change
"
,"conditions = isinstance(scorer_names, list) and len(scorer_names) > 0\nassert conditions",get_scorer_names,,metrics.get_scorer_names(),,1.3,from sklearn import metrics\nscorer_names = ,"Retrieve and list all available scorer names, ensuring they are returned in a list format.",scikit-learn,numpy==1.23.5,"
name change
"
,"conditions = isinstance(scorer_names, list) and len(scorer_names) > 0\nassert conditions",get_scorer_names,,list(metrics.SCORERS.keys()),,1.2,from sklearn import metrics\nscorer_names = ,"Retrieve and list all available scorer names, ensuring they are returned in a list format.",scikit-learn,numpy==1.23.5,"
name change
"
,"expected_result = np.array([1, 5, 5, 1, 9, 3])\nassert np.allclose(result, expected_result, atol=1e-3)",manhattan_distances,,"np.sum(distances, axis=1)",,1.1,"from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\ndistances = manhattan_distances(X, Y, sum_over_features=False)\nresult = ",Adapt the use of `manhattan_distances` to obtain a pairwise distance matrix.,scikit-learn,numpy==1.23.5,"
argument change
"
,"expected_result = np.array([[1, 5], [5, 1], [9, 3]])\nassert np.allclose(result, expected_result, atol=1e-3)",manhattan_distances,,"manhattan_distances(X, Y)",,1.2,"from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\nresult = ",Adapt the use of `manhattan_distances` in scikit-learn version 1.2 to obtain a pairwise distance matrix.,scikit-learn,numpy==1.23.5,"
argument change
"
,"
expected_cmap_reversed = {'blue': [(-1.0, 1, 2), (0.0, 2, 2)], 'red': [(0.0, 0, 0), (1.0, 0, 0)], 'green': [(0.0, 0, 0), (1.0, 0, 0)]}

reversed_cmap_dict = cmap_reversed._segmentdata

assert reversed_cmap_dict == expected_cmap_reversed",revcmap,,"LinearSegmentedColormap(""custom_cmap"", cmap).reversed()
",,3.4.0,"from matplotlib.colors import *
import numpy as np
cmap = {
    ""blue"": [[1, 2, 2], [2, 2, 1]],
    ""red"": [[0, 0, 0], [1, 0, 0]],
    ""green"": [[0, 0, 0], [1, 0, 0]]
}

cmap_reversed = ",Reverse the following color mapping.,matplotlib,,"
name change
"
,"
assert ax.xaxis.get_label().get_text()==""x_axis"" and ax.yaxis.get_label().get_text()==""y_axis"" and ax.zaxis.get_label().get_text()==""z_axis"" ",,,"ax.xaxis.set_label_text('x_axis')
ax.yaxis.set_label_text('y_axis')
ax.zaxis.set_label_text('z_axis')",,3.6.0,"import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

x = [1, 2, 3, 4, 5]
y = [5, 4, 3, 2, 1]
z = [2, 3, 4, 5, 6]

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(x, y, z, c='r', marker='o')
","Set the labels for the x, y and z axis to 'x_axis' 'y_axis' and 'z_axis'",matplotlib,,"
name change (attribute)
"
,"
assert contour_set.ax == ax",,,"ax.contour(x, y, Z)
contour_set.ax = ax
",,3.5.0,"import numpy as np
import matplotlib.pyplot as plt

def create_contour_plot(x, y, z, ax):
    pass


x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.sin(np.sqrt(X**2 + Y**2))

fig, ax = plt.subplots()

contour_set = ","Create a contour plot and set the axis for the ContourSet object. Write a function create_contour_plot that creates a contour plot using given x, y, and z data and assigns it to a specific axis",matplotlib,numpy,"
name change (attribute)
"
03-18-2023,"assert hasattr(Imputer, 'time_strategy' and Imputer.time_strategy == 'mean'",Simple_Imputer,,(time_strategy=”mean' target=target),,3,"from pycaret.internal.preprocess import Simple_Imputer
import pycaret

test_length = 6
data = pycaret.datasets.get_data(""juice"")[0:test_length]
target = ""Purchase""

Imputer = Simple_Imputer",Write the python code to construct a SimpleImputer with the time strategy as 'mean'using PyCaret version 3.0,PyCaret,,"
attribute change
"
03-18-2023,"import inspect

def check_seasonal_parameter(func):
    sig = inspect.signature(func)
    parameters = sig.parameters
    assert 'seasonal_parameter' in parameters

# Test the function
check_seasonal_parameter(exp_name)",setup,,"(data=airline,seasonal_parameter='H')",,3,"from pycaret.datasets import get_data
airline = get_data('airline')
from pycaret.time_series import setup
exp_name = setup",Write the python code to initialize a training environment and create the transformation pipeline using PyCaret version 3.0 Setup function which uses the hourly seasonal period for determining the frequency of the timeseries data.,PyCaret,,"
attribute change
"
,assert grouped_df.equals(expected_output),groupby,,"expected_output = pd.DataFrame({'y': [3, 4]}, index=pd.Index([1, None], name='x'))",,1.5.0,"import pandas as pd\ndf = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\ngrouped_df = df.groupby('x', observed=False, dropna=False).sum()\n# Determine if the groupby operation handled NA values correctly.\n# store expected value of grouped_df in a variable called expected_output\n","Use the pandas groupby operation with observed=False and dropna=False, where the intention is to include unobserved categories without dropping NA values. Your job is to predict the expected output after this operation.",pandas,numpy==1.21.6,"
output behaviour
"
,assert grouped_df.equals(expected_output),groupby,,"expected_output = pd.DataFrame({'y': [3, 0, 0]}, index=pd.Index([1, 2, 3], name='x'))",,1.5.1,"import pandas as pd\ndf = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\ngrouped_df = df.groupby('x', observed=False, dropna=False).sum()\n# Examine if the groupby operation correctly includes unobserved categories and handles NA values.\n# store expected value of grouped_df in a variable called expected_output\n","Use the pandas groupby operation with observed=False and dropna=False, where the intention is to include unobserved categories without dropping NA values. Your job is to predict the expected output after this operation.",pandas,numpy==1.21.6,"
output behaviour
"
,"correct_prices=pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)\nassert expected_prices.equals(correct_prices)",iloc,,"expected_prices = pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)",,1.5.0,"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\n# store expected value original prices in variable called expected_prices\n",Predict behaviour of setting values with iloc inplace.,pandas,numpy==1.21.6,"gh
output behaviour
"
,"correct_prices=pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)\nassert expected_prices.equals(correct_prices)",iloc,,"expected_prices = pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)",,2,"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\n# store expected value of original prices in variable called expected_prices\n",Predict behaviour of setting values with iloc inplace.,pandas,numpy==1.21.6,"
output behaviour
"
,"assert sliced_ser.equals(expected_output), 'Slicing does not match expected output'",Series slicing,,"expected_output = pd.Series([3, 4], index=[5, 7], dtype=np.int64)",,1.5.0,"import pandas as pd\nimport numpy as np\nser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nsliced_ser = ser[2:4]\n# put answer in variable called expected_output\n",Predict behaviour of integer slicing on a Series.,pandas,numpy==1.21.6,"
output behaviour
"
,"assert sliced_ser.equals(expected_output), 'Slicing does not match expected label-based output'",Series slicing,,"expected_output = pd.Series([3, 4], index=[5, 7], dtype=np.int64)",,2,"import pandas as pd\nimport numpy as np\nser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nsliced_ser = ser.iloc[2:4]\n# put answer in variable called expected_output\n",Predict behaviour of integer slicing on a Series.,pandas,numpy==1.21.6,"
output behaviour
"
,assert is_correct_type,Index,, 'int64',,1.4.0,"import pandas as pd\nindex = pd.Index([1, 2, 3], dtype='int32')\nis_correct_type = index.dtype ==",Predict the correct type.,pandas,numpy==1.21.6,"
output behaviour
"
,"expected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'",append,,"combined_series = series1.append(series2, ignore_index=True)\ncombined_dataframe = df1.append(df2, ignore_index=True)",,1.4.0,"import pandas as pd\nseries1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n# put answers in variables combined_series and combined_dataframe\n",Combine series and dataframes.,pandas,numpy==1.21.6,"
name change
"
,assert is_correct_type,Index,, 'int32',,2,"import pandas as pd\nindex = pd.Index([1, 2, 3], dtype='int32')\nis_correct_type = index.dtype ==",Predict the correct type.,pandas,numpy==1.21.6,"
output behaviour
"
,"expected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'",append,,"combined_series = pd.concat([series1, series2], ignore_index=True)\ncombined_dataframe = pd.concat([df1, df2], ignore_index=True)",,2,"import pandas as pd\nseries1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n# put answers in variables combined_series and combined_dataframe\n",Combine series and dataframes.,pandas,numpy==1.21.6,"
name change
"
22/06/2021,"
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_convolution_full(arr1, arr2).all() == np.convolve(arr1, arr2, 'full').all()",numpy.convolve,,"np.convolve(arr1, arr2, mode=""full"")",,1.21.0,"import numpy as np

def apply_convolution_full(arr1, arr2):
    return ",Implement a function that calculates the convolution of two arrays with the mode set to full.,numpy,,"
argument change
"
22/06/2021,"
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_convolution_valid(arr1, arr2).all() == np.convolve(arr1, arr2, 'valid').all()",numpy.convolve,,"np.convolve(arr1, arr2, mode=""valid"")",,1.21.0,"import numpy as np

def apply_convolution_valid(arr1, arr2):
    return ",Implement a function that calculates the convolution of two arrays with the mode set to valid.,numpy,,"
argument change
"
22/06/2021,"
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_correlate_full(arr1, arr2).all() == np.correlate(arr1, arr2, 'full').all()",numpy.correlate,,"np.correlate(arr1, arr2, mode=""full"")",,1.21.0,"import numpy as np

def apply_correlate_full(arr1, arr2):
    return ",Implement a function that calculates the Cross-correlation of two 1-dimensional sequences with the mode set to full.,numpy,,"
argument change
"
17/06/2023,"
array1 = np.array([1, 2, 3])
array2 = np.array([4.0, 5.0, 6.0])

assert find_common_type(array1, array2) == np.common_type(array1, array2)",find_common_type,,"common_type(array1, array2)",,1.25.0,"import numpy as np

def find_common_type(arr1, arr2):
    return np.","Given two arrays, find their common types.",numpy,,"
deprecation
"
22/06/2021,"
array1 = np.array([1, 2, 3])
array2 = np.array([4.0, 5.0, 6.0])

assert find_common_type(array1, array2) == np.find_common_type(array1, array2)",find_common_type,,"find_common_type(array1, array2)",,1.21.0,"import numpy as np

def find_common_type(arr1, arr2):
    return np.","Given two arrays, find their common types.",numpy,,"
deprecation
"
17/06/2023,"

def test_custom_round():
    arr = np.array([1.5, 2.3, 3.7])
    result = custom_round(arr)
    expected = np.round(arr)
    assert np.array_equal(result, expected)

test_custom_round()",round_,,np.round(arr),,1.25.0,"import numpy as np

def custom_round(arr):
    return ",Write a function that rounds an array of numbers.,numpy,,"
deprecation
"
17/06/2023,"

def test_custom_product():
    arr = np.array([1, 2, 3, 4])
    result = custom_product(arr)
    expected = np.prod(arr)
    assert result == expected

test_custom_product()",product,,np.prod(arr),,1.25.0,"import numpy as np

def custom_product(arr):
    return ",Write a function that computes the product of an array.,numpy,,"
deprecation
"
17/06/2023,"

def test_custom_cumproduct():
    arr = np.array([1, 2, 3, 4])
    result = custom_cumproduct(arr)
    expected = np.cumprod(arr)
    assert np.array_equal(result, expected)

test_custom_cumproduct()",cumproduct,,np.cumprod(arr),,1.25.0,"import numpy as np

def custom_cumproduct(arr):
    return ",Write a function that computes the cumulative product of an array.,numpy,,"
deprecation
"
17/06/2023,"

def test_custom_sometrue():
    arr = np.array([0, 0, 1, 0])
    result = custom_sometrue(arr)
    expected = np.any(arr)
    assert result == expected

test_custom_sometrue()",sometrue,,np.any(arr),,1.25.0,"import numpy as np

def custom_sometrue(arr):
    return ",Write a function that checks if any elements in an array are true.,numpy,,"
deprecation
"
17/06/2023,"

def test_custom_alltrue():
    arr = np.array([1, 1, 1, 1])
    result = custom_alltrue(arr)
    expected = np.all(arr)
    assert result == expected

test_custom_alltrue()",alltrue,,np.all(arr),,1.25.0,"import numpy as np

def custom_alltrue(arr):
    return ",Write a function that checks if all elements in an array are true.,numpy,,"
deprecation
"
22/06/2021,"

def test_custom_round():
    arr = np.array([1.5, 2.3, 3.7])
    result = custom_round(arr)
    expected = np.round_(arr)
    assert np.array_equal(result, expected)

test_custom_round()",round_,,np.round_(arr),,1.21.0,"import numpy as np

def custom_round(arr):
    return ",Write a function that rounds an array of numbers.,numpy,,"
deprecation
"
22/06/2021,"

def test_custom_product():
    arr = np.array([1, 2, 3, 4])
    result = custom_product(arr)
    expected = np.product(arr)
    assert result == expected

test_custom_product()",product,,np.product(arr),,1.21.0,"import numpy as np

def custom_product(arr):
    return ",Write a function that computes the product of an array.,numpy,,"
deprecation
"
22/06/2021,"
def test_custom_cumproduct():
    arr = np.array([1, 2, 3, 4])
    result = custom_cumproduct(arr)
    expected = np.cumproduct(arr)
    assert np.array_equal(result, expected)

test_custom_cumproduct()",cumproduct,,np.cumproduct(arr),,1.21.0,"import numpy as np

def custom_cumproduct(arr):
    return ",Write a function that computes the cumulative product of an array.,numpy,,"
deprecation
"
22/06/2021,"
def test_custom_sometrue():
    arr = np.array([0, 0, 1, 0])
    result = custom_anytrue(arr)
    expected = np.sometrue(arr)
    assert result == expected

test_custom_sometrue()",sometrue,,np.sometrue(arr),,1.21.0,"import numpy as np

def custom_anytrue(arr):
    return ",Write a function that checks if any elements in an array are true.,numpy,,"
deprecation
"
22/06/2021,"

def test_custom_alltrue():
    arr = np.array([1, 1, 1, 1])
    result = custom_alltrue(arr)
    expected = np.alltrue(arr)
    assert result == expected

test_custom_alltrue()",alltrue,,np.alltrue(arr),,1.21.0,"import numpy as np

def custom_alltrue(arr):
    return ",Write a function that checks if all elements in an array are true.,numpy,,"
deprecation
"
2020-08,"import numpy as np
expected_values = np.array([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0])
np.testing.assert_array_equal(pred, expected_values)",predict,,".predict(data, start_iteration=10) ",,3.0.0,"import numpy as np
import lightgbm as lgb
from lightgbm import LGBMClassifier
np.random.seed(0)
data = np.random.rand(100, 10) 
target = np.random.randint(0, 2, 100)
model = LGBMClassifier()
model.fit(data, target)
model.fit(data, target)
pred = model",Predict values for each sample with the starting iteration of the tenth iteration.,lightgbm,numpy==1.26.4,Argument or Attribute change
2020-08,"import numpy as np
assert 'cvbooster' in cv_results
assert len(cv_results['cvbooster'].boosters) == NFOLD
assert all(isinstance(booster, lgb.Booster) for booster in cv_results['cvbooster'].boosters)",cv,,"return_cvbooster=True
)",,3.0.0,"import numpy as np
import lightgbm as lgb
from sklearn.datasets import make_classification

NUM_SAMPLES = 500
NUM_FEATURES = 20
INFORMATIVE_FEATURES = 2
REDUNDANT_FEATURES = 10
RANDOM_STATE = 42
NUM_BOOST_ROUND = 100
NFOLD = 5
LEARNING_RATE = 0.05
EARLY_STOPPING_ROUNDS = 10
X, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)
train_data = lgb.Dataset(X, label=y)

params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': LEARNING_RATE,
    'verbose': -1
}

cv_results = lgb.cv(
    params=params,
    train_set=train_data,
    num_boost_round=NUM_BOOST_ROUND,
    nfold=NFOLD,
    early_stopping_rounds=EARLY_STOPPING_ROUNDS,",Perform cross-validation with the given parameters and return the evaluation history for each fold.,lightgbm,numpy==1.26.4 scikit-learn==1.3.2,Argument or Attribute change
2020-08,"assert decoded_string == 'hello', ""Decoded string should be 'hello'""
",decode_string,,compat.decode_string(ENCODED_STRING),,3.0.0,"ENCODED_STRING = b'\x68\x65\x6c\x6c\x6f'

import lightgbm.compat as compat

decoded_string = ",Decode a byte string (ENCODED_STRING),lightgbm,,Semantics or Function Behaviour change
2020-08,"assert {'train binary_logloss-mean', 'train binary_logloss-stdv', 'valid binary_logloss-mean', 'valid binary_logloss-stdv'}.issubset(cv_results.keys())",cv,,"eval_train_metric=True
)",,3.0.0,"import numpy as np
import lightgbm as lgb
from sklearn.datasets import make_classification

NUM_SAMPLES = 500
NUM_FEATURES = 20
INFORMATIVE_FEATURES = 2
REDUNDANT_FEATURES = 10
RANDOM_STATE = 42
NUM_BOOST_ROUND = 100
NFOLD = 5
LEARNING_RATE = 0.05
EARLY_STOPPING_ROUNDS = 10
X, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)
train_data = lgb.Dataset(X, label=y)

params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': LEARNING_RATE,
    'verbose': -1
}

cv_results = lgb.cv(
    params=params,
    train_set=train_data,
    num_boost_round=NUM_BOOST_ROUND,
    nfold=NFOLD,
    early_stopping_rounds=EARLY_STOPPING_ROUNDS,",Perform cross-validation with the given parameters and display the training metric in progress. ,lightgbm,numpy==1.26.4 scikit-learn==1.3.2,Argument or Attribute change
2020-08,"assert isinstance(np_array, np.ndarray)
assert np_array.shape == (5,)
assert np.array_equal(np_array, np.array([1, 2, 3, 4, 5], dtype=np.int32))",cint32_array_to_numpy,,".basic.cint32_array_to_numpy(c_pointer, length)",,3.0.0,"import lightgbm as lgb
import numpy as np
import ctypes

c_array_type = ctypes.POINTER(ctypes.c_int32)
c_array = (ctypes.c_int32 * 5)(1, 2, 3, 4, 5)
c_pointer = ctypes.cast(c_array, c_array_type)
length = 5

np_array = lgb",Convert a ctypes pointer to a NumPy array of the specified length.,lightgbm,numpy==1.26.4,Function Name change
2020-08,"assert isinstance(params, dict) or params is None
",get_params,,dataset.get_params(),,3.0.0,"import lightgbm as lgb
import numpy as np

data = np.random.rand(10, 2)
label = np.random.randint(2, size=10)
dataset = lgb.Dataset(data, label=label)

params =",Get the parameters of a dataset object as a dictionary.,lightgbm,numpy==1.26.4,Semantics or Function Behaviour change
2020-08,"assert json_data == '[1, 2, 3]'




",json_default_with_numpy,,", default=json_default_with_numpy)",,3.0.0,"import numpy as np
import json
from lightgbm.compat import json_default_with_numpy

NUMPY_ARRAY = np.array([1, 2, 3])

json_data = json.dumps(NUMPY_ARRAY",Serialize a NumPy array to a JSON string using a custom default function that converts NumPy arrays to lists.,lightgbm,numpy==1.26.4,Function Name change
2024-01,"assert all(isinstance(i, float) for i in c_array)
assert all(c_array[i] == VALUES[i] for i in range(len(VALUES)))",basic._c_array,,"basic._c_array(CTYPE, VALUES)",,4.3.0,"import ctypes
import lightgbm.basic as basic

CTYPE = ctypes.c_double
VALUES = [0.1, 0.2, 0.3, 0.4, 0.5]

c_array =",Create a ctypes array from a list of values.,lightgbm,,Function Name change
2024-01,"assert isinstance(c_string, ctypes.c_char_p)
assert c_string.value.decode('utf-8') == python_string",basic._c_str,,lgb.basic._c_str(python_string),,4.3.0,"import lightgbm as lgb
import ctypes

# Test cases for c_str function
python_string = ""lightgbm""
c_string = ",Convert a Python string to a C string.,lightgbm,,Function Name change
2024-01,"assert isinstance(fixed_data, np.ndarray)
assert fixed_data.shape == sliced_data.shape
assert np.array_equal(fixed_data, sliced_data)
",basic._convert_from_sliced_object,,.basic._convert_from_sliced_object(sliced_data),,4.3.0,"import lightgbm as lgb
import numpy as np

data = np.random.rand(100, 10)
sliced_data = data[:, :5]

fixed_data = lgb",Convert a sliced NumPy array back to a contiguous NumPy array.,lightgbm,numpy==1.26.4,Function Name change
2023-01,"assert labels == ('GPE', 'PERSON')",labels,,.labels,,3.5.0,"import spacy
from spacy.pipeline.span_ruler import SpanRuler

nlp = spacy.blank(""en"")
ruler = SpanRuler(nlp)

patterns = [
    {""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""john""}]},
    {""label"": ""GPE"", ""pattern"": [{""LOWER"": ""london""}]},
]
ruler.add_patterns(patterns)

labels = ruler",Get the labels of the span ruler.,spacy,numpy==1.26.4,New feature or additional dependency based change
2023-01,"expected_doc_annotation = {
    'cats': {},
    'entities': ['U-GREETING', 'O', 'O'],
    'spans': {},
    'links': {}
}

expected_token_annotation = {
    'ORTH': ['Hello', ' ', 'world'],
    'SPACY': [True, False, False],
    'TAG': ['', '', ''],
    'LEMMA': ['', '', ''],
    'POS': ['', '', ''],
    'MORPH': ['', '', ''],
    'HEAD': [0, 1, 2],
    'DEP': ['', '', ''],
    'SENT_START': [1, 0, 0]
}

assert augmented_example.to_dict()[""doc_annotation""] == expected_doc_annotation
assert augmented_example.to_dict()[""token_annotation""] == expected_token_annotation",make_whitespace_variant,,"augment.make_whitespace_variant(nlp, example, whitespace, position)",,3.5.0,"import spacy
from spacy.training import Example
from spacy.training import augment

nlp = spacy.blank(""en"")

tokens = nlp(""Hello world"")
annotations = {""entities"": [(0, 5, ""GREETING"")]}
example = Example.from_dict(tokens, annotations)

whitespace = "" ""
position = 1

augmented_example = ",Create a whitespace variant of an example.,spacy,numpy==1.26.4,New feature or additional dependency based change
2023-01,"assert len(ruler.patterns) == 1
remaining_pattern_ids = [pattern[""id""] for pattern in ruler.patterns]
assert pattern_id_to_remove not in remaining_pattern_ids",remove_by_id,,.remove_by_id(pattern_id_to_remove),,3.5.0,"import spacy
from spacy.pipeline.span_ruler import SpanRuler

nlp = spacy.blank(""en"")
ruler = SpanRuler(nlp)

patterns = [
    {""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""john""}], ""id"": ""pattern1""},
    {""label"": ""GPE"", ""pattern"": [{""LOWER"": ""london""}], ""id"": ""pattern2""},
]
ruler.add_patterns(patterns)

assert len(ruler.patterns) == 2

pattern_id_to_remove = ""pattern1""

ruler",Remove a pattern from a span ruler by its ID.,spacy,numpy==1.26.4,New feature or additional dependency based change
2022-02,"assert isinstance(parsed_tree, Tree)
assert parsed_tree.label() == 'S'",fromstring,,".fromstring(s, remove_empty_top_bracketing=True)",,3.7,"from nltk import Tree

s = ""(S (NP (DT The) (NN cat)) (VP (VBZ sits) (PP (IN on) (NP (DT the) (NN mat)))))""

parsed_tree = Tree",Parse a string representation of a tree into an NLTK Tree object.,nltk,,New feature or additional dependency based change
2022-02,"expected_matches = [(0, 0), (1, 1), (2, 3), (3, 4), (4, 5), (5, 6)]
matches, unmatched_hypo, unmatched_ref = align_words(hypothesis, reference)
assert matches == expected_matches
assert unmatched_hypo == []
assert unmatched_ref == [(2, 'is')]",align_words,,nltk.translate.meteor_score.align_words,,3.7,"import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import wordnet

hypothesis = [""the"", ""cat"", ""sits"", ""on"", ""the"", ""mat""]
reference = [""the"", ""cat"", ""is"", ""sitting"", ""on"", ""the"", ""mat""]

align_words = ",Align words in a hypothesis and reference sentence using the METEOR algorithm.,nltk,,New feature or additional dependency based change
2022-02,"assert isinstance(examples, list)
assert examples == ['the dog barked all night']",examples,,synset.examples(),,3.7,"import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import wordnet

synset = wordnet.synset('dog.n.01')

examples = ",Get examples of a synset from WordNet.,nltk,,New feature or additional dependency based change
2022-02,"assert isinstance(parsed_tree, Tree)
assert parsed_tree.label() == ""NP""",fromstring,,Tree.fromstring(tree_string),,3.7,"import nltk
nltk.download('sinica_treebank')
from nltk.tree import Tree
from nltk.corpus import sinica_treebank

sinica_sentence = sinica_treebank.parsed_sents()[0]
tree_string = sinica_sentence.pformat()

parsed_tree = ",Parse a string representation of a tree into an NLTK Tree object.,nltk,,New feature or additional dependency based change
2020-04,"assert result == [1, 3, 6, 10, 15]",accumulate,,"accumulate(iterable, func))",,3.5,"from nltk.lm.api import accumulate
import operator

iterable = [1, 2, 3, 4, 5]
func = operator.add

result = list(",Accumulate the results of applying a function to elements of an iterable.,nltk,,Semantics or Function Behaviour change
2020-04,"assert isinstance(tokens, list)
assert tokens == [""This"", ""is"", ""a"", ""test"", ""sentence"", "".""]",tokenize,,.tokenize.destructive.NLTKWordTokenizer().tokenize(s),,3.5,"import nltk.tokenize.destructive

s = ""This is a test sentence.""
tokens = nltk",Tokenize a string,nltk,,New feature or additional dependency based change
2023-12,"
assertion_value = utc_time.tzname() == 'UTC'
assert assertion_value
assertion_value = utc_time.isoformat() == '2024-11-05T00:00:00+00:00'
assert assertion_value
",utils.timezone.utc,1.0,"
from datetime import timezone as py_timezone
utc_time = timezone.datetime(year, month, day, tzinfo=py_timezone.utc)
",,5.0.0,"
import django
from django.conf import settings
from django.utils import timezone

settings.configure()

year = 2024
month = 11
day = 5
","
Define time zone settings to utc for the given datetime, store in a variable named utc_time. If needed, use another library",django,,name change
2021-12,"
assertion_value =  utc_time.tzname() == 'UTC'
assert assertion_value
assertion_value =  utc_time.isoformat() == '2024-11-05T00:00:00+00:00'
assert assertion_value
",utils.timezone.utc,1.0,"
utc_time = timezone.datetime(year, month, day, tzinfo=timezone.utc)
",,4.0.0,"
import django
from django.conf import settings
from django.utils import timezone

settings.configure()

year = 2024
month = 11
day = 5
","
Define time zone settings to utc for the given datetime, store in a variable named utc_time. If needed, use another library",django,,name change
2021-12,"
assertion_result = result == 'dummy_instance_value_result'
assert assertion_result
",BaseModelFormSet.save_existing,2.0,"instance=to_save['instance'])
",,4.0.0,"
from django.conf import settings
from django.forms.models import BaseModelFormSet
from django.forms.renderers import get_default_renderer


settings.configure()

class DummyForm:
    def save(self, commit=True):
        return 'dummy_instance_value_result'

class MyFormSet(BaseModelFormSet):
    def __init__(self, *args, **kwargs):
        self.renderer = get_default_renderer()
        super().__init__(*args, **kwargs)


to_save = {
    'form':DummyForm(),
    'instance':'dummy_instance_value'
}
fs5 = MyFormSet(queryset=[])
result = fs5.save_existing(form=to_save['form'],","
Calls save_existing on the formset instance using keyword arguments",django,,argument or attribute change
2023-12,"
assertion_result = result == 'dummy_instance_value_result'
assert assertion_result",BaseModelFormSet.save_existing,2.0,"obj=to_save['instance'])
",,5.0.0,"
from django.conf import settings
from django.forms.models import BaseModelFormSet
from django.forms.renderers import get_default_renderer


settings.configure()

class DummyForm:
    def save(self, commit=True):
        return 'dummy_instance_value_result'

class MyFormSet(BaseModelFormSet):
    def __init__(self, *args, **kwargs):
        self.renderer = get_default_renderer()
        super().__init__(*args, **kwargs)


to_save = {
    'form':DummyForm(),
    'instance':'dummy_instance_value'
}
fs5 = MyFormSet(queryset=[])
result = fs5.save_existing(form=to_save['form'],","
Calls save_existing on the formset instance using keyword arguments",django,,argument or attribute change
2021-12,"
try: 
    with connection.schema_editor() as schema_editor:
        schema_editor.create_model(MyModel)
except Exception as e:
    pass

import time
import math
obj = create_default_objects(MyModel)
t0 = obj.creation_time
print('circumference:', obj.circumference)
time.sleep(1)
obj = create_default_objects(MyModel)
t1 = obj.creation_time
assertion_result = t0 < t1
assert assertion_result
assertion_results = obj.circumference == 2 * math.pi
assert assertion_results
",Field.db_default,2.0,"(models.DateTimeField(default=Now()), models.FloatField(default= 2 * Pi()))",,4.0.0,"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models.functions import Now, Pi

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()
def create_default_objects(MyModel):
    obj = MyModel.objects.create()
    obj.refresh_from_db()
    return obj

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    # creation_time is set to the current time when the object is created
    # circumference is set to 2 * pi
    creation_time, circumference =","
Set the default value of a field to the current time and the circumference to 2 * pi given the create_default_objects function 
",django,,other library or new feature
2023-12,"
try: 
    with connection.schema_editor() as schema_editor:
        schema_editor.create_model(MyModel)
except Exception as e:
    pass

import time
import math
obj = create_default_objects(MyModel)
t0 = obj.creation_time
print('circumference:', obj.circumference)
time.sleep(1)
obj = create_default_objects(MyModel)
t1 = obj.creation_time

assertion_result = t0 < t1
assert assertion_result
assertion_results = obj.circumference == 2 * math.pi
assert assertion_results
",Field.db_default,2.0,"models.DateTimeField(db_default=Now()), models.FloatField(db_default=2 * Pi())",,5.0.0,"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models.functions import Now, Pi

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()

def create_default_objects(MyModel):
    obj = MyModel.objects.create()
    return obj

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    creation_time,circumference = ","
Set the default value of a field to the current time and the circumference to 2 * pi given the create_default_objects function 
",django,,other library or new feature
2023-12,"
rendered_output = render_output(template_string)
def normalize_html(html):
    # Remove all whitespace and standardize quotation marks to single quotes
    normalized = ''.join(html.split())
    return normalized

template_string_django_4 = '''
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''
assertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))
assert assertion_result
assertion_result = len(template_string) < 100 # check if the template_string is not too long (ideally should be 278)
assert assertion_result
",BoundField.as_field_group(),2.0,"
<form>
  <div>
    {{ form.name.as_field_group }}
  </div>
</form>
'''",,5.0.0,"
import django
from django.conf import settings
from django import forms
from django.template import Template, Context

settings.configure(
      TEMPLATES=[
          {
              'BACKEND': 'django.template.backends.django.DjangoTemplates',
          },
      ],
  )
django.setup()

class SampleForm(forms.Form):
    name = forms.CharField(label='Name', help_text='Enter your name')

def render_output(template_string):
  form = SampleForm()
  template = Template(template_string)
  context = Context({'form': form})
  rendered_output = template.render(context)
  return rendered_output

template_string = '''","
Adapt the template_string variable to form the html string provided in comments, making the best use of the templates for form field rendering
",django,,other library or new feature
2021-12,"
rendered_output = render_output(template_string)
def normalize_html(html):
    # Remove all whitespace and standardize quotation marks to single quotes
    normalized = ''.join(html.split())
    return normalized

template_string_django_4 = '''
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''
assertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))
assert assertion_result
assertion_result = len(template_string) < 300 # check if the template_string is not too long (ideally should be 278)
assert assertion_result
",BoundField.as_field_group(),2.0,"
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''",,4.0.0,"
import django
from django.conf import settings
from django import forms
from django.template import Template, Context

settings.configure(
      TEMPLATES=[
          {
              'BACKEND': 'django.template.backends.django.DjangoTemplates',
          },
      ],
  )
django.setup()

class SampleForm(forms.Form):
    name = forms.CharField(label='Name', help_text='Enter your name')

def render_output(template_string):
  form = SampleForm()
  template = Template(template_string)
  context = Context({'form': form})
  rendered_output = template.render(context)
  return rendered_output

template_string = '''","
Adapt the template_string variable to form the html string provided in comments, making the best use of the templates for form field rendering
",django,,other library or new feature
2021-12,"
with connection.schema_editor() as schema_editor:
    schema_editor.create_model(Square)
square = create_square(side=5)
correct_result = (5, 25)
assert display_side_and_area(square) == correct_result
",BoundField.models.GeneratedField(),2.0,"models.BigIntegerField(editable=False)

    def save(self, *args, **kwargs):
        # Compute the area before saving.
        self.area = self.side * self.side
        super().save(*args, **kwargs)

",,4.0.0,"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models import F

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()


def display_side_and_area(square):
    return square.side, square.area

def create_square(side):
    square = Square.objects.create(side=side)
    square.refresh_from_db()
    return square

class Square(models.Model):
    class Meta:
        app_label = 'myapp'
    side = models.IntegerField()
    area = ","
Create Square model with a side field and an area field that is calculated as the square of the side.
",django,,other library or new feature
2023-12,"
with connection.schema_editor() as schema_editor:
    schema_editor.create_model(Square)
square = create_square(side=5)
correct_result = (5, 25)
assert display_side_and_area(square) == correct_result
",BoundField.models.GeneratedField(),2.0,"models.GeneratedField(
        expression=F('side') * F('side'),
        output_field=models.BigIntegerField(),
        db_persist=True,
    )
",,5.0.0,"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models import F

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()


def display_side_and_area(square):
    return square.side, square.area

def create_square(side):
    square = Square.objects.create(side=side)
    square.refresh_from_db()
    return square

class Square(models.Model):
    class Meta:
        app_label = 'myapp'
    side = models.IntegerField()
    area = ","
Create Square model with a side field and an area field that is calculated as the square of the side.
",django,,other library or new feature
2023-12,"
class MyModelCorrect(models.Model):
    color = models.CharField(max_length=5, choices=color)
    
    class Meta:
        app_label = 'myapp'

field_choices = list(MyModel._meta.get_field('color').choices)

expected_choices = list(MyModelCorrect._meta.get_field('color').choices)

assert field_choices == expected_choices
",Field.choices,2.0," choices=color)
    
",,5.0.0,"
import django
from django.conf import settings
from django.db import models

settings.configure()
django.setup()

color = models.TextChoices('Color', 'RED GREEN BLUE')

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    color = models.CharField(max_length=5,","
create a model based on the given color choices",django,,argument or attribute change
2021-12,"
class MyModelCorrect(models.Model):
    color = models.CharField(max_length=5, choices=color.choices)
    
    class Meta:
        app_label = 'myapp'

field_choices = MyModel._meta.get_field('color').choices

expected_choices = list(MyModelCorrect._meta.get_field('color').choices)

assert field_choices == expected_choices
",Field.choices,2.0," choices=color.choices)
",,4.0.0,"
import django
from django.conf import settings
from django.db import models

settings.configure()
django.setup()

color = models.TextChoices('Color', 'RED GREEN BLUE')

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    color = models.CharField(max_length=5,","
create a model based on the given color choices",django,,argument or attribute change
2021-11,"
assertion_value   = np.allclose(output, distance.wminkowski(u,v,3,w))
assert assertion_value",distance.wminkowski,1.0,"
output = distance.wminkowski(u,v,3,w)",,1.7.3,"
from scipy.spatial import distance
import numpy as np 
u = np.asarray([11,12,13,14,15])
v = np.asarray([1,2,3,4,5])
w = np.asarray([0.1,0.3,0.15,0.25,0.2])","Compute the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w, and store the results in a variable named output",scipy,,name change
2022-10,"
assertion_value = np.allclose(output, distance.minkowski(u,v,3,w=w))
assert assertion_value",distance.minkowski,1.0,"
output = distance.minkowski(u,v,3,w=w)",,1.9.2,"
from scipy.spatial import distance
import numpy as np 
u = np.asarray([11,12,13,14,15])
v = np.asarray([1,2,3,4,5])
w = np.asarray([0.1,0.3,0.15,0.25,0.2])","Compute the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w, and store the results in a variable named output",scipy,,name change
2022-05,"
assertion_value = np.allclose(output, np.stack([linalg.expm(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",linalg.expm,1.0,"
output = np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = linalg.expm(A[i])",,1.8.1,"
from scipy import linalg
import numpy as np 
A = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],
        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],
        [0.92575218, 0.66167048, 0.81779481, 0.15405207],
        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],
       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],
        [0.11275151, 0.95192602, 0.58950113, 0.78663422],
        [0.64955361, 0.47670695, 0.96824964, 0.74915994],
        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],
       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],
        [0.78721405, 0.09705598, 0.88774379, 0.81623277],
        [0.64821764, 0.62400451, 0.53916194, 0.96522881],
        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])","
Compute the matrix exponential of batched matrices, stored in a nd-array A, and store the results in a variable named output",scipy,,argument or attribute change
2022-10,"
assert np.allclose(output, linalg.expm(A))",linalg.expm,1.0,"
output = linalg.expm(A)",,1.9.2,"
from scipy import linalg
import numpy as np 
A = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],
        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],
        [0.92575218, 0.66167048, 0.81779481, 0.15405207],
        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],
       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],
        [0.11275151, 0.95192602, 0.58950113, 0.78663422],
        [0.64955361, 0.47670695, 0.96824964, 0.74915994],
        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],
       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],
        [0.78721405, 0.09705598, 0.88774379, 0.81623277],
        [0.64821764, 0.62400451, 0.53916194, 0.96522881],
        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])","
Compute the matrix exponential of batched matrices, stored in a nd-array A, and store the results in a variable named output",scipy,,argument or attribute change
2022-05,"
assertion_value =  np.allclose(np.asarray(output),np.asarray([-stats.combine_pvalues(1-A,'fisher')[0],(1-stats.combine_pvalues(1-A,'fisher')[1])]))
assert assertion_value",stats.combine_pvalues,1.0,"
output = stats.combine_pvalues(A,'pearson')
output = -output[0], 1-output[1]",,1.8.1,"
from scipy import stats
import numpy as np 
A = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,
       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])","
Combine the p values from various independent tests stored in a 1D array, p_vals,
 using pearson method, make sure that higher values of the statistic now correspond to lower
  p-values and store the results in a variable named output",scipy,,output change
2022-10,"
assertion_value = np.allclose(np.asarray(output),np.asarray([-stats.combine_pvalues(1-A,'fisher')[0],(1-stats.combine_pvalues(1-A,'fisher')[1])]))
assert assertion_value",stats.combine_pvalues,1.0,"
output = stats.combine_pvalues(A,'pearson')",,1.9.2,"
from scipy import stats
import numpy as np 
A = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,
       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])","
Combine the p values from various independent tests stored in a 1D array, p_vals,
 using pearson method, make sure that higher values of the statistic now correspond to lower
  p-values, and store the results in a variable named output",scipy,,output change
2022-05,"
assertion_value = np.allclose(output.todense(), linalg.expm(A).todense())
assert assertion_value",linalg,1.0,linalg.expm(A),,1.8.1,"
from scipy import sparse,linalg
import numpy as np 
A = sparse.lil_matrix((3, 3))
A[0, 0] = 4
A[1, 1] = 5
A[1, 2] = 6
output = ","
Compute the matrix exponential of a sparse array A",scipy,,name change
2022-10,"
assertion_value = np.allclose(output.todense(), sparse.linalg.expm(A).todense())
assert assertion_value",linalg,1.0,sparse.linalg.expm(A),,1.9.2,"
from scipy import sparse,linalg
import numpy as np 
A = sparse.lil_matrix((3, 3))
A[0, 0] = 4
A[1, 1] = 5
A[1, 2] = 6
output = ","
Compute the matrix exponential of a sparse array A",scipy,,name change
2022-05,"
assertion_value = np.allclose(output,1-np.abs(np.mean(np.exp(1j*a))))
assert assertion_value",stats.circvar,1.0,"
output = 1-np.abs(np.mean(np.exp(1j*a)))",,1.8.1,"
from scipy import stats
import numpy as np 
a = np.array([0, 2*np.pi/3, 5*np.pi/3])
","
Compute the circular variance: 1-R, where R is the mean resultant vector. Store the results in a variable named output",scipy,,output change
2022-10,"
assertion_value = np.allclose(output,1-np.abs(np.mean(np.exp(1j*a))))
assert assertion_value ",stats.circvar,1.0,"
output = stats.circvar(a)",,1.9.2,"
from scipy import stats
import numpy as np 
a = np.array([0, 2*np.pi/3, 5*np.pi/3])
","
Compute the circular variance: 1-R, where R is the mean resultant vector. Store the results in a variable named output",scipy,,output change
2023-08,"
import numpy as np
assertion_value = np.allclose(output,dist.moment(order=n))
assert assertion_value",rv_continuous.momentr,1.0,"
output = dist.moment(order=n)",,1.11.2,"
from scipy.stats import norm
dist = norm(15, 10)
n=5
","
 Store the results in a variable named output",scipy,,argument or attribute change
2022-10,"
import numpy as np
assertion_value = np.allclose(output,dist.moment(order=n))
assert assertion_value",rv_continuous.moment,1.0,"
output = dist.moment(n=n)",,1.9.2,"
from scipy.stats import norm
dist = norm(15, 10)
n=5
","
 Store the results in a variable named output",scipy,,argument or attribute change
2022-10,"
assertion_value=np.allclose(output, np.stack([det(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.linalg.det,1.0,"
output = np.zeros(A.shape[0])
for i in range(A.shape[0]):
    output[i] = det(A[i])",,1.9.2,"
from scipy.linalg import det
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
","
Compute the determinant of batched matrices (batched in the first dimention), store the results in a variable named output",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, det(A))
assert assertion_value",scipy.linalg.det,1.0,"
output = det(A)",,1.11.2,"
from scipy.linalg import det
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
","
Compute the determinant of batched matrices (batched in the first dimention), store the results in a variable named output",scipy,,argument or attribute change
2022-10,"
assertion_value = np.allclose(np.stack([p,u,v],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.linalg.lu,1.0," [np.zeros(A.shape) for i in range(3)]
for i in range(A.shape[0]):
    p[i],u[i],v[i] = lu(A[i])",,1.9.2,"
from scipy.linalg import lu
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
p,u,v =","
Compute the lu decomposition of batched square matrices (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(np.stack([p,u,v],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.linalg.lu,1.0, lu(A),,1.11.2,"
from scipy.linalg import lu
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
p,u,v =","
Compute the lu decomposition of batched square matrices (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
window_numpy = 2*np.arange(window_size)/(window_size-1) - 1 
window_numpy = np.sinc(window_numpy)
window_numpy = window_numpy / np.max(window_numpy)
assertion_value = np.allclose(window,window_numpy)
assert assertion_value",signal.windows.lanczos,1.0,windows.lanczos(window_size),,1.11.2,"
import scipy.signal.windows as windows
import numpy as np
window_size=31
window = ","
Using scipy if possible, create a lanczos windows",scipy,-,other library or new feature
2022-10,"
window_numpy = 2*np.arange(window_size)/(window_size-1) - 1 
window_numpy = np.sinc(window_numpy)
window_numpy =   window_numpy / np.max(window_numpy)
assertion_value = np.allclose(window,window_numpy)
assert assertion_value",signal.windows.lanczos,1.0,"2*np.arange(window_size)/(window_size-1) - 1 
window = np.sinc(window)
window = window / np.max(window)",,1.9.2,"
import scipy.signal.windows as windows
import numpy as np
window_size=31
window = ","
Using scipy if possible, create a lanczos windows",scipy,-,other library or new feature
2023-02,"
assertion_value = np.allclose(output,gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma))
assert assertion_value",ndimage.gaussian_filter1d,1.0,"gaussian_filter1d(x, radius=radius, sigma=sigma)",,1.10.1,"
from scipy.ndimage import gaussian_filter1d
import numpy as np
x = np.random.rand(100)
radius = 10
sigma= np.pi
output = ","
Apply a 1D gaussian filter",scipy,-,argument or attribute change
2022-10,"
assertion_value = np.allclose(output,gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma))
assert assertion_value ",ndimage.gaussian_filter1d,1.0,"gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma)",,1.9.2,"
from scipy.ndimage import gaussian_filter1d
import numpy as np
x = np.random.rand(100)
radius = 10
sigma= np.pi
output = ","
Apply a 1D gaussian filter",scipy,-,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, np.stack([rank_filter(A[i],rank,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.rank_filter,1.0,"rank_filter(A,rank,size=size,axes=[1,2])",,1.11.2,"
from scipy.ndimage import rank_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
rank = 6
size=3
output =","
Apply a rank filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2022-10,"
assertion_value = np.allclose(output, np.stack([rank_filter(A[i],rank,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.rank_filter,1.0,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = rank_filter(A[i],rank,size=size)",,1.9.2,"
from scipy.ndimage import rank_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
rank = 6
size = 3
output =","
Apply a rank filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, np.stack([percentile_filter(A[i],percentile=percentile,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.percentile_filter,1.0,"percentile_filter(A,percentile=percentile,size=size,axes=[1,2])",,1.11.2,"
from scipy.ndimage import percentile_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
percentile = 90
size=3
output =","
Apply a percentile filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2022-10,"
assertion_value = np.allclose(output, np.stack([percentile_filter(A[i],percentile=percentile,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.percentile_filter,1.0,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = percentile_filter(A[i],percentile=percentile,size=size)",,1.9.2,"
from scipy.ndimage import percentile_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
percentile = 90
size=3
output =","
Apply a percentile filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, np.stack([ median_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.median_filter,1.0,"median_filter(A,size=size,axes=[1,2])",,1.11.2,"
from scipy.ndimage import median_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a median filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2022-10,"
assertion_value = np.allclose(output, np.stack([ median_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.median_filter,1.0,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  median_filter(A[i], size=size)",,1.9.2,"
from scipy.ndimage import median_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a median filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, np.stack([ uniform_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.uniform_filter,1.0,"uniform_filter(A,size=size,axes=[1,2])",,1.11.2,"
from scipy.ndimage import uniform_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a uniform filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2022-10,"
assertion_value = np.allclose(output, np.stack([uniform_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.uniform_filter,1.0,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  uniform_filter(A[i], size=size)",,1.9.2,"
from scipy.ndimage import uniform_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a uniform filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, np.stack([ minimum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.minimum_filter,1.0,"minimum_filter(A,size=size,axes=[1,2])",,1.11.2,"
from scipy.ndimage import minimum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a minimum filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2022-10,"
assertion_value= np.allclose(output, np.stack([minimum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.minimum_filter,1.0,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  minimum_filter(A[i], size=size)",,1.9.2,"
from scipy.ndimage import minimum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a minimum filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, np.stack([ maximum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.maximum_filter,1.0,"maximum_filter(A,size=size,axes=[1,2])",,1.11.2,"
from scipy.ndimage import maximum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a maximum filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2022-10,"
assertion_value = np.allclose(output, np.stack([ maximum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.maximum_filter,1.0,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  maximum_filter(A[i], size=size)",,1.9.2,"
from scipy.ndimage import maximum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","
Apply a maximum filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2023-08,"
assertion_value = np.allclose(output, np.stack([ gaussian_filter(A[i],sigma=sigma) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.gaussian_filter,1.0,"gaussian_filter(A,sigma=sigma,axes=[1,2])",,1.11.2,"
from scipy.ndimage import gaussian_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
sigma=2
output =","
Apply a gaussian filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2022-10,"
assertion_value = np.allclose(output, np.stack([gaussian_filter(A[i],sigma=sigma) for i in range(A.shape[0])],axis=0))
assert assertion_value",scipy.ndimage.gaussian_filter,1.0,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  gaussian_filter(A[i],sigma=sigma)",,1.9.2,"
from scipy.ndimage import gaussian_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
sigma=2
output =","
Apply a gaussian filter on batched images (batched in the first dimention)",scipy,,argument or attribute change
2021-05,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_set):
    return flask.jsonify({'numbers': num_set})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})
assert assertion_result
",app.json_encoder,2.0,"
import json
class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app.json_encoder = MyCustomJSONHandler
",,2.0.0,"
import flask

app = flask.Flask('test')
@app.route('/data')
def data(num_set):
    return flask.jsonify({'numbers': num_set})

def eval(app, data_fn, num_set):
    with app.test_request_context():
        response = data_fn(num_set)
        return response.get_data(as_text=False)

","
Complete the app set-up for the json encoding to return a sorted list
 when called with the eval function provided with the variable 
 num_set being a set of numbers, use other libraries if needed",flask,werkzeug==2.0.0,argument or attribute change
2023-09,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_set):
    return flask.jsonify({'numbers': num_set})
class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})
assert assertion_result
",app.json_encoder,2.0,"
class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",,3.0.0,"
import flask

app = flask.Flask('test')
@app.route('/data')
def data(num_set):
    return flask.jsonify({'numbers':num_set})

def eval(app, data_fn, num_set):
    with app.test_request_context():
        response = data_fn(num_set)
        return response.get_data(as_text=True)

","
Complete the app set-up for the json encoding to return a sorted list
 when called with the eval function provided with the variable 
 num_set being a set of numbers, use other libraries if needed",flask,,argument or attribute change
2021-05,"
content_disp = get_content_disp(app1, download)
assertion_result = 'filename=hello.txt' in content_disp
assert assertion_result
",flask.send_file,1.0,"attachment_filename=attachment_filename)
",,2.0.0,"
from flask import Flask, send_file
from io import BytesIO

app1 = Flask(__name__)

def get_content_disp(app, download_fn):
    with app.test_request_context():
        response = download_fn()
    content_disp = response.headers.get('Content-Disposition')
    return content_disp

@app1.route('/download')
def download():
    data = BytesIO(b'Hello, World!')
    attachment_filename = 'hello.txt'
    return send_file(data, as_attachment=True,
","
Complete the download definition to download the data in the variable attachment_filename",flask,werkzeug==2.0.0,argument or attribute change
2023-09,"
content_disp = get_content_disp(app1, download)
assertion_result = 'filename=hello.txt' in content_disp
assert assertion_result
",flask.send_file,1.0,"download_name=attachment_filename)
",,3.0.0,"
from flask import Flask, send_file
from io import BytesIO

app1 = Flask(__name__)

def get_content_disp(app, download_fn):
    with app.test_request_context():
        response = download_fn()
    content_disp = response.headers.get('Content-Disposition')
    return content_disp

@app1.route('/download')
def download():
    data = BytesIO(b'Hello, World!')
    attachment_filename = 'hello.txt'
    return send_file(data, as_attachment=True,

","
Complete the download definition to download the data in the variable attachment_filename",flask,,argument or attribute change
2021-05,"
assertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'
assert assertion_result",Flask.config.from_json,1.0,"from_json(config_file)
",,2.0.1,"
import json
import tempfile
from flask import Flask

config_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}
with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:
    json.dump(config_data, tmp)
    tmp.flush()
    config_file = tmp.name

app = Flask(__name__)
app.config.","
Load the json file to config the Flask app",flask,werkzeug==2.0.0,name change
2023-09,"
assertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'
assert assertion_result
",Flask.config.from_file,1.0,"from_file(config_file, load=json.load)
",,3.0.0,"
import json
import tempfile
from flask import Flask

config_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}
with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:
    json.dump(config_data, tmp)
    tmp.flush()
    config_file = tmp.name

app = Flask(__name__)
app.config.","
Load the json file to config the Flask app",flask,,name change
2021-05,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'

try : 
    joined = safe_join_fail_404(base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined = safe_join_fail_404(base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt'
assert assertion_result",flask.safe_join,1.0,"
    joined = flask.safe_join(base_path, sub_path)

    return joined
",,2.0.1,"
import flask
import werkzeug

error404 = werkzeug.exceptions.NotFound

def safe_join_fail_404(base_path,sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
","
Complete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed",flask,werkzeug==2.0.0,name change
2023-09,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'

try : 
    joined = safe_join_fail_404(base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined = safe_join_fail_404(base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt'
assert assertion_result",flask.safe_join,1.0,"
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    return joined
",,3.0.0,"
import flask
import werkzeug

error404 = werkzeug.exceptions.NotFound

def safe_join_fail_404(base_path,sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
","
Complete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed",flask,,name change
2021-05,"
import datetime
td = datetime.timedelta(hours=2, minutes=30,microseconds=1)
assertion_results = convert_timedelta_to_seconds(td)==9000
assert assertion_results",helpers.total_seconds,1.0,"
    return flask.helpers.total_seconds(td)
",,2.0.1,"
import flask

def convert_timedelta_to_seconds(td):","
Complete the function convert_timedelta_to_seconds, use other libraries if needed",flask,werkzeug==2.0.0,name change
2023-09,"
import datetime
td = datetime.timedelta(hours=2, minutes=30,microseconds=1)
assertion_results = convert_timedelta_to_seconds(td)==9000.000001
assert assertion_results",helpers.total_seconds,1.0,"
    return td.total_seconds()
",,3.0.0,"
import flask

def convert_timedelta_to_seconds(td):","
Complete the function convert_timedelta_to_seconds, use other libraries if needed",flask,,name change
2020-01,"
env = setup_environment('greet',greet)
template = env.from_string('''
{{ 'World'| greet }}''')
assertion_results = 'Hi, World!' in template.render(prefix='Hi')
assert assertion_results 
assertion_results = 'Hello, World!' in template.render()
assert assertion_results 
",jinja2.contextfilter,1.0,"jinja2.contextfilter
def greet(ctx, name):
    prefix = ctx.get('prefix', 'Hello')
    return f'{prefix}, {name}!'

    
",,2.11,"
import jinja2 
def setup_environment(filtername,filter):
    env = jinja2.Environment()
    env.filters[filtername] = filter
    return env

@","
Create a custom filter named greet. This filter should: 
accept the context and a parameter name, 
retrieve a variable prefix from the context (defaulting to 'Hello' if not provided), 
return a greeting message combining the prefix and the name.",jinja2,markupsafe==2.0.1,name change
2022-03,"
env = setup_environment('greet',greet)
template = env.from_string('''
{{ 'World'| greet }}''')
assertion_results = 'Hi, World!' in template.render(prefix='Hi')
assert assertion_results 
assertion_results = 'Hello, World!' in template.render()
assert assertion_results 
",jinja2.pass_context,1.0,"jinja2.pass_context
def greet(ctx, name):
    prefix = ctx.get('prefix', 'Hello')
    return f'{prefix}, {name}!'
    
",,3.1,"
import jinja2 
def setup_environment(filtername,filter):
    env = jinja2.Environment()
    env.filters[filtername] = filter
    return env

@","
Create a custom filter named greet. This filter should: 
accept the context and a parameter name, 
retrieve a variable prefix from the context (defaulting to 'Hello' if not provided), 
return a greeting message combining the prefix and the name.",jinja2,,name change
2020-01,"
env = Environment(autoescape=True)
output = get_output(env,nl2br)
expected = '<br>Hello</br> World'

assert output == expected, f'Expected: {expected!r}, but got: {output!r}'
",jinja2.evalcontextfilter,1.0,"evalcontextfilter
def nl2br(eval_ctx, value):
    return nl2br_core(eval_ctx, value)
",,2.11,"
import re
from jinja2 import Environment, evalcontextfilter
from markupsafe import Markup, escape

def get_output(env, filter_fn):
    env.filters['nl2br'] = filter_fn
    template = env.from_string('{{ text | nl2br }}')
    output = template.render(text='Hello World')
    return output

def nl2br_core(eval_ctx, value):
    br = '<br>Hello</br>'
    if eval_ctx.autoescape:
        value = escape(value)
        br = Markup(br)
    result = re.sub(r'Hello', br, value)
    return Markup(result) if eval_ctx.autoescape else result

@","
Write the nl2br function which is a custom filter that takes the evaluation context 
and a text value, then replaces every occurrence of the substring 'Hello' 
with the HTML string '<br>Hello</br>', while respecting the autoescape setting.
You can use the nl2br_core function",jinja2,markupsafe==2.0.1,name change
2022-03,"
env = Environment(autoescape=True)
output = get_output(env,nl2br)
expected = '<br>Hello</br> World'

assert output == expected, f'Expected: {expected!r}, but got: {output!r}'
",jinja2.pass_eval_context,1.0,"pass_eval_context
def nl2br(eval_ctx, value):
    return nl2br_core(eval_ctx, value)
",,3.1,"
import re
from jinja2 import Environment, pass_eval_context
from markupsafe import Markup, escape

def get_output(env, filter_fn):
    env.filters['nl2br'] = filter_fn
    template = env.from_string('{{ text | nl2br }}')
    output = template.render(text='Hello World')
    return output

def nl2br_core(eval_ctx, value):
    br = '<br>Hello</br>'
    if eval_ctx.autoescape:
        value = escape(value)
        br = Markup(br)
    result = re.sub(r'Hello', br, value)
    return Markup(result) if eval_ctx.autoescape else result

@","
Write the nl2br function which is a custom filter that takes the evaluation context 
and a text value, then replaces every occurrence of the substring 'Hello' 
with the HTML string '<br>Hello</br>', while respecting the autoescape setting.
You can use the nl2br_core function",jinja2,,name change
2023-06,"
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]]
])
assertion_value = check_invertibility(matrices)
assert assertion_value
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]],

    [[0, 0],
     [0, 0]]
])
assertion_value = not check_invertibility(matrices)
assert assertion_value",linalg.det,2.0,"
    return np.all(det(matrices))
",,1.11.1,"
import warnings
from scipy.linalg import det
import numpy as np
warnings.filterwarnings('error')

def check_invertibility(matrices):","
complete the following function that check if all the batch of matrices are invertible, using numpy 1.25.1",scipy,numpy==1.25.1,argument or attribute change
2022-10,"
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]]
])
assertion_value = check_invertibility(matrices)
assert assertion_value
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]],

    [[0, 0],
     [0, 0]]
])
assertion_value = not check_invertibility(matrices)
assert assertion_value",linalg.det,2.0,"
    return np.alltrue([det(A) for A in matrices])
",,1.9.1,"
import warnings
from scipy.linalg import det
import numpy as np
warnings.filterwarnings('error')

def check_invertibility(matrices):","
complete the following function that check if all the batch of matrices are invertible, using numpy 1.21.6",scipy,numpy==1.21.6,argument or attribute change
2023-06,"
data = np.array([
    [1, 2, 3],
    [2, 2, 2],
    [1, np.nan, 3],
    [4, 5, 6],
    [np.nan, 1, np.nan],
    [1, 2, 3]
])
assertion_value = count_unique_hmean(data) == 5
assert assertion_value
",stats.hmean,2.0,"
    hmean_values = hmean(np.asarray(data), axis=1)
    unique_vals = np.unique(hmean_values, equal_nan=False).shape[0]
    return unique_vals

",,1.11.1,"
import numpy as np
from scipy.stats import hmean

def count_unique_hmean(data):
    # data shape: (n, m)
    # n: number of arrays
    # m: number of elements in each array ","
Complete the function count_unique_hmean that takes a 2D numpy array as 
input and returns the number of unique harmonic mean values across 
the rows of the array, counting each nan value as unique. We are using numpy 1.25.1",scipy,numpy==1.25.1,output change
2022-05,"

data = np.array([
    [1, 2, 3],
    [2, 2, 2],
    [1, np.nan, 3],
    [4, 5, 6],
    [np.nan, 1, np.nan],
    [1, 2, 3]
])
assertion_value = count_unique_hmean(data) == 5
assert assertion_value
",stats.hmean,2.0,"
    hmean_values = []
    for arr in data:
        if np.isnan(arr).any():
            hm = np.nan
        else:
            hm = hmean(arr)
        hmean_values.append(hm)
    
    hmean_values = np.asarray(hmean_values)
    non_nan_vals = hmean_values[~np.isnan(hmean_values)]
    counts_non_nan = np.unique(non_nan_vals).shape[0]
    nan_count = np.sum(np.isnan(hmean_values))
    return counts_non_nan + nan_count
",,1.8.1,"

import numpy as np
from scipy.stats import hmean

def count_unique_hmean(data):
    # data shape: (n, m)
    # n: number of arrays
    # m: number of elements in each array","
Complete the function count_unique_hmean that takes a 2D numpy array as 
input and returns the number of unique harmonic mean values across 
the rows of the array, counting each nan value as unique. We are using numpy 1.21.6",scipy,numpy==1.21.6,output change
2023-06,"

a = np.array([1.0, 2.0, 3.0], dtype=np.float32)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
assertion_value = False
try :
    compute_hilbert_transform(a, b, dtype=np.float32)
except TypeError:
    assertion_value = True
assert assertion_value
b=b.astype(np.float32)
computed = compute_hilbert_transform(a, b, dtype=np.float32)
expected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)
assert assertion_value
a=a.astype(np.float64)
b=b.astype(np.float64)
computed = compute_hilbert_transform(a, b, dtype=np.float64)
expected = expected.astype(dtype=np.complex128)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)
assert assertion_value
",signal.hilbert,2.0,"
    stacked = np.vstack((a, b), dtype=dtype,
                         casting='safe')
    return hilbert(stacked)
",,1.11.1,"
import numpy as np
from scipy.signal import hilbert

def compute_hilbert_transform(a, b, dtype=np.float64):
    # compute_hilbert_transform should return the Hilbert transform of the
    # a and b arrays stacked vertically, with safe casting and the specified
    # dtype. 
    # raise TypeError if needed
    ","
Complete the function compute_hilbert_transform. We are using numpy 1.25.1",scipy,numpy==1.25.1,argument or attribute change
2022-05,"
a = np.array([1.0, 2.0, 3.0], dtype=np.float32)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
assertion_value = False
try :
    compute_hilbert_transform(a, b, dtype=np.float32)
except TypeError:
    assertion_value = True
assert assertion_value
b=b.astype(np.float32)
computed = compute_hilbert_transform(a, b, dtype=np.float32)
expected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)
assert assertion_value

a=a.astype(np.float64)
b=b.astype(np.float64)
computed = compute_hilbert_transform(a, b, dtype=np.float64)
expected = expected.astype(dtype=np.complex128)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)
assert assertion_value
",signal.hilbert,2.0,"
    if not (np.can_cast(a.dtype, dtype, casting='safe') and np.can_cast(b.dtype, dtype, casting='safe')):
        raise TypeError('Unsafe casting from input dtype to specified dtype')
    
    a_cast = a.astype(dtype, copy=False)
    b_cast = b.astype(dtype, copy=False)
    
    stacked = np.vstack((a_cast, b_cast))
    
    result = hilbert(stacked)
    
    if dtype == np.float32:
        complex_dtype = np.complex64
    elif dtype == np.float64:
        complex_dtype = np.complex128
    else:
        complex_dtype = np.complex128

    return result.astype(complex_dtype)
    ",,1.8.1,"
import numpy as np
from scipy.signal import hilbert

def compute_hilbert_transform(a, b, dtype=np.float64):
    # compute_hilbert_transform should return the Hilbert transform of the
    # a and b arrays stacked vertically, with safe casting and the specified
    # dtype.
    # raise TypeError if needed
    ","
Complete the function compute_hilbert_transform. We are using numpy 1.25.1",scipy,numpy==1.21.6,argument or attribute change
2021-05,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))
assert assertion_results
",app.json_encoder,2.0,"
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
",,2.0.0,"
import flask
import json
import numpy as np
app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):"," 
Complete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained 
in the numpy array when called, we are using numpy 1.21.6",flask,numpy==1.21.6 werkzeug==2.0.0,argument or attribute change
2023-09,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_results = eval_app(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval_app(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))
assert assertion_results
",app.json_encoder,2.0,"
        if isinstance(obj, np.ndarray):
            unique_vals = np.unique(obj, equal_nan=False)
            return unique_vals.tolist()
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",,3.0.0,"
import flask
import numpy as np

app = flask.Flask('test1')

@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
    "," 
Complete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained 
in the numpy array when called, we are using numpy 1.25.1",flask,numpy==1.25.1,argument or attribute change
2021-05,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))
assert assertion_results
",app.json_encoder,2.0,"
            res = fastCopyAndTranspose(obj).flatten().tolist()
            return res
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler

",,2.0.0,"
import flask
import json
import numpy as np
from numpy import fastCopyAndTranspose 
app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):"," 
Complete the app set-up for the json encoding to perform a fast copy and 
transpose when given a numpy array before flattening and converting 
the result to a list,we are using numpy 1.21.6",flask,numpy==1.21.6 werkzeug==2.0.0,argument or attribute change
2023-09,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_results = eval_app(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval_app(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))
assert assertion_results
",app.json_encoder,2.0,"
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",,3.0.0,"
import flask
import numpy as np
import warnings
from numpy import fastCopyAndTranspose 
warnings.filterwarnings('error')

app = flask.Flask('test1')

@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):"," 
Complete the app set-up for the json encoding to perform a fast copy and 
transpose when given a numpy array before flattening and converting 
the result to a list, we are using numpy 1.25.1",flask,numpy==1.25.1,argument or attribute change
2021-05,"

base_path = '/var/www/myapp'
sub_path = '../secret.txt'
a = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
b = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)
arr_list = [a, b]
casting_policy = 'safe'
out_dtype=np.float64
stacked_correct = np.vstack(arr_list).astype(np.float64)
try : 
    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
try : 
    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except TypeError as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
casting_policy = 'unsafe'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)

assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32
assert assertion_result
",flask.safe_join,2.0,"
    joined = flask.safe_join(base_path, sub_path)
    casted_list = []

    for arr in arr_list:
        if not np.can_cast(arr.dtype, out_dtype, casting=casting_policy):
            raise TypeError('Cannot cast array')
        casted_list.append(arr.astype(out_dtype, copy=False))
    
    stacked = np.vstack(casted_list)
    return joined, stacked
",,2.0.0,"
import flask
import werkzeug
import numpy as np

error404 = werkzeug.exceptions.NotFound

def stack_and_save(arr_list,base_path,sub_path, casting_policy,out_dtype):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # stack the arrays in arr_list with the casting policy and the out_dtype.
    # if the out_dtype is not compatible with the casting policy, raise a TypeError
    # and out_dtype could be np.float32 or np.float64
    # casting policy could be safe or unsafe
    # Return the joined path and the stacked array to be saved "," 
Complete the stack_and_save function, we are using numpy 1.21.6",flask,numpy==1.21.6 werkzeug==2.0.0,name change
2023-09,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'


a = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
b = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)
arr_list = [a, b]
casting_policy = 'safe'
out_dtype=np.float64
stacked_correct = np.vstack(arr_list).astype(np.float64)
try : 
    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
try : 
    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except TypeError as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
casting_policy = 'unsafe'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)

assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32
assert assertion_result
",flask.safe_join,2.0,"
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    stacked = np.vstack(arr_list,casting=casting_policy,dtype=out_dtype)
    return joined, stacked
",,3.0.0,"
import flask
import werkzeug
import numpy as np

error404 = werkzeug.exceptions.NotFound

def stack_and_save(arr_list,base_path,sub_path, casting_policy,out_dtype):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # stack the arrays in arr_list with the casting policy and the out_dtype.
    # if the out_dtype is not compatible with the casting policy, raise a TypeError
    # and out_dtype could be np.float32 or np.float64
    # casting policy could be safe or unsafe
    # Return the joined path and the stacked array to be saved "," 
Complete the stack_and_save function, we are using numpy 1.25.1",flask,numpy==1.25.1,name change
2023-09,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)
a = np.random.random((6,3,3))

assertion_results = eval_app(app2, data2,a) == eval_app(app, data,a)
assert assertion_results",app.json_encoder,2.0,"

            res = linalg.det(obj)
            return res.tolist()
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app) ",,3.0.0,"
import flask
import numpy as np
from scipy import linalg

app = flask.Flask('test1')
@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})
def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] : "," 
Complete the app set-up so that, when given a batch of matrix,
the json encoding compute the determinants of each matrix, 
before flattening and converting the result to a list, we are using scipy 1.11.1",flask,scipy==1.11.1,argument or attribute change
2021-05,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
a = np.random.random((6,3,3))

assertion_results = eval(app2, data2,a) == eval(app, data,a)
assert assertion_results
",app.json_encoder,2.0,"
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
",,2.0.0," 
import flask
import json
import numpy as np
from scipy import linalg

app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :"," 
Complete the app set-up so that, when given a batch of matrix,
the json encoding compute the determinants of each matrix, 
before flattening and converting the result to a list, we are using scipy 1.8.1 ",flask,scipy==1.8.1 Werkzeug==2.0.0,argument or attribute change
2023-09,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = hmean(obj,axis=1).tolist()
            return res
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)
assertion_results = eval_app(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval_app(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))
assert assertion_results

",app.json_encoder,2.0,"
            res = hmean(obj,axis=1).tolist()
            return res
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",,3.0.0,"
import flask
import numpy as np
from scipy.stats import hmean

app = flask.Flask('test1')

@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):"," 
Complete the app set-up so that, when given a batch (first dim) of values,
compute the harmonic mean along the second dimension (It should handle nan values),
before flattening and converting the result to a list, we are using scipy 1.11.1",flask,scipy==1.11.1,argument or attribute change
2021-05,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = np.zeros((obj.shape[0],1))
            for i_arr in range(obj.shape[0]):
                if np.isnan(obj[i_arr]).any():
                    res[i_arr] = np.nan
                else:
                    res[i_arr]  = hmean(obj[i_arr])
            res = res.flatten().tolist()
            return res
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))
assert assertion_results
",app.json_encoder,2.0,"
            res = np.zeros((obj.shape[0],1))
            for i_arr in range(obj.shape[0]):
                if np.isnan(obj[i_arr]).any():
                    res[i_arr] = np.nan
                else:
                    res[i_arr]  = hmean(obj[i_arr])
            res = res.flatten().tolist()
            return res
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
",,2.0.0,"
import flask
import json
import numpy as np
from scipy.stats import hmean

app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):"," 
Complete the app set-up so that, when given a batch (first dim) of values,
compute the harmonic mean along the second dimension (It should handle nan values),
before flattening and converting the result to a list, we are using scipy 1.8.1",flask,scipy==1.8.1 Werkzeug==2.0.0,argument or attribute change
2023-09,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'
import numpy as np

a = np.random.random((4,3,3))
expected = np.zeros(a.shape)
for i in range(expected.shape[0]):
    expected[i] = linalg.expm(a[i])

try : 
    joined, results = save_exponential(a,base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'

joined, results = save_exponential(a,base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)
assert assertion_result
",flask.safe_join,2.0,"
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    output = linalg.expm(A)
    return joined, output
",,3.0.0,"
import flask
import werkzeug
from scipy import linalg

error404 = werkzeug.exceptions.NotFound

def save_exponential(A, base_path, sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # compute the exponential of the batched matrices (m, m) in A (n,m,m)
    # return the save_path and the exponential of the matrices
    "," 
Complete the save_exponential function, we are using scipy 1.11.1",flask,scipy==1.11.1,name change
2021-05,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'
import numpy as np

a = np.random.random((4,3,3))
expected = np.zeros(a.shape)
for i in range(expected.shape[0]):
    expected[i] = linalg.expm(a[i])

try : 
    joined, results = save_exponential(a,base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'

joined, results = save_exponential(a,base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)
assert assertion_result
",flask.safe_join,2.0,"
    joined = flask.safe_join(base_path, sub_path)
    output = np.zeros(A.shape)
    for i in range(A.shape[0]):
        output[i] = linalg.expm(A[i])
    return joined, output
",,2.0.0," 
import flask
import werkzeug
from scipy import linalg

error404 = werkzeug.exceptions.NotFound

def save_exponential(A, base_path, sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # compute the exponential of the batched matrices (m, m) in A (n,m,m)
    # return the save_path and the exponential of the matrices
    "," 
Complete the save_exponential function, we are using scipy 1.8.1",flask,scipy==1.8.1 Werkzeug==2.0.0,name change
2021-06 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

def test_custom_generateRandomSampleDice():
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(""always"", SymPyDeprecationWarning)  # Capture all warnings
        output = custom_generateRandomSampleDice(X)
        expect = [sample(X) for i in range(3)]
        assert isinstance(output, list), ""Test Failed: Output is not a list!""
        assert len(output) == len(expect), ""Test Failed: Output length does not match expected!""
        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""

test_custom_generateRandomSampleDice()",stats.sample,,[sample(X) for i in range(3)],,1.9,"    
from sympy.stats import Die, sample
X = Die('X', 6)
def custom_generateRandomSampleDice(X):
    return ","Generate random samples from a six-sided die, return a list.",sympy,scipy,new func/method/class
2021-06 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

def test_custom_computeDFT():
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(""always"", SymPyDeprecationWarning)  # Capture all warnings
        output = custom_computeDFT(4)
        expect = DFT(4).as_explicit()
        assert output == expect
        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""

test_custom_computeDFT()",sympy.physics.matrices.mdft,,DFT(n).as_explicit(),,1.9,"
from sympy.matrices.expressions.fourier import DFT

def custom_computeDFT(n):
    return ",Compute the Discrete Fourier Transform (DFT) matrix of size  n \times n and show it explicitly.,sympy,,new func/method/class
2021-06 ,"from sympy import Matrix
expected = (Matrix([
    [1/z,   0],
    [  0, 1/z]
]), 0, True)
assert output == expected
",laplace_transform,,"laplace_transform(eye(2), t, z, legacy_matrix=False)",,1.9,"from sympy import laplace_transform, symbols, eye
t, z = symbols('t z')
output = ","Compute the Laplace transform of the 2 \times 2 identity matrix I_2 (i.e., eye(2)) with respect to t and return a single tuple. The first element of the tuple should be the transformed matrix, and the second element should be the combined convergence condition. Store the results in a variable named output.
",sympy,,new func/method/class
2022-03 ,"from sympy.physics.quantum.trace import Tr
expect = Tr(2)
assert custom_trace(2) == expect",trace,,S.physics.quantum.trace.Tr(n),,1.11,"import sympy as S
import sympy.physics.quantum
def custom_trace(n):
    return ",Instantiate the trace object with the number n return the resulting trace object.,sympy,,breaking change
2022-03 ,"expect = [7]
assert list(custom_preorder_traversal(expr)) == expect",preorder_traversal,,sympy.preorder_traversal(expr),,1.11,"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_preorder_traversal(expr):
    return ",Instantiate the preorder_traversal object with existing expression.,sympy,,breaking change
2022-08 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 24
    assert custom_parse_mathematica(expr) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",parse_mathematica,,"parse_mathematica(expr).replace(Function(""F""), lambda *x: Max(*x)*Min(*x))",,1.11,"from sympy.parsing.mathematica import parse_mathematica
from sympy import Function, Max, Min

expr = ""F[6,4,4]""
def custom_parse_mathematica(expr):
    return ",Let F to be a function that returns the maximum value multiplied by the minimum value.,sympy,,new func/method/class
2023-05 ,"expect1 = parent.frame.x
expect2 = -child.frame.x

assert pin.parent_point.pos_from(parent.masscenter) == expect1
assert pin.child_point.pos_from(child.masscenter) == expect2",sympy.physics.mechanics.PinJoint,,"PinJoint('pin', parent, child, parent_point=parent.frame.x,child_point=-child.frame.x)",,1.12,"from sympy.physics.mechanics import Body, PinJoint
parent, child = Body('parent'), Body('child')
pin = ","Instantiate a PinJoint in the parent to be positioned at parent.frame.x with respect to the mass center, and in the child at -child.frame.x. Store the results in a variable named pin.",sympy,,argument change	
2023-05 ,"
assert isinstance(pin.coordinates, sp.Matrix)
assert isinstance(pin.speeds, sp.Matrix)",sympy.physics.mechanics,,"PinJoint('pin', parent, child)",,1.12,"import sympy as sp
from sympy.physics.mechanics import Body, PinJoint

parent, child = Body('parent'), Body('child')
pin = ",Instantiate a PinJoint that connects a parent body to a child body. Store the results in a variable named pin.,sympy,,output behaviour
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy import is_carmichael
    expect = is_carmichael(561)
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","sympy.functions.combinatorial.numbers.carmichael.is_carmichael
",,is_carmichael(561),,1.13,"from sympy import *

output = ","Check number 561 is carmichael or not, return bool value.  Store the results in a variable named output.",sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 12
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.ntheory.factor_.divisor_sigma,,"divisor_sigma(6, 1)",,1.13,"from sympy import *

output = ",Compute the sum of the divisors of the integer 6. Store the results in a variable named output.,sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 2
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",ModularInteger.to_int(),,K.to_int(a),,1.13,"from sympy import GF
K = GF(6)
a = K(8)
output = ","Consider the field of integers modulo 6, create an element corresponding to the number 8, and then convert this modular element into its standard integer representation. Store the results in a variable named output.",sympy,,new func/method/class
2023-07 ,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy.physics.mechanics import inertia
    expect = inertia(N, Ixx, Iyy, Izz)
    assert custom_generateInertia(N, Ixx, Iyy, Izz) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.physics.mechanics,,"physics.mechanics import inertia
    return inertia(N, Ixx, Iyy, Izz)",,1.13,"from sympy import symbols
from sympy.physics.mechanics import ReferenceFrame
N = ReferenceFrame('N')
Ixx, Iyy, Izz = symbols('Ixx Iyy Izz')

def custom_generateInertia(N, Ixx, Iyy, Izz):
    from sympy.","Create a custom function that accepts an input and returns a symbolic representation of a rigid body’s inertia tensor relative to a specified reference frame, constructed using the symbolic variables for its principal moments of inertia. Based on the provided code, give the complete function and import the necessary libraries.",sympy,,new func/method/class
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = eq.lhs - eq.rhs
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Eq.rewrite(Add),,eq.lhs - eq.rhs,,1.13,"from sympy import *

x, y = symbols('x y')
eq = Eq(x, y)

output = ","Use two defined given symbols and an equality between them, then compute the difference between two symbols.",sympy,,new func/method/class
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = [1,2,3]
    assert custom_generatePolyList(p) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",DMP.rep attribute,,p.rep.to_list(),,1.13,"from sympy import symbols, Poly
x = symbols('x')
p = Poly(x**2 + 2*x + 3)

def custom_generatePolyList(poly):
    return ",Create a custom function that accepts a polynomial object and returns a list of its coefficients based on its internal representation.,sympy,,new func/method/class
2023-07 ,"from sympy import symbols, Function, Derivative, Matrix, sin, cos
t = symbols('t')
l, Pendulum_mass, cart_mass, Pendulum_izz = symbols('l Pendulum_mass cart_mass Pendulum_izz')

q_j = Function('q_j')
u_j = Function('u_j')
u_s = Function('u_s')
M = Matrix([
    [Pendulum_mass*l*u_j(t)**2*sin(q_j(t)) - Pendulum_mass*l*cos(q_j(t))*Derivative(u_j(t), t)
     - (Pendulum_mass + cart_mass)*Derivative(u_s(t), t)],
    [-Pendulum_mass*l*cos(q_j(t))*Derivative(u_s(t), t)
     - (Pendulum_izz + Pendulum_mass*l**2)*Derivative(u_j(t), t)]
])
assert custom_motion(wall,slider, pin) == M",sympy.physics.mechanics.JointsMethod,,"System
    system = System.from_newtonian(wall)
    system.add_joints(slider, pin)
    return system.form_eoms()
",,1.13,"from sympy import symbols
from sympy.physics.mechanics import (
  Particle, PinJoint, PrismaticJoint, RigidBody)
l = symbols(""l"")
wall = RigidBody(""wall"")
cart = RigidBody(""cart"")
pendulum = RigidBody(""Pendulum"")
slider = PrismaticJoint(""s"", wall, cart, joint_axis=wall.x)
pin = PinJoint(""j"", cart, pendulum, joint_axis=cart.z,
               child_point=l * pendulum.y)

def custom_motion(wall,slider, pin):
    from sympy.physics.mechanics import ","Using Sympy’s mechanics module, create a mechanical system involving three rigid bodies—a wall, a cart, and a pendulum. The wall serves as the inertial (fixed) reference, while the cart is connected to the wall by a sliding joint along the wall’s x-axis. The pendulum is attached to the cart via a rotational joint about the cart’s z-axis, with its connection point offset by a symbolic distance along the pendulum’s y-axis. Write a custom function that sets up this system using the Newtonian formulation (with the wall as the inertial frame), adds the slider and pin joints, and returns the system’s equations of motion. What are the equations of motion generated by your function?",sympy,,new func/method/class
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    exp1, exp2 = custom_body(rigid_body_text, particle_text)
    assert exp1.name == rigid_body_text
    assert exp2.name == particle_text
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.physics.mechanics.Body,,"RigidBody(rigid_body_text), Particle(particle_text)",,1.13,"from sympy.physics.mechanics import *
rigid_body_text = ""rigid_body""
particle_text = ""particle""

def custom_body(rigid_body_text, particle_text):
    return ","Using the SymPy's mechanics module, define a function that takes two strings as inputs—one representing the name of a rigid body and the other representing the name of a particle—and returns a tuple containing a rigid body and a particle created with those names. 
",sympy,,new func/method/class
2021-06 ,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = a.free_symbols
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",expr_free_symbols,,a.free_symbols,,1.9,"from sympy import Indexed

a = Indexed(""A"", 0)
output = ","Get the given set of free symbols, store the results in output variable.",sympy,,new func/method/class
2021-06 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expected_shape = (2, 2)
    expected_content = [[1, 2], [3, 4]]
    output = custom_create_matrix(first, second)

    assert output.shape == expected_shape

    assert output.tolist() == expected_content
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.polys.solvers.RawMatrix,,"Matrix([first, second])",,1.9,"from sympy import Matrix

first = [1,2]
second =[3,4]
def custom_create_matrix(first,second):
    return ",Write a custom function named custom_create_matrix that takes two lists (first and second) and returns a matrix composed of these two lists as rows.,sympy,,new func/method/class
2021-06 ,"

output = custom_function(m)
output[0] = 100

assert m[0, 0] == 1
assert output[0] == 100

",DenseMatrix._flat,,matrix.flat(),,1.9,"from sympy import Matrix

m = Matrix([[1, 2], [3, 4]])
    
def custom_function(matrix):
    return ","Write a custom function that accepts a Sympy Matrix and returns a flat, read-only copy of its data ",sympy,,new func/method/class
2021-06 ,"output = custom_function(m)
output[(0, 0)] = 100

assert m[0, 0] == 1
assert output[(0, 0)] == 100


",SparseMatrix._todok,,matrix.todok(),,1.9,"from sympy import Matrix

m = Matrix([[1, 2], [3, 4]])
    
def custom_function(matrix):
    return ",Write a custom function that accepts a Sympy SparseMatrix and returns its dictionary-of-keys representation.,sympy,,new func/method/class
2022-03 ,"expect = 7

assert custom_bottom_up(expr) == expect",sympy.bottom_up,,"sympy.bottom_up(expr, lambda x: x.doit())",,1.1,"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_bottom_up(expr):
    return ",Write a custom function named custom_bottom_up that applies a bottom-up traversal to expr with a lambda function on each node.,sympy,,breaking change
2022-03 ,"
expect = 7

assert custom_use(expr) == expect",sympy.use,," sympy.use(expr, lambda x: x.doit())",,1.1,"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_use(expr):
    return ","Write a custom function that traverses the expression so that every subexpression is evaluated, and returns final evaluated result. ",sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_is_perfect_square(4)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",carmichael.is_perfect_square,,sympy.ntheory.primetest.is_square(n),,1.11,"import sympy

def custom_is_perfect_square(n):
    return ",Write a custom function that check if the input is a perfect square.,sympy,,new func/method/class
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_is_prime(13)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",carmichael.is_prime,,sympy.isprime(n),,1.11,"import sympy

def custom_is_prime(n):

    return ",Write a custom function that check if the input is prime,sympy,,new func/method/class
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_divides(10,2)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",carmichael.divides,,n % p == 0,,1.11,"import sympy

def custom_divides(n, p):

    return ","Write a custom function that checks whether the integer p divides the integer n evenly (i.e., with no remainder).",sympy,,new func/method/class
2023-05 ,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy.tensor.array.expressions.from_array_to_matrix import convert_array_to_matrix

    expect = convert_array_to_matrix(array_expr)
    output = custom_array_to_matrix(array_expr)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.tensor.array.expressions.conv_*,,"from_array_to_matrix import convert_array_to_matrix
    return convert_array_to_matrix(array)
",,1.12,"from sympy import Matrix, symbols, Array

a1, a2, a3, a4 = symbols('a1 a2 a3 a4')
array_expr = Array([[a1, a2], [a3, a4]])

def custom_array_to_matrix(array):
    from sympy.tensor.array.expressions.",Write a custom function that convert input array into matrix by import correct sympy modules.,sympy,,breaking change
2023-07 ,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_jacobi_symbols(1001, 9907)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","sympy.ntheory.residue_ntheory.jacobi_symbol
",,"sympy.jacobi_symbol(a, n)",,1.13,"
import sympy

def custom_jacobi_symbols(a, n):
    return ",Write a custom function that compute the Jacobi symbol (a/n).,sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 7
    output = custom_npartitions(5)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.partitions_.npartitions,,sympy.functions.combinatorial.numbers.partition(n),,1.13,"
import sympy

def custom_npartitions(n):
    return ",Write a custom function that compute the number of partitions of n.,sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 3
    output = custom_primefactors(18)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","sympy.ntheory.factor_.primeomega
",,sympy.primeomega(n),,1.13,"
import sympy

def custom_primefactors(n):
    return ",Write a custom function that compute the number of distinct prime factors of n.,sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 10
    output = custom_prime_counting(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.ntheory.generate.primepi,,"sympy.primepi(n)
",,1.13,"
import sympy

def custom_prime_counting(n):
    return ",Write a custom function that compute the prime counting function for n.,sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 8
    output = custom_totient(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.ntheory.factor_.totient,,sympy.totient(n),,1.13,"
import sympy

def custom_totient(n):
    return ",Write a custom function that compute Euler's totient function for n (number of integers relatively prime to n).,sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_mobius(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",sympy.ntheory.residue_ntheory.mobius,,sympy.mobius(n),,1.13,"import sympy

def custom_mobius(n):
    return ",Write a custom function that compute the Möbius function for n.,sympy,,breaking change
2023-07 ,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_legendre(200, 13)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","sympy.ntheory.residue_ntheory.legendre_symbol
",,"sympy.legendre_symbol(a, n)",,1.13,"
import sympy

def custom_legendre(a, n):
    return ",Write a custom function that compute the Legendre symbol (a/p).,sympy,,breaking change
2023-09 ,"

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_pointplot(data)
    
    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]

    if any(""dataframegroupby.apply"" in msg for msg in warning_messages):
        pass  
    elif not any(""deprecated"" in msg and ""removed"" in msg for msg in warning_messages):
        raise AssertionError(""Expected deprecation warning was not raised."")

    for line in output.lines:
        if line.get_linestyle() != ""None"":
            raise AssertionError(""Linestyle is not set to 'none' as expected."")
        break",seaborn.pointplot(),,"sns.pointplot(x='x', y='y', data=data, markers=""o"", linestyles=""none"")",,0.13.0,"import seaborn as sns
import pandas as pd


data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})

def custom_pointplot(data):
    return ","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, with remove connecting lines",seaborn,panda,new func/method/class
2023-09 ,"
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_pointplot(data)
    
    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]

    if any(""dataframegroupby.apply"" in msg for msg in warning_messages):
        pass  
    elif not any(""deprecated"" in msg and ""removed"" in msg for msg in warning_messages):
        raise AssertionError(""Expected deprecation warning was not raised."")
    
    found_correct_linewidth = False
    for line in output.lines: 
        linewidth = line.get_linewidth()
        if linewidth == 2:
            found_correct_linewidth = True
            break

    if not found_correct_linewidth:
        raise AssertionError(""Error bar linewidth is not set to 2 as expected."")",seaborn.pointplot(),,"sns.pointplot(x='x', y='y', data=data, err_kws={""linewidth"": 2})",,0.13.0,"import seaborn as sns
import pandas as pd


data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})

def custom_pointplot(data):
    return ","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, adjust error bar width to 2.",seaborn,panda,new func/method/class
2023-09 ,"
import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_violinplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""bw"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""bw parameter should not be used. Use bw_method and bw_adjust instead."")

    for collection in output.collections:
            if hasattr(collection, ""get_paths""):
                assert sns.violinplot.__defaults__[0] == 1.5
    ",seaborn.violinplot,,"sns.violinplot(x='x', y='y', data=data, bw_adjust=1.5)",,0.13.0,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_violinplot(data):
    return ","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, scales the bandwidth to 1.5.",seaborn,panda,new func/method/class
2023-09 ,"
import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_violinplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""bw"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""bw parameter should not be used. Use bw_method and bw_adjust instead."")
    
    collections = [c for c in output.collections if isinstance(c, plt.Line2D)]  # Extract violin plot lines
    
    assert output is not None, ""Violin plot output should not be None.""",seaborn.violinplot,,"sns.violinplot(x='x', y='y', data=data, bw_method=""scott"")",,0.13.0,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_violinplot(data):
    return ","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, choose bandwidth to scott.",seaborn,panda,new func/method/class
2023-09 ,"
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    ax = custom_barplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""errcolor"" in msg or ""errwidth"" in msg for msg in warning_messages):
        raise AssertionError(""errcolor and errwidth should not be used. Use err_kws instead."")

    for line in ax.lines:
        if line.get_linewidth() == 2 and line.get_color() == 'red':
            break
    else:
        raise AssertionError(""Error bars are not set with err_kws correctly."")",seaborn.barplot(),,"sns.barplot(x='x', y='y', data=data, err_kws={'color': 'red', 'linewidth': 2})",,0.13.0,"import seaborn as sns
import pandas as pd

import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_barplot(data):
    return ","Write a Seaborn barplot() function that visualizes x and y from a Pandas DataFrame, adjust error bar with color red and linewidth 2.",seaborn,panda,new func/method/class
2023-09 ,"
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_boxenplot(data)

    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""scale"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""scale should not be used in boxenplot. Use width_method instead."")

    for artist in output.get_children():
        if hasattr(artist, ""get_linestyle"") and artist.get_linestyle() in [""-"", ""--""]:
            break
    else:
        raise AssertionError(""Boxen elements are missing, width_method might not be applied."")",seaborn.boxenplot(),,"sns.boxenplot(x='x', y='y', data=data, width_method='exponential')",,0.13.0,"import seaborn as sns
import pandas as pd
import warnings
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_boxenplot(data):
    return ","Write a Seaborn boxenplot() function that visualizes x and y from a Pandas DataFrame, make width method to exponential.",seaborn,panda,argument change
2022-09 ,"
ax = custom_set_axis_labels(data)
x_expect = ""My X Label""
y_expect = ""My Y Label""
assert ax.get_xlabel() == x_expect and ax.get_ylabel() == y_expect, (
    ""Axis labels not set correctly using ax.set().""
)
",seaborn.scatterplot().set(),,"set(xlabel=""My X Label"", ylabel=""My Y Label"")
    return ax",,0.12.0,"import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

data = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})

def custom_set_axis_labels(data):
    ax = sns.scatterplot(x='x', y='y', data=data)
    ax.","Write a custom function that visualizes x and y from a Pandas DataFrame, set the X label to be ""My X Label"" and Y label to be ""My Y Label"".",seaborn,panda,argument change
2022-09 ,"computed_iqr = custom_iqr(data_array)
expect = 2
assert computed_iqr == expect
",seaborn.iqr(),,"scipy.stats import iqr
    return iqr(data)
",,0.12.0,"import numpy as np

data_array = np.array([1, 2, 3, 4, 5])

def custom_iqr(data):
    from ","Write a custom function to compute iqr for input data. If needed, use another library.",seaborn,scipy,new func/method/class
2022-11 ,"expect_peername = (""127.0.0.1"", 111)
expect_sockname = (""127.0.0.1"", 222)

assert output_client.peername == expect_peername
assert output_client.sockname == expect_sockname
",mitmproxy.connection.Client,," conn.Client(
    peername=(ip_address, i_port),
    sockname=(ip_address, o_port),
    timestamp_start=time.time()
)",,9.0.1,"import time
import mitmproxy.connection as conn

ip_address = ""127.0.0.1""
i_port = 111
o_port = 222

output_client = ","Using mitmproxy’s connection API, create a client connection by specifying the IP address (ip_address), the input port (i_port), the output port (o_port), and the timestamp (using time.time() for the current time). Store the resulting client connection in the variable output_client.",mitmproxy,,argument change
2022-11 ,"expect = (""192.168.1.1"", 80)
assert output_server.address == expect",mitmproxy.connection.Server,," conn.Server(
    address=(ip_address, server_port)
)",,9.0.1,"import mitmproxy.connection as conn

ip_address = ""192.168.1.1""
server_port = 80

output_server = ","Using mitmproxy’s connection API, create a server connection with the given parameters: an IP address (ip_address), a server port (server_port), and store the resulting instance in the variable output_server. ",mitmproxy,,argument change
2021-07 ,"        
import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_connected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_connected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()","server_connected
",,"server_connected(self, server_conn):
        print(f""Server connected with local address {server_conn.sockname}"")",,7.0.0,"import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the server connected event hook. When a server connection is established, the implemented method should be called with a parameter server_conn, and it must print the server’s local address using the attribute server_conn.sockname in the following format: Server connected with local address {server_conn.sockname}.",mitmproxy,,argument change
2021-07 ,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_connect(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_connect(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",server_connect,,"server_connect(self, server_conn):
        print(f""Server connect : {server_conn.sockname}"")",,7.0.0,"import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the server connect event hook. When a server connection is established, the implemented method should be called with a parameter server_conn, and it must print the server’s local address using the attribute server_conn.sockname in the following format: Server connect {server_conn.sockname}.",mitmproxy,,new func/method/class
2021-07 ,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_disconnected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_disconnected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()","server_disconnected
",,"server_disconnected(self, server_conn):
        print(f""Server disconnected: {server_conn.sockname}"")",,7.0.0,"import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the server disconnected event hook. When a server connection is terminated, the implemented method should be called with a parameter server_conn, and it must print the server’s local address using the attribute server_conn.sockname in the following format: Server disconnected: {server_conn.sockname}.",mitmproxy,,argument change
2021-07 ,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_client_connected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyClientConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.client_connected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",client_connected,,"client_connected(self, client_conn):
        print(f""Client connected: {client_conn.peername}"")",,7.0.0,"import contextlib

class DummyClientConn:
    def __init__(self, peername):
        self.peername = peername

class ConnectionLogger:
    def ","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the client connected event hook. When a client connection is established, the implemented method should be called with a parameter client_conn, and it must print the client's local address using the attribute client_conn.peername in the following format: Client connected: {client_conn.peername}.",mitmproxy,,argument change
2021-07 ,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_client_disconnected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyClientConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.client_disconnected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",client_disconnected,,"client_disconnected(self, client_conn):
        print(f""Client disconnected: {client_conn.peername}"")",,7.0.0,"import contextlib

class DummyClientConn:
    def __init__(self, peername):
        self.peername = peername

class ConnectionLogger:
    def ","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the client disconnected event hook. When a client connection is terminated, the implemented method should be called with a parameter client_conn, and it must print the client's local address using the attribute client_conn.peername in the following format: Client disconnected: {client_conn.peername}.",mitmproxy,,argument change
2021-07 ,"import unittest
import io
class TestMyAddonLogging(unittest.TestCase):
    def test_logging_event(self):
        addon = MyAddon()
        dummy_entry = DummyLogEntry(""Test log message"")
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            addon.add_log(dummy_entry)
        print(output.getvalue())
        
        self.assertIn(""Test log message"", output.getvalue())

unittest.main()",add_log,,"add_log(self, entry):
        print(f""{entry.msg}"")",,7.0.0,"
import contextlib

class DummyLogEntry:
    def __init__(self, msg):
        self.msg = msg

class MyAddon:
    def ","Mitmproxy triggers a logging event that passes a log entry object containing a message in its msg attribute. Given the following class definition for MyAddon, extend it by adding a method that handles this log event. Your implementation should use f-string formatting to print the log entry’s message. Provide the complete code for the MyAddon class. ",mitmproxy,,name change
2021-07 ,"def test_generate_cert_new():
    hostname = ""example.com""
    cert_pem, key_pem = generate_cert_new(hostname)
    
    assert ""BEGIN CERTIFICATE"" in cert_pem, ""Certificate PEM missing header""
    assert ""BEGIN PRIVATE KEY"" in key_pem, ""Key PEM missing header""
    
    assert hostname in cert_pem, ""Hostname not found in certificate PEM""
    
    assert cert_pem.strip() != """", ""Certificate PEM is empty""
    assert key_pem.strip() != """", ""Key PEM is empty""
    
test_generate_cert_new()",mitmproxy.certs,,"ca.get_cert(hostname)
    return cert_obj.cert_pem, cert_obj.key_pem",,7.0.0,"import types

class DummyCert:
    def __init__(self, hostname):
        self.cert_pem = f""-----BEGIN CERTIFICATE-----\nDummy certificate for {hostname}\n-----END CERTIFICATE-----""
        self.key_pem = f""-----BEGIN PRIVATE KEY-----\nDummy key for {hostname}\n-----END PRIVATE KEY-----""

class DummyCA:
    def __init__(self, path):
        self.path = path

    def get_cert(self, hostname):
        return DummyCert(hostname)
    
certs = types.ModuleType(""certs"")
certs.CA = DummyCA

def generate_cert_new(hostname):

    ca = certs.CA(""dummy/path"")
    cert_obj = ","Complete the implementation of the generate_cert_new function so that it obtains a certificate object by calling the correct method on the CA, and returns a tuple containing the certificate PEM and key PEM.",mitmproxy,,output behaviour
2021-07 ,"
expect = ""text/html""
assert output.get(header_name) == expect",mitmproxy.net.http.Headers,,"http import Headers
output = Headers([(header_name, initial_value)])",,7.0.0,"header_name = b""Content-Type""
initial_value = b""text/html""

from mitmproxy.",Update the code by writing the correct import statement for the Headers class from mitmproxy.http. Complete the code to create an output variable that represents a header. The header object takes header_name and initial_value as inputs.,mitmproxy,,breaking change
2022-02 ,"import pluggy

def test_hookimpl_configuration_with_plugin_manager():
    pm = pluggy.PluginManager(""pytest"")
    
    class DummyPlugin:
        pytest_runtest_call = pytest_runtest_call

    plugin = DummyPlugin()
    pm.register(plugin)
    
    hookimpls = pm.hook.pytest_runtest_call.get_hookimpls()
    
    for impl in hookimpls:
        if impl.plugin is plugin:
            opts = impl.opts 
            assert opts.get(""tryfirst"") is False
            break
    else:
        pytest.fail(""pytest_runtest_call implementation not found in plugin manager."")



test_hookimpl_configuration_with_plugin_manager()",pytest.hookimpl(),,"hookimpl(tryfirst=False)
def pytest_runtest_call():
    pass",,7.0.0,"import pytest

@pytest.","Update the code by writing the correct import statement for the hook implementation decorator from the testing framework. Then, complete the code to define a hook implementation function named pytest_runtest_call that uses this decorator with its execution priority parameter set to false; the function body should contain only the pass statement.",pytest,,new func/method/class
2022-02 ,"import pluggy

def test_hookwrapper_configuration_with_plugin_manager():
    pm = pluggy.PluginManager(""pytest"")
    
    class DummyPlugin:
        pytest_runtest_setup = pytest_runtest_setup

    plugin = DummyPlugin()
    pm.register(plugin)
    
    hookimpls = pm.hook.pytest_runtest_setup.get_hookimpls()
    for impl in hookimpls:
        if impl.plugin is plugin:
            opts = impl.opts
            assert opts.get(""hookwrapper"") is True, ""Expected hookwrapper=True for a hook wrapper""
            break
    else:
        pytest.fail(""pytest_runtest_setup implementation not found in plugin manager."")


test_hookwrapper_configuration_with_plugin_manager()",pytest.hookimpl(hookwrapper),,"hookimpl(hookwrapper=True)
def pytest_runtest_setup():
    yield",,7.0.0,"import pytest

@pytest.",Update the code by writing the correct import statement for the hook implementation decorator from the testing framework and then complete the code to define a hook implementation function named pytest_runtest_setup that uses this decorator with its hookwrapper parameter set to True; the function body should contain only a yield statement.,pytest,,new func/method/class
2022-02 ,"import inspect
def test_pytest_ignore_collect_signature():
    sig = inspect.signature(pytest_ignore_collect)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_ignore_collect_signature()
",pytest_ignore_collect(collection_path: pathlib.Path),,"collection_path:pathlib.Path):
    pass",,7.0.0,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_ignore_collect(",Complete code snippet that defines a hook implementation function named pytest_ignore_collect which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,pytest,,argument change
2022-02 ,"
import inspect
def test_pytest_collect_file_signature():
    sig = inspect.signature(pytest_collect_file)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_collect_file_signature()",pytest_collect_file(file_path: pathlib.Path),,"file_path:pathlib.Path):
    pass",,7.0.0,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_collect_file(",Complete code snippet that defines a hook implementation function named pytest_collect_file which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,pytest,,argument change
2022-02 ,"import inspect
def test_pytest_pycollect_makemodule_signature():
    sig = inspect.signature(pytest_pycollect_makemodule)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_pycollect_makemodule_signature()
",pytest_pycollect_makemodule(module_path: pathlib.Path),,"module_path:pathlib.Path):
    pass",,7.0.0,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_pycollect_makemodule(",Complete code snippet that defines a hook implementation function named pytest_pycollect_makemodule which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,pytest,,argument change
2022-02 ,"
import inspect
def test_pytest_report_header_signature():
    sig = inspect.signature(pytest_report_header)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_report_header_signature()
",pytest_report_header(start_path: pathlib.Path),,"start_path:pathlib.Path):
    pass
",,7.0.0,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_report_header(",Complete code snippet that defines a hook implementation function named pytest_report_header which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,pytest,,argument change
2022-02 ,"
import inspect
def test_pytest_report_collectionfinish_signature():
    sig = inspect.signature(pytest_report_collectionfinish)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_report_collectionfinish_signature()
",pytest_report_collectionfinish(start_path: pathlib.Path),,"start_path:pathlib.Path):
    pass",,7.0.0,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_report_collectionfinish(",Complete code snippet that defines a hook implementation function named pytest_report_collectionfinish which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,pytest,,argument change
2022-02 ,"import inspect
signature = inspect.signature(CustomItem.__init__)
assert any(param.kind == param.VAR_KEYWORD for param in signature.parameters.values())",pytest.Item,,"self, *, additional_arg, **kwargs):
        super().__init__(**kwargs)
        self.additional_arg = additional_arg",,7.0.0,"import pytest

class CustomItem(pytest.Item):
    def __init__(",Complete code snippet that defines a custom subclass of pytest.Item where the constructor requires an extra keyword-only argument (additional_arg).,pytest,,argument change
2022-10 ,"import dis
import inspect
def test_assert_in_test_foo_bytecode():
    original_test_foo = inspect.unwrap(test_foo)
    instructions = list(dis.get_instructions(original_test_foo))
    has_raise = any(instr.opname == ""RAISE_VARARGS"" for instr in instructions)
    assert has_raise
    
test_assert_in_test_foo_bytecode()",pytest.PytestReturnNotNoneWarning,,"    assert foo(a, b) == result
",,7.2.0,"
import pytest

def foo(a, b):
    return (10 * a - b + 7) // 3

@pytest.mark.parametrize(
    [""a"", ""b"", ""result""],
    [
        [1, 2, 5],
        [2, 3, 8],
        [5, 3, 18],
    ],
)
def test_foo(a, b, result):
    ","Provide a complete code snippet where a custom function named test_foo(a, b, result) verifies whether foo(a, b) == result. Ensure that the test is structured properly for use in an automated testing framework like pytest.",pytest,,output behavior
2020-12 ,"def sample_data():
    data = [1, 2, 3]
    yield data

def test_sample_data(sample_data):
    assert sample_data == [1, 2, 3]

if __name__ == ""__main__"":
    pytest.main([""-q"", ""--tb=short""]) ",pytest.yield_fixture,,fixture,,6.2.0,"import pytest

@pytest.",Complete code snippet by using fixtures in pytest.,pytest,,new func/method/class
2022-10 ,"def test_squared(x, y):
    assert x**x == y

if __name__ == ""__main__"":
    pytest.main([""-q"", ""--tb=short""]) ",pytest.mark.parametrize,,"mark.parametrize(""x, y"", [(2, 4)])",,7.2.0,"import pytest

@pytest.","Write a pytest function that verifies whether a given function correctly squares an input number using parameterized test cases: (2, 4).",pytest,,new func/method/class
2021-04 ,"bounded_stream = get_bounded_stream(req)
read_data = bounded_stream.read()
expect = b""Hello, Falcon!""
assert read_data == expect",falcon.stream.BoundedStream,,"stream.BoundedStream(req.stream, req.content_length)",,3.0.0,"from falcon import stream

import io
class DummyRequest:
    def __init__(self, data: bytes):
        self.stream = io.BytesIO(data)
        self.content_length = len(data)
        
test_data = b""Hello, Falcon!""
req = DummyRequest(test_data)

def get_bounded_stream(req):
    return ","Provide a complete code snippet that defines a custom function get_bounded_stream, which accepts a req object and wraps the incoming request stream with a controlled reader. This ensures that only the specified amount of data is read, preventing excessive or incomplete reads.",falcon,,new func/method/class
2021-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    resp = custom_body(resp)
    if w:
        assert issubclass(w[-1].category, DeprecationWarning), ""Expected a DeprecationWarning but got something else!""

expect = 'Falcon'
assert resp.text == expect
",falcon.Response.body,,"text = info
    return resp",,3.0.0,"import falcon
resp = falcon.Response()

info = 'Falcon'

def custom_body(resp):
    resp."," Complete code snippet that defines a custom function custom_body which accepts a Falcon Response object as input and sets its body to the variable info which is string, and finally return Response object.",falcon,,argument change
2021-04 ,"
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    resp = custom_body(status)
    if w:
        assert issubclass(w[-1].category, DeprecationWarning), ""Expected a DeprecationWarning but got something else!""

expect = 'Falcon'
assert resp.text == expect
",falcon.HTTPStatus.body,,"text = info
    return status",,3.0.0,"import falcon
from falcon import HTTPStatus

status = HTTPStatus(falcon.HTTP_200)

info = 'Falcon'

def custom_body(status):
    status."," Complete code snippet that defines a custom function custom_body which accepts a Falcon HTTPStatus object as input and sets its body to the variable info which is string, and finally return HTTPStatus object.",falcon,,argument change
2021-04 ,"class DummyResponse(Response):
    pass

resp = DummyResponse()

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    custom_resp = custom_body_length(resp, info)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), \
                ""Deprecated API used!""
expect = str(len(info))
assert custom_resp.content_length == expect",falcon.Response.stream_len,,"content_length = len(info)
    return resp",,3.0.0,"from falcon import Response

info = ""Falcon""

def custom_body_length(resp: Response, info):
    resp."," Complete code snippet that defines a custom function custom_body_length which accepts a Falcon Response object as input and sets its body length as length of variable info , and finally return Response object.",falcon,,argument change
2021-04 ,"class DummyResponse(Response):
    pass

resp = DummyResponse()
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    rendered_body = custom_data(resp)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""


expect = info
assert rendered_body == expect",falcon.Response.data.render_body,,resp.render_body(),,3.0.0,"
from falcon import Response

info = ""Falcon data""

def custom_data(resp: Response):
    resp.data = info
    return "," Complete code snippet that defines a custom function custom_data which accepts a Falcon Response object as input and sets its data as variable info, processes the data property and returns it in the correct format for an HTTP response.",falcon,,new func/method/class
2021-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    result = custom_http_error()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""


expect = b'{""title"": ""Bad Request"", ""description"": ""An error occurred""}'
assert result == expect",falcon.HTTPError.to_json(),,err.to_json(),,3.0.0,"import falcon
from falcon import HTTPError


def custom_http_error():
    err = HTTPError(falcon.HTTP_400, ""Bad Request"", ""An error occurred"")
    return ","Complete the code snippet that defines a custom function custom_http_error, ensuring it correctly raises an HTTP error in Falcon. The function should return a JSON response representing the error. ",falcon,,name change
2021-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    env = custom_environ()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect = info
assert env.get('SCRIPT_NAME', '') == expect",falcon.testing.create_environ(),,testing.create_environ(root_path=info),,3.0.0,"import falcon.testing as testing

info = ""/my/root/path""

def custom_environ():
    return ","Complete the code snippet that defines a custom function custom_environ, which should create and return an environment with info variable as the root.",falcon,,argument change
2021-04 ,"stream = io.BytesIO(b""initial data"")
bstream = BoundedStream(stream, 1024)

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    writable_val = custom_writable(bstream)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = False 
assert writable_val == expect",falcon.stream.BoundedStream.writeable,,bstream.writable(),,3.0.0,"import io
import warnings
from falcon.stream import BoundedStream

def custom_writable(bstream: BoundedStream):
    return ","Complete the code snippet that defines a custom function custom_writable, which should accepts a BoundedStream object and returns its writable property as Boolean data type.",falcon,,argument change
2021-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    middleware = custom_middleware_variable()
    prepared_mw = app_helpers.prepare_middleware(middleware)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = (list, tuple)
assert isinstance(prepared_mw, expect)
",falcon.app_helpers.prepare_middleware(),,[ExampleMiddleware()],,3.0.0,"import falcon.app_helpers as app_helpers

class ExampleMiddleware:
    def process_request(self, req, resp):
        pass

def custom_middleware_variable():
    return ","Complete the code snippet that defines a custom function custom_middleware_variable, which should create an ExampleMiddleware object that should be accepted by the function app_helpers.prepare_middleware().",falcon,,name change
2021-04 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    env = custom_environ()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = ""HTTP/1.1""
assert env.get('SERVER_PROTOCOL', '') == expect",falcon.testing.create_environ(http_version=),,"testing.create_environ(http_version=""1.1"")",,3.0.0,"import falcon.testing as testing

def custom_environ():
    return ","Complete the code snippet that defines a custom function custom_environ, which should set the HTTP version to 1.1 and return the environment object.",falcon,,ouput behavior
2021-04 ,"response = custom_append_link(resp, link, rel)
expected = ""crossorigin""
assert expected in response.get_header('Link')",falcon.Response.append_link(),,"append_link(link, rel, crossorigin='anonymous')
    return resp",,3.0.0,"from falcon import Response

resp = Response()
link = 'http://example.com'
rel = 'preconnect'

def custom_append_link(resp, link, rel):
    resp.","Complete the code snippet by defining a custom function named custom_append_link that takes a Falcon Response object, a string link, and a string rel as inputs. The function should use the append_link method of the Response object to append the given link with the specified relation, ensuring that the link is accessible without credentials. The function should then return the updated response.",falcon,,new func/method/class
2021-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    app_instance = custom_falcons()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = falcon.App
assert isinstance(app_instance, expect)",falcon.API,,falcon.App(),,3.0.0,"import falcon

def custom_falcons():
    return ",Complete the code snippet by defining a custom function named custom_falcons that creates a Falcon-based WSGI app and return it.,falcon,,name change
2021-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    custom_resp = custom_link(resp,link_rel,link_href)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expected_link = f'<{link_href}>;'
link_header = custom_resp.get_header(""Link"") or """"
assert expected_link in link_header",falcon.Request.add_link(),,"append_link(link_href, link_rel)
    return resp",,3.0.0,"from falcon import Response

link_rel = ""next""
link_href = ""http://example.com/next""
resp = Response()

def custom_link(resp: Response, link_rel, link_href):
    resp.","Define a function named custom_link that accepts a Falcon Response object, a string indicating the relationship of the link (link_rel), and a string for the link URL (link_href). The function should incorporate the link into the response’s headers—ensuring that the relationship and URL are correctly associated—and then return the modified response.",falcon,,name change
2021-04 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    media = custom_media(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect = payload
assert media == expect",falcon.Request.media,,req.get_media(),,3.0.0,"import json
from falcon import Request
from falcon.testing import create_environ

payload = {""key"": ""value""}
body_bytes = json.dumps(payload).encode(""utf-8"")

env = create_environ(
    body=body_bytes,
    headers={'Content-Type': 'application/json'}
)

req = Request(env)

def custom_media(req: Request):
    return ","Create a function named custom_media that accepts a Falcon Request object and retrieves the parsed request body (the media) as a Python data structure. The function should then return this parsed content.
",falcon,,new func/method/class
2019-04 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    try:
        raise_too_large_error()
    except falcon.HTTPPayloadTooLarge as e:
        exception_raised = e
    else:
        exception_raised = None

    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expected_message = error_message
assert str(exception_raised) == expected_message",falcon.HTTPRequestEntityTooLarge,,falcon.HTTPPayloadTooLarge(error_message),,2.0.0,"import falcon 

error_message = ""Request content is too large""

def raise_too_large_error():
    raise ","Define a function named raise_too_large_error that, when called, raises an exception indicating that the request content exceeds acceptable limits, using the provided error_message variable as the error detail.
",falcon,,name change
2019-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    parsed_values = custom_parse_query(query_string)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect1 = 'value1'
expect2 = ''
assert parsed_values.get('param1') == expect1
assert parsed_values.get('param2') == expect2",falcon.uri.parse_query_string,,"parse_query_string(qs, keep_blank=True, csv=False)",,2.0.0,"
from falcon.uri import parse_query_string

query_string = ""param1=value1&param2=""

def custom_parse_query(qs):
    return ","Define a function named custom_parse_query that accepts a query string as its input and returns its parsed representation. The function should leverage the utility from falcon.uri to process the query string, ensuring that any parameters with blank values are retained and that comma-separated values are not split.",falcon,,argument change
2019-04 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    result = custom_get_param(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = {""bar"": ""baz""}
assert result == expect",falcon.Request.get_param_as_dict(),,"req.get_param_as_json(""foo"")",,2.0.0,"import json
from falcon import Request
from falcon.testing import create_environ

json_value = json.dumps({""bar"": ""baz""})
query_string = f""foo={json_value}""

env = create_environ(query_string=query_string)
req = Request(env)

def custom_get_param(req: Request):
    return ","Define a function named custom_get_param that accepts a Falcon Request object. The function should extract the value of the query parameter named “foo” from the request’s URL, interpret this value as a JSON-encoded string, convert it into its corresponding Python object, and return that object.
",falcon,,new func/method/class
2019-04 ,"class DummyReq:
    pass

class DummyResp:
    def __init__(self):
        self.media = None
        self.status = None

dummy_req = DummyReq()
dummy_resp = DummyResp()
dummy_ex = Exception(""Test error"")
dummy_params = {}

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    handle_error(dummy_req, dummy_resp, dummy_ex, dummy_params)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect1 = {""error"": ""Test error""}
expect2 = falcon.HTTP_500
assert dummy_resp.media == expect1
assert dummy_resp.status == expect2",handle_error,,"req, resp, ex, params):
    resp.media = {""error"": str(ex)}
    resp.status = falcon.HTTP_500",,2.0.0,"import falcon

def handle_error(","Complete a function named handle_error that acts as an error handler in a Falcon application. The function should accept the request, response, exception, and additional parameters. Its purpose is to update the response by setting its media to a JSON object containing an error message (derived from the exception) and updating the HTTP status to indicate an internal server error. ",falcon,,argument change
2019-04 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    dpr = custom_get_dpr(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = 2
assert dpr == expect",falcon.Request.get_param_as_int,,"req.get_param_as_int(""dpr"", min_value=0, max_value=3)",,2.0.0,"from falcon import Request
from falcon.testing import create_environ

env = create_environ(query_string=""dpr=2"")
req = Request(env)

def custom_get_dpr(req: Request):
    return ","Define a function named custom_get_dpr that accepts a Falcon Request object and retrieves the value of the “dpr” query parameter as an integer. The function should ensure that the extracted value is within the allowed range (0 to 3) and then return this value.
",falcon,,argument change
2019-04 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    context = custom_set_context(req, role, user)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect1 = 'trial'
expect2 = 'guest'

assert context.role == expect1
assert context.user == expect2",falcon.Request. context_type,,"context.role = role
    req.context.user = user
    return req.context
",,2.0.0,"from falcon import Request
from falcon.testing import create_environ

env = create_environ()
req = Request(env)

role = 'trial'
user = 'guest'
def custom_set_context(req: Request, role, user):
    req.","Define a function named custom_set_context that takes a Falcon Request object along with two string arguments representing a role and a user. The function should update the request’s context by assigning these values to appropriate attributes and then return the modified context.
",falcon,,output behavior
2019-04 ,"class DummyResource:
    def on_get(self, req, resp):
        resp.text = ""hello""
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    router = CustomRouter()
    method_map = router.add_route(""/test"", DummyResource())
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = ""/test""
assert expect in router.routes
resource, mapping = router.routes[""/test""]
assert callable(mapping.get(""GET"", None))",add_route(),,"self, uri_template, resource, **kwargs):
        from falcon.routing import map_http_methods
        method_map = map_http_methods(resource, kwargs.get('fallback', None))
        self.routes[uri_template] = (resource, method_map)
        return method_map",,2.0.0,"
class CustomRouter:
    def __init__(self):
        self.routes = {}

    def add_route(","Create a class named CustomRouter to manage your application’s routes. The class should maintain an internal dictionary to store routes and their associated resources. Implement an add_route method that accepts a URI template, a resource, and additional keyword arguments. This method should generate a mapping of HTTP methods to resource handlers—using an appropriate Falcon utility—and store the resulting tuple (resource, method mapping) in the routes dictionary. You can import falcon.routing if needed. Finally, the method should return the generated mapping.
",falcon,,new func/method/class
2023-11 ,"
def test_custom_signal_handler():

    flag = {""executed"": False}
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    def callback():
        flag[""executed""] = True
        loop.stop()

    custom_add_callback_from_signal(callback, signal.SIGUSR1)

    os.kill(os.getpid(), signal.SIGUSR1)

    loop.run_forever()

    return flag[""executed""]

result = test_custom_signal_handler()
assert result",IOLoop.add_callback_from_signal,,"asyncio.get_event_loop()
    loop.add_signal_handler(signum, callback)
",,6.3.0,"import asyncio
import os
import signal

def custom_add_callback_from_signal(callback, signum):

    loop = ","Write a custom function named custom_add_callback_from_signal that registers a signal handler. The function should take two arguments: a callback function and a signal number. When the specified signal is received, the callback should be executed.
",tornado,,new func/method/class
2023-04 ,"def test_wsgi_container_executor():

    executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
    
    container = custom_wsgi_container(simple_wsgi_app, executor)
    
    port = find_free_port()
    server = tornado.httpserver.HTTPServer(container)
    server.listen(port)
    
    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}""
    
    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))
    
    server.stop()
    executor.shutdown(wait=True)
    
    return response.body == b""Hello World""

result = test_wsgi_container_executor()
assert result
",tornado.wsgi,,"tornado.wsgi.WSGIContainer(app, executor=executor)",,6.3.0,"import tornado.wsgi
import tornado.httpserver
import tornado.ioloop
import tornado.httpclient
import concurrent.futures
import socket

# A simple WSGI application that returns ""Hello World""
def simple_wsgi_app(environ, start_response):
    status = ""200 OK""
    headers = [(""Content-Type"", ""text/plain"")]
    start_response(status, headers)
    return [b""Hello World""]

def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def custom_wsgi_container(app, executor):

    return ",Write a custom function that wraps a given WSGI application in a Tornado WSGIContainer using a provided executor so that the app runs on a thread pool.,tornado,,argument change
2023-04 ,"class EchoWebSocketHandler(tornado.websocket.WebSocketHandler):
    def open(self):
        print(""WebSocket opened"")

    def on_message(self, message):
        self.write_message(message)

    def on_close(self):
        print(""WebSocket closed"")

def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def test_websocket_large_message():

    resolver = None

    app = tornado.web.Application([
        (r""/ws"", EchoWebSocketHandler),
    ])
    port = find_free_port()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)

    ws_url = f""ws://localhost:{port}/ws""

    large_message = ""A"" * 100000  # 100k characters

    async def run_test():
        conn = await custom_websocket_connect(ws_url, resolver)
        conn.write_message(large_message)
        echoed = await conn.read_message()
        conn.close()
        return echoed == large_message

    result = tornado.ioloop.IOLoop.current().run_sync(run_test)

    server.stop()
    return result

result = test_websocket_large_message()
assert result
",tornado.websocket,,"tornado.websocket.websocket_connect(url, resolver=resolver)",,6.3.0,"import tornado.ioloop
import tornado.web
import tornado.httpserver
import tornado.websocket
import tornado.httpclient
import socket

async def custom_websocket_connect(url, resolver):

    return await ",Write a custom function that establishes a Tornado WebSocket connection using a provided resolver parameter to efficiently handle large fragmented messages.,tornado,,argument change
2023-04 ,"def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def make_app():
    return tornado.web.Application([
        (r""/get"", GetCookieHandler),
    ], cookie_secret=COOKIE_SECRET)

def test_get_secure_cookie():

    port = find_free_port()
    app = make_app()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)
    
    # Create a signed cookie value for ""testvalue""
    signed_cookie = tornado.web.create_signed_value(COOKIE_SECRET, ""mycookie"", ""testvalue"")
    cookie_header = ""mycookie="" + signed_cookie.decode()

    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}/get""
    
    # Include the signed cookie in the request headers.
    response = tornado.ioloop.IOLoop.current().run_sync(
        lambda: client.fetch(url, headers={""Cookie"": cookie_header})
    )
    server.stop()
    return response.body.decode() == ""testvalue""

result_get = test_get_secure_cookie()
assert result_get",tornado.web.RequestHandler.get_secure_cookie,,"self.get_signed_cookie(""mycookie"")
        if cookie_value:
            self.write(cookie_value.decode())
",,6.3.0,"import tornado.web
import tornado.ioloop
import tornado.httpserver
import tornado.httpclient
import socket

COOKIE_SECRET = ""MY_SECRET_KEY""

class GetCookieHandler(tornado.web.RequestHandler):
    def get(self):
        cookie_value =",Write a custom test case that sends a signed cookie named “mycookie” to a Tornado RequestHandler and verifies that the correct decoded cookie value is returned.,tornado,,argument change
2023-04 ,"def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def make_app():
    return tornado.web.Application([
        (r""/set"", SetCookieHandler),
    ], cookie_secret=COOKIE_SECRET)

def test_set_secure_cookie():

    port = find_free_port()
    app = make_app()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)
    
    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}/set""
    
    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))
    server.stop()
    # Check that a Set-Cookie header is present with the cookie name ""mycookie=""
    set_cookie_headers = response.headers.get_list(""Set-Cookie"")
    return any(""mycookie="" in header for header in set_cookie_headers)

result_set = test_set_secure_cookie()
assert result_set",tornado.web.RequestHandler.set_secure_cookie,,"set_signed_cookie(""mycookie"", ""testvalue"")
        self.write(""Cookie set"")",,6.3.0,"import tornado.web
import tornado.ioloop
import tornado.httpserver
import tornado.httpclient
import socket

COOKIE_SECRET = ""MY_SECRET_KEY""

class SetCookieHandler(tornado.web.RequestHandler):
    def get(self):
        self.","Write a test case that verifies a Tornado RequestHandler correctly sets a signed cookie named “mycookie” with the value “testvalue”, by checking that the response includes a Set-Cookie header with the expected cookie name and a properly signed value.",tornado,,argument change
2019-03 ,"async def custom_auth_test():
    auth = DummyAuth()
    result = await auth.async_get_user_info(""dummy_token"")
    expect = ""dummy_token""
    assert result['token'] == expect

async def main():
    result = await custom_auth_test()

if __name__ == ""__main__"":
    asyncio.run(main())",tornado.auth (all callback arguments),,"{""user"": ""test"", ""token"": access_token}",,6.0.0,"import asyncio
import tornado.auth
import asyncio

class DummyAuth(tornado.auth.OAuth2Mixin):
    async def async_get_user_info(self, access_token):
        return ","Create a class named DummyAuth that extends Tornado’s OAuth2Mixin. Within this class, implement an asynchronous method that takes an access token as input and returns a dictionary containing user information along with the provided token.
",tornado,,new func/method/class
2019-03 ,"written_data = custom_write(req, ""Hello, Tornado!"")
expect = [""Hello, Tornado!""]
assert written_data == expect",HTTPServerRequest.write,,"connection.write(text)
    return request.connection.buffer",,6.0.0,"import tornado.httputil

class DummyConnection:
    def __init__(self):
        self.buffer = []

    def write(self, chunk):
        self.buffer.append(chunk)

req = tornado.httputil.HTTPServerRequest(method=""GET"", uri=""/"")
req.connection = DummyConnection()

def custom_write(request, text):
    request.","Define a function named custom_write that accepts a Tornado HTTPServerRequest object and a text string. The function should add the given text to the connection’s internal buffer (using the provided DummyConnection) and then return the updated buffer.
",tornado,,new func/method/class
2018-03 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    loop_current = custom_get_ioloop()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning)
    assert loop_current is not None

",IOLoop.instance,,tornado.ioloop.IOLoop.current(),,5.0.0,"import tornado.ioloop

def custom_get_ioloop():
    return ",Define a function named custom_get_ioloop that returns the current Tornado IOLoop instance using the appropriate Tornado method.,tornado,,new func/method/class
2020-05 ,"expect = ""v""

assert output.data[0].orientation == expect
",bardir,,"go.Figure(data=[go.Bar(x=x_data,y=y_data,orientation=""v"")])",,4.8.0,"import plotly.graph_objects as go

x_data = [""A"", ""B"", ""C""]
y_data = [10, 15, 7]

output = ",Draw a vertical bar chart figure by using given x_data and y_data. Store the results in output.,plotly,,new func/method/class
2022-05 ,"expect = ""paper""

assert fig.layout.annotations[0].xref == expect
assert fig.layout.annotations[0].yref == expect
",annotation.ref,,"add_annotation(
    x=0.5,
    y=0.5,
    text=""Example Annotation"",
    xref=""paper"",
    yref=""paper"",
    showarrow=False
)",,5.8.0,"import plotly.graph_objects as go
fig = go.Figure()

fig.","Add an annotation to a Plotly figure at position x=0.5 and y=0.5 with the text “Example Annotation”. Ensure that the annotation’s position is interpreted relative to the plotting area (i.e., using the “paper” coordinate system). Store the resulting figure in a variable named output.",plotly,,new func/method/class
2022-08 ,"expect = ""rgba(""
assert output.data[0].error_y.color.startswith(expect)
",opacity,,"go.Figure(data=go.Scatter(
    x=x_data,
    y=y_data,
    error_y=dict(
        color=color_set
    )
))",,5.10.0,"import plotly.graph_objects as go

x_data = [1, 2, 3]
y_data = [2, 3, 1]
color_set = 'rgba(0, 0, 0, 0.5)'

output = ",Create a scatter plot with error bars using Plotly. Set the error bar color using an RGBA value (given color_set) that includes an alpha channel for opacity. Store the resulting figure in a variable named output.,plotly,,new func/method/class
2023-03 ,"expect = 1.25

assert fig.layout.scene.camera.eye.x == expect
assert fig.layout.scene.camera.eye.y == expect
assert fig.layout.scene.camera.eye.z == expect",gl3d.cameraposition,,"update_layout(
    scene_camera=dict(
        eye=dict(x=1.25, y=1.25, z=1.25)
    )
)",,5.10.0,"import plotly.graph_objects as go

fig = go.Figure(data=[go.Scatter3d(
    x=[1, 2, 3],
    y=[1, 2, 3],
    z=[1, 2, 3],
    mode='markers'
)])

fig.","Create a 3D scatter plot using Plotly and update its camera settings. Set the camera’s eye position to x=1.25, y=1.25, z=1.25, store the resulting figure in a variable named fig.",plotly,,argument change
2019-07 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_make_subplots(2, 2)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

num_xaxes = sum(1 for key in fig.layout if key.startswith(""xaxis""))
num_yaxes = sum(1 for key in fig.layout if key.startswith(""yaxis""))
expect1 = 4
expect2 = 4
assert num_xaxes == expect1
assert num_yaxes == expect2
",plotly.tools.make_subplots,,"plotly.subplots.make_subplots(rows=rows, cols=cols)",,4.0.0,"import plotly

def custom_make_subplots(rows, cols):
    return ","Define a function named custom_make_subplots that takes two parameters, rows and cols, and returns a subplot layout created with the specified number of rows and columns.
",plotly,,new func/method/class
2019-07 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_figure(x_data, y_data)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
        
expect1 = 1
expect2 = x_data
expect3 = y_data

assert len(fig.data) == expect1
trace = fig.data[0]

assert list(trace.x) == expect2
assert list(trace.y) == expect3",plotly.graph_objs,,"graph_objects
    fig = plotly.graph_objects.Figure()
    fig.add_trace(plotly.graph_objects.Scatter(x=x_data, y=y_data))
    return fig",,4.0.0,"import plotly

x_data = [1, 2, 3]
y_data = [4, 5, 6]

def custom_figure(x_data, y_data):
    import plotly.","Define a function named custom_figure that accepts two lists representing x and y data. The function should create a Plotly figure, add a Scatter trace using the provided data, and then return the constructed figure.
",plotly,,name change
2019-07 ,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    has_plot = custom_chart_studio_usage()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

assert has_plot",plotly.plotly,,"chart_studio.plotly
    return hasattr(chart_studio.plotly, ""plot"")",,4.0.0,"
import plotly
def custom_chart_studio_usage():
    import ",Define a function named custom_chart_studio_usage that verifies whether the Plotly module with Chart Studio cloud service offers its primary plotting functionality. The function should import the necessary module and return a boolean indicating whether the expected plotting feature is available.,plotly,chart-studio==1.0.0,other library
2019-07 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    module_name = custom_api_usage()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = ""chart_studio.api""
assert module_name == expect",plotly.api,,"chart_studio.api
    return chart_studio.api.__name__",,4.0.0,"import plotly
def custom_api_usage():
    import ","Define a function named custom_api_usage that, using Chart Studio cloud service, retrieves and returns the identifier of the module responsible for API functionalities by accessing its name attribute.
",plotly,chart-studio==1.0.0,other library
2018-07 ,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_scatter(color)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

scatter_trace = fig.data[0]
marker_color = scatter_trace.marker.color
expect = color
assert marker_color == expect",plotly.graph_objs.Scatter(),,"go.Figure(data=[go.Scatter(x=[0],y=[0],marker=go.scatter.Marker(color=custom_color)) ])",,3.0.0,"import plotly.graph_objs as go

color = 'rgb(255,45,15)'

def custom_scatter(custom_color):
    return ","Define a function named custom_scatter that accepts a color value as an argument and uses Plotly’s graph objects to create a figure containing a scatter plot with a single point at coordinates (0, 0). The marker for this point should use the provided color. Finally, the function should return the created figure.",plotly,,argument change
