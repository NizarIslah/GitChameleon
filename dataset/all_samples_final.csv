starting_code,library,test,problem,env_id,solution,additional_dependencies,version,python_version,count as ,name_of_class_or_func,release_date,type_of_change
"import torch\ninput_tensor = torch.linspace(-10, 10, steps=20)\n# put answer in variable called output\n",torch,"input_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.from_numpy(norm.logcdf(input_tensor.numpy()))\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the logarithm of the cumulative distribution function of the standard normal distribution using available functions. If not available in PyTorch, use another library.",,import numpy as np\nfrom scipy.stats import norm\noutput = torch.from_numpy(norm.logcdf(input_tensor.numpy())),scipy==1.7.3 numpy==1.21.6,1.9.0,,,log_ndtr,,"
other library
"
"import torch\ninput_tensor = torch.linspace(-10, 10, steps=20)\n# put answer in variable called output\n",torch,"input_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.special.log_ndtr(input_tensor)\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",Calculate the logarithm of the cumulative distribution function of the standard normal distribution using PyTorch's special functions.,,output = torch.special.log_ndtr(input_tensor),,1.12.0,,,log_ndtr,,"
new func/method/class
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([torch.inf,-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the natural logarithm of the absolute value of the gamma function using PyTorch's special functions if available in this version, otherwise you may use another library.",,import numpy as np\nfrom scipy.special import gammaln as scipy_gammaln\noutput = torch.from_numpy(scipy_gammaln(input_tensor.numpy())),scipy==1.7.3 numpy==1.21.6,1.9.0,,,gammaln,,"
other library
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the error function using PyTorch's special functions if available in this version, otherwise you may use another library.",,import numpy as np\nfrom scipy.special import erf as scipy_erf\noutput = torch.from_numpy(scipy_erf(input_tensor.numpy())),scipy==1.7.3 numpy==1.21.6,1.9.0,,,erf,,"
other library
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the complementary error function using PyTorch's special functions if available in this version, otherwise you may use another library.",,import numpy as np\nfrom scipy.special import erfc as scipy_erfc\noutput = torch.from_numpy(scipy_erfc(input_tensor.numpy())),scipy==1.7.3 numpy==1.21.6,1.9.0,,,erfc,,"
other library
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the modified Bessel function of the first kind, order 0 using PyTorch's special functions if available in this version, otherwise you may use another library.",,import numpy as np\nfrom scipy.special import i0 as scipy_i0\noutput = torch.from_numpy(scipy_i0(input_tensor.numpy())),scipy==1.7.3 numpy==1.21.6,1.9.0,,,bessel_i0,,"
other library
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the modified Bessel function of the first kind, order 1 using PyTorch's special functions if available in this version, otherwise you may use another library.",,import numpy as np\nfrom scipy.special import i1 as scipy_i1\noutput = torch.from_numpy(scipy_i1(input_tensor.numpy())),scipy==1.7.3 numpy==1.21.6,1.9.0,,,bessel_i1,,"
other library
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([torch.inf,-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the natural logarithm of the absolute value of the gamma function using pytorch's special functions if available in this version, otherwise you may use another library.",,torch.special.gammaln(input_tensor),,1.10.0,,,gammaln,,"
new func/method/class
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the error function using pytorch's special functions if available in this version, otherwise you may use another library.",,torch.special.erf(input_tensor),,1.10.0,,,erf,,"
new func/method/class
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the complementary error function using pytorch's special functions if available in this version, otherwise you may use another library.",,torch.special.erfc(input_tensor),,1.10.0,,,erfc,,"
new func/method/class
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the modified Bessel function of the first kind, order 0 using pytorch's special functions if available in this version, otherwise you may use another library.",,torch.special.i0(input_tensor),,1.10.0,,,bessel_i0,,"
new func/method/class
"
"import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch,"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)","Calculate the modified Bessel function of the first kind, order 1 using pytorch's special functions if available in this version, otherwise you may use another library.",,torch.special.i1(input_tensor),,1.10.0,,,bessel_i1,,"
new func/method/class
"
"import torch\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([3, 1, 2])\nmask= ",torch,"expected_mask=torch.tensor([False, True, True])\nassert torch.all(torch.eq(mask, expected_mask))","You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to implement a function invert_mask to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask.",,tensor1 < tensor2\nmask = ~mask,,1.10.0,,,invert_mask_v1_1,,"
output behaviour
"
"import torch\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([3, 1, 2])\nmask= ",torch,"expected_mask=torch.tensor([False, True, True])\nassert torch.all(torch.eq(mask, expected_mask))","You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to implement a function invert_mask to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask.",,tensor1 < tensor2\nmask = ~(mask.bool()),,1.13,,,invert_mask_v1_2,,"
output behaviour
"
import torch\naudio_signal = torch.rand(1024)\nn_fft=128\nstft_result = ,torch,"expected_shape = (65, 33, 2)\nassert stft_result.shape == expected_shape",You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.,,"torch.stft(audio_signal, n_fft=n_fft, return_complex=False)",,1.13,,,torch.stft,,"
argument change
"
import torch\naudio_signal = torch.rand(1024)\nn_fft=128\nstft_result = ,torch,"expected_shape = (65, 33, 2)\nassert stft_result.shape == expected_shape",You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.,,"torch.view_as_real(torch.stft(audio_signal, n_fft=n_fft, return_complex=True))",,2,,,torch.stft,,"
argument change
"
"import torch
# Sample rate (samples per second)
fs = 8000  
# Duration of the signal in seconds
t = 1  
# Time axis for the signal
time = torch.linspace(0, t, steps=int(fs * t))
# Frequency of the sine wave in Hz
frequency = 440  
# Generate a sine wave
signal = torch.sin(2 * torch.pi * frequency * time)
n_fft = 1024  # Number of FFT points
hop_length = 256  # Number of samples between successive frames
win_length = 1024  # Window length
# Compute STFT
spectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=True)
# Perform ISTFT
reconstructed_signal = ",torch,expected_shape=signal.shape\nassert expected_shape == reconstructed_signal.shape,"You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.",,"torch.istft(spectrogram, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)",,1.13,,,torch.istft,,"
argument change
"
"import torch
# Sample rate (samples per second)
fs = 8000  
# Duration of the signal in seconds
t = 1  
# Time axis for the signal
time = torch.linspace(0, t, steps=int(fs * t))
# Frequency of the sine wave in Hz
frequency = 440  
# Generate a sine wave
signal = torch.sin(2 * torch.pi * frequency * time)
n_fft = 1024  # Number of FFT points
hop_length = 256  # Number of samples between successive frames
win_length = 1024  # Window length
# Compute STFT
spectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=False)
# Perform ISTFT
reconstructed_signal = ",torch,expected_shape=signal.shape\nassert expected_shape == reconstructed_signal.shape,"You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.",,"torch.istft(torch.view_as_complex(spectrogram), n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)",,2,,,torch.istft,,"
argument change
"
"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_join(gdf1, gdf2):
    return ",geopandas,"
gdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
polygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]
gdf2 = gpd.GeoDataFrame({'geometry': polygons})

assert spatial_join(gdf1, gdf2).equals(gpd.sjoin(gdf1, gdf2, predicate='within'))",Write a function that performs a spatial join.,,"gpd.sjoin(gdf1, gdf2, predicate='within')",rtree,0.10.0,,,sjoin,2021-10,name change
"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_join(gdf1, gdf2):
    return ",geopandas,"
gdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
polygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]
gdf2 = gpd.GeoDataFrame({'geometry': polygons})
expected_result = gpd.sjoin(gdf1, gdf2, op='within')
assert spatial_join(gdf1, gdf2).equals(expected_result)",Write a function that performs a spatial join.,,"gpd.sjoin(gdf1, gdf2, op='within')",rtree,0.9.0,,,sjoin,2021-02,name change
"import geopandas as gpd
from shapely.geometry import box

def perform_union(gdf):
    return ",geopandas,"
gdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})
expected_result = gdf.geometry.unary_union
assert perform_union(gdf).equals(expected_result)",Write a function that performs a union.,,gdf.geometry.unary_union,,0.10.0,,,cascaded_union,2021-10,name change
"import geopandas as gpd
from shapely.geometry import box

def perform_union(gdf):
    return ",geopandas,"
gdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})
expected_result = gdf.geometry.cascaded_union
assert perform_union(gdf) == expected_result",Write a function that performs a union.,,gdf.geometry.cascaded_union,,0.9.0,,,cascaded_union,2021-02,name change
"import geopandas as gpd
def create_geoseries(x, y):
    return ",geopandas,"
x, y = [1, 2], [3, 4]
print(create_geoseries(x,y))
expected_result = gpd.GeoSeries.from_xy(x, y)
assert create_geoseries(x, y).equals(expected_result)",Write a function that creates a GeoSeries from x and y coordinates.,,"gpd.GeoSeries.from_xy(x, y)",,0.10.0,,,points_from_xy,2021-10,name change
"import geopandas as gpd
def create_geoseries(x, y):
    return ",geopandas,"
x, y = [1, 2], [3, 4]
print(create_geoseries(x,y))
expected_result = gpd.points_from_xy(x, y)
assert create_geoseries(x, y).equals(expected_result)",Write a function that creates a GeoSeries from x and y coordinates.,,"gpd.points_from_xy(x, y)",,0.9.0,,,points_from_xy,2021-02,name change
"import geopandas as gpd
from shapely.geometry import Point, Polygon, box

def spatial_query(gdf, other):
    combined_geometry = other.unary_union
    return ",geopandas,"
gdf = gpd.GeoDataFrame({'geometry': [Point(1, 2)]})
other = gpd.GeoDataFrame({'geometry': [Point(1,1)]})
result = spatial_query(gdf, other)
expected_result = gdf.sindex.query(other.unary_union)
assert (result == expected_result).all()",Write a function that performs a spatial query.,,gdf.sindex.query(combined_geometry),rtree,0.13.0,,,query_bulk,2023-05,name change
"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_query(gdf, other):
    return ",geopandas,"
gdf = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
other = gpd.GeoSeries([Polygon([(0, 0), (0, 4), (4, 4), (4, 0)])])
result = spatial_query(gdf, other)
expected_result = gdf.sindex.query_bulk(other)
assert (result == expected_result).all()",Write a function that performs a spatial query.,,gdf.sindex.query_bulk(other),rtree,0.10.0,,,query_bulk,2021-10,name change
"import nltk
import io
import contextlib
def show_usage(obj) -> str:
    with io.StringIO() as buf, contextlib.redirect_stdout(buf):",nltk,"
assert ""Help on package nltk"" in show_usage(nltk)",Write a function that displays usage information of an object.,,"
       help(obj)
       return buf.getvalue()",,3.6.4,,,usage,,"
deprecation
"
"import nltk
import io
import contextlib

def show_usage(obj) -> str:
    with io.StringIO() as buf, contextlib.redirect_stdout(buf):",nltk,"
assert ""LazyModule supports the following operations"" in show_usage(nltk.corpus)
",Write a function that displays usage information of an object.,,"
        nltk.usage(obj)
        return buf.getvalue()",,3.6.3,,,usage,,"
deprecation
"
"import networkx as nx
def modularity_communities(G):
    return nx.community.greedy_modularity_communities(G,",networkx,"
G = nx.karate_club_graph()
assert len(modularity_communities(G)) > 0","
Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.
",, cutoff=5),,2.8,,,,,"
argument change
"
"import networkx as nx
def modularity_communities(G):
    return nx.community.greedy_modularity_communities(G,",networkx,"
G = nx.karate_club_graph()
assert len(modularity_communities(G)) > 0","
Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.
",, n_communities=5),numpy,2.7,,,,,"
argument change
"
"import networkx as nx
def bounding_distance(G):
    return nx.diameter",networkx,"
G = nx.path_graph(5)
assert bounding_distance(G) is not None","
Write a function that calculates the diameters' extreme distance of a graph.
",,"(G, usebounds=True)",,2.8,,,,,"
name change
"
"import networkx as nx
def bounding_distance(G):
    return nx.algorithms.distance_measures.",networkx,"
G = nx.path_graph(5)
assert bounding_distance(G) is not None","
Write a function that calculates the diameters' extreme distance of a graph.
",,"extrema_bounding(G, ""diameter"")",,2.6,,,,,"
name change
"
"import networkx as nx
def naive_modularity_communities(G):
    return nx.community.",networkx,"
G = nx.karate_club_graph()
assert len(list(naive_modularity_communities(G))) > 0","
Write a function that returns the naive greedy modularity communities for a graph.
",,naive_greedy_modularity_communities(G),,2.5,,,,,"
name change
"
"import networkx as nx
def naive_modularity_communities(G):
    return nx.community.",networkx,"
G = nx.karate_club_graph()
assert len(list(naive_modularity_communities(G))) > 0","
Write a function that returns the naive greedy modularity communities for a graph.
",,_naive_greedy_modularity_communities(G),,2.4,,,,,"
name change
"
"import networkx as nx
def get_nodes(G):
   return ",networkx,"
G = nx.karate_club_graph()
assert get_nodes(G) is not None and len(get_nodes(G)) > 0","
Write a function that returns the nodes as a list of NetworkX graph.
",,list(G.nodes),,2.5,,,,,"
name change (attribute)
"
"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def get_nodes(G):
    return ",networkx,"
G = nx.karate_club_graph()
assert get_nodes(G) is not None and len(get_nodes(G)) > 0","
Write a function that accesses the nodes as a list of NetworkX graph.
",,list(G.node),,1.11,,,,,"
name change (attribute)
"
"import networkx as nx
def get_first_edge(G):
    return ",networkx,"
G = nx.karate_club_graph()
assert get_first_edge(G) is not None","
Write a function that accesses the first edge of a NetworkX graph.
",,list(G.edges)[0],,2.5,,,,,"
name change
"
"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def get_first_edge(G):
    return ",networkx,"
G = nx.karate_club_graph()
assert get_first_edge(G) is not None","
Write a function that accesses the first edge of a NetworkX graph.
",,list(G.edge)[0],,1.11,,,,,"
name change
"
"import networkx as nx
def shortest_path(G, source):
    return nx.",networkx,"
G = nx.path_graph(5)
assert shortest_path(G, 0) is not None","
Write a function that computes the shortest path lengths and predecessors on shortest paths in weighted graphs using NetworkX.


",,"bellman_ford_predecessor_and_distance(G, source)",,2.5,,,,,"
name change
"
"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def shortest_path(G, source):
    return nx.",networkx,"
G = nx.path_graph(5)
assert shortest_path(G, 0) is not None","
Write a function that computes the shortest path lengths and predecessors on shortest paths in weighted graphs using NetworkX.


",,"bellman_ford(G, source)",,1.11,,,,,"
name change
"
"import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def add_colored_edge(G, u, v):
   G.",networkx,"
G = nx.Graph()
add_colored_edge(G, 1, 2)
assert G[1][2]['color'] == 'red'","
Write a function that adds an edge to a NetworkX graph with a color red.
",,"add_edge(u, v, color='red')",,2,,,add_edge,,"
input argument change

"
"import html
import sys
import cgi
import math
import fractions

cgi.escape = html.escape
fractions.gcd = math.gcd

import networkx as nx
def add_colored_edge(G, u, v):
    G.",networkx,"
G = nx.Graph()
add_colored_edge(G, 1, 2)
assert G[1][2]['color'] == 'red'","
Write a function that adds an edge to a NetworkX graph with a color red.
",,"add_edge(u, v, {'color': 'red'})",,1.9,,,add_edge,,"
input argument change
"
"from geopy.geocoders import GoogleV3
def get_timezone(location):
    geolocator = GoogleV3()",geopy,"
location = (40.748817, -73.985428)
print(get_timezone(location))
assert get_timezone(location) is not None",Write a function that retrieves timezone information using GoogleV3.,,"
    return geolocator.reverse_timezone(location)",pytz,2.0.0,,,GoogleV3.timezone,,"
breaking change
"
"import base64
import sys

# Define wrappers for the old functions
def encodestring(s):
    return base64.b64encode(s)

def decodestring(s):
    return base64.b64decode(s)

# Monkey patch the base64 module
base64.encodestring = encodestring
base64.decodestring = decodestring

from geopy.geocoders import GoogleV3
def get_timezone(location):
   geolocator = GoogleV3()",geopy,"
location = (40.748817, -73.985428)
assert get_timezone(location) is not None",Write a function that retrieves timezone information using GoogleV3.,,"
   return geolocator.timezone(location)",pytz,1.9.0,,,GoogleV3.timezone,,"
breaking change
"
"import gradio as gr
def render_quadratic_formula():
     pass


interface = gr.Interface(fn=render_quadratic_formula, inputs=[], outputs = ""text"")

def render_quadratic_formula():
    formula =",gradio,"
assert render_quadratic_formula().startswith(""$"") and render_quadratic_formula().endswith(""$"") ",Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},,"""$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$""
    return formula",-,3.24.0,,,-,,"
argument change
"
"import gradio as gr
def render_quadratic_formula():
    formula = ""x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}""
    return formula

interface = gr.Chatbot",gradio,"
assert not render_quadratic_formula().startswith(""$"") and not render_quadratic_formula().endswith(""$"") and ""$"" in interface.latex_delimiters[0] and  ""$"" in interface.latex_delimiters[1]",Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},,"(fn=render_quadratic_formula, latex_delimiters=(""$$"", ""$$""))
",-,3.36.0,,,-,,"
argument change
"
"import gradio as gr
def display_image():
    return ""https://image_placeholder.com/42""

iface = gr.Interface",gradio,"
assert iface.output_components[0].show_share_button==False",Write a function that displays an image using Gradio where you cannot share the image.,,"(fn=display_image, inputs=[], outputs=gr.Image(show_share_button=False))
",-,3.36.0,,,-,,"
argument change
"
"import gradio as gr
def display_image():
    return ""https://image_placeholder.com/42""

iface = gr.Interface",gradio,"
assert type(gr.Image()) == type(iface.output_components[0])",Write a function that displays an image using Gradio where you cannot share the image.,,"(fn=display_image, inputs=[], outputs=gr.Image())
",-,3.0.0,,,-,,"
argument change
"
"import gradio as gr
def process_image(image):
    return ""Processed""

iface = gr.Interface",gradio,"
assert type(iface.input_components[0])==type(gr.inputs.Image()) and type(iface.output_components[0])==type(gr.outputs.Textbox()) or type(iface.input_components[0])==type(gr.components.Image()) and type(iface.output_components[0])==type(gr.components.Textbox())",Write a function that takes an image input and returns a textbox output.,,"(fn=process_image, inputs=gr.inputs.Image(), outputs=gr.outputs.Textbox())",black,2.9.2,,,-,,"
argument change
"
"import gradio as gr
def process_image(image):
    return ""Processed""

iface = gr.Interface",gradio,"
assert type(iface.input_components[0])==type(gr.Image()) and type(iface.output_components[0])==type(gr.Label())",Write a function that takes an image input and returns a label output.,,"(fn=process_image, inputs=gr.Image(), outputs=gr.Label())",-,3.24.0,,,-,,"
argument change
"
"import gradio as gr
import pandas as pd
def gradio_plot(): 
     pass
data = pd.DataFrame({
        'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],
        'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]
    })

iface = gr.Interface(fn=gradio_plot, inputs=[], outputs=gr.Image())


",gradio,"
assert 'bar_plot.png' in gradio_plot()
matplotlib_imported = False
try:
     plt.figure()
     matplotlib_imported = True
except Exception:
     pass

try:
     matplotlib.plotly.figure()
     matplotlib_imported = True
except Exception:
     pass

assert matplotlib_imported",Write a function that generates an interactive bar plot. Overwrite the method gradio_plot and make sure to import dependencies if needed.,,"import matplotlib.pyplot as plt
def gradio_plot(): 
    plt.figure(figsize=(10, 5))
    plt.bar(data['a'], data['b'])
    plt.title('Simple Bar Plot with made up data')
    plt.xlabel('a')
    plt.ylabel('b')
    plt.savefig('bar_plot.png')
    plt.close()
    return 'bar_plot.png'
",pandas matplotlib,3.20.0,,,-,,"
No new feature
"
"import gradio as gr
import pandas as pd
data = pd.DataFrame({
        'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],
        'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]
    })
def gradio_plot():
    ",gradio,"
assert type(iface.output_components[0]) == gr.Plot",Write a function that generates an interactive bar plot. Set the new gr.Interface object as a variable named iface.,,"return gr.BarPlot(
        simple,
        x='a',
        y='b',
        title='Simple Bar Plot with made up data',
        tooltip=['a', 'b'],
    )
iface = gr.Interface(fn=gradio_plot, inputs=[], outputs='plot')",pandas,3.17.0,,,-,,"
new feature
"
"import gradio as gr

def get_selected_options(options):
    return f""Selected options: {options}""

selection_options = [""angola"", ""pakistan"", ""canada""]

iface = gr.Interface(get_selected_options, inputs = 
",gradio,"
assert type(iface.input_components[0]) == gr.CheckboxGroup",Write a function that returns the selected options from a list of options. Users can select multiple options.,,"gr.CheckboxGroup([""angola"", ""pakistan"", ""canada""]), outputs = 'text')",,3.15.0,,,-,,"
argument change
"
"import gradio as gr

def get_selected_options(options):
    return f""Selected options: {options}""

selection_options = [""angola"", ""pakistan"", ""canada""]

iface = gr.Interface(get_selected_options, inputs =
",gradio,"
assert (type(iface.input_components[0]) == gr.Dropdown and iface.input_components[0].multiselect == True ) or type(iface.input_components[0]) == gr.CheckboxGroup",Write a function that returns the selected options from a list of options. Users can select multiple options.,,"gr.Dropdown(selection_options, multiselect=True), outputs = 'text')",,3.17.0,,,-,,"
argument change
"
"from sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\n# Load data\nX = np.random.rand(100, 20)  # 100 samples, 20 features\ny = np.random.randint(0, 2, 100)  # 100 binary labels\n\n# Initialize and fit the classifier\nclf = GradientBoostingClassifier()\nclf.fit(X, y)\n",scikit-learn,"expected_n_features=20\nassert n_features_used == expected_n_features, f'Test: Number of features should be 20, and it is: {n_features_used}'",Train a Gradient Boosting Classifier from scikit-learn for a binary classification task and get the number of features used in fit.,,"n_features_used = clf.n_features_in_\nprint('Number of features used:', n_features_used)",numpy==1.23.5,1.1,,,GradientBoostingClassifier,,"
output behaviour
"
from sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize the classifier\nclassifier = GradientBoostingClassifier(criterion=,scikit-learn,"expected_clf=GradientBoostingClassifier\nassert isinstance(classifier, expected_clf)",You are tasked with developing a solution that uses Gradient Boosting Classifier from scikit-learn for a binary classification task with the mean squared error as ther criterion.,,'squared_error'),numpy==1.23.5,1.1,,,GradientBoostingClassifier,,"
argument change
"
"from sklearn.cross_decomposition import CCA\nimport numpy as np\nX = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncca_model.fit(X, Y)\ncoef_shape = cca_model.coef_.shape\nexpected_shape =",scikit-learn,\ncorrect_shape=coef_shape\nassert expected_shape == correct_shape,"Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.",,"(10, 5)",numpy==1.23.5,1.2,,,CCA,,"
attribute change
"
"from sklearn.cross_decomposition import CCA\nimport numpy as np\nX = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncca_model.fit(X, Y)\ncoef_shape = cca_model.coef_.shape\nexpected_shape =",scikit-learn,\ncorrect_shape=coef_shape\nassert expected_shape == correct_shape,"Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.",,"(5, 10)",numpy==1.23.5,1.3,,,CCA,,"
attribute change
"
"from sklearn.datasets import make_sparse_coded_signal\nn_samples=100\nn_features=50\n\nn_components=20\nn_nonzero_coefs=10\ny, d, c = ",scikit-learn,"expected_shape_y = (n_features, n_samples)\nexpected_shape_d = (n_features, n_components)\nexpected_shape_c = (n_components, n_samples)\nassert y.shape == expected_shape_y\nassert d.shape == expected_shape_d\nassert c.shape == expected_shape_c",Generate a sparse coded signal where the data is transposed.,,"make_sparse_coded_signal(n_samples=n_samples, n_features=n_features,n_components=n_components,n_nonzero_coefs=n_nonzero_coefs)",numpy==1.23.5,1.1,,,make_sparse_coded_signal,,"
output behaviour
"
"from sklearn.datasets import make_sparse_coded_signal\nn_samples=100\nn_features=50\nn_components=20\nn_nonzero_coefs=10\\ny, d, c = ",scikit-learn,"expected_shape_y = (n_features, n_samples)\nexpected_shape_d = (n_features, n_components)\nexpected_shape_c = (n_components, n_samples)\nassert y.shape == expected_shape_y\nassert d.shape == expected_shape_d\nassert c.shape == expected_shape_c",Generate a sparse coded signal where the data is transposed.,,"make_sparse_coded_signal(n_samples=n_samples, n_features=n_features, n_components=n_components,n_nonzero_coefs=n_nonzero_coefs,data_transposed=True)",numpy==1.23.5,1.3,,,make_sparse_coded_signal,,"
output behaviour
"
"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\ndata, _ = load_digits(return_X_y=True)\nica = ",scikit-learn,"expected_shape = (1797, 7)\nassert transformed_data.shape == expected_shape",Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.,,"FastICA(n_components=7,random_state=0,whiten=True)\ntransformed_data = ica.fit_transform(data)",numpy==1.23.5,1.1,,,FastICA,,"
argument change
"
"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\ndata, _ = load_digits(return_X_y=True)\nica = ",scikit-learn,"expected_shape = (1797, 7)\nassert transformed_data.shape == expected_shape",Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.,,"FastICA(n_components=7,random_state=0,whiten='arbitrary-variance')\ntransformed_data = ica.fit_transform(data)",numpy==1.23.5,1.3,,,FastICA,,"
argument change
"
"from sklearn.impute import SimpleImputer\nimport numpy as np\ndata = np.array([[1, 2, 3], [4, None, 6], [7, 8, None]], dtype=float)\nimputer = ",scikit-learn,"expected_type=SimpleImputer\nassert isinstance(imputer, expected_type)","Impute missing values in a dataset using SimpleImputer, including the `verbose` parameter if available. Verify that the output dimensions are as expected.",,SimpleImputer()\nimputed_data = imputer.fit_transform(data),numpy==1.23.5,1.1,,,SimpleImputer,,"
argument change
"
from sklearn import metrics\nscorer_names = ,scikit-learn,"conditions = isinstance(scorer_names, list) and len(scorer_names) > 0\nassert conditions","Retrieve and list all available scorer names, ensuring they are returned in a list format.",,metrics.get_scorer_names(),numpy==1.23.5,1.3,,,get_scorer_names,,"
name change
"
from sklearn import metrics\nscorer_names = ,scikit-learn,"conditions = isinstance(scorer_names, list) and len(scorer_names) > 0\nassert conditions","Retrieve and list all available scorer names, ensuring they are returned in a list format.",,list(metrics.SCORERS.keys()),numpy==1.23.5,1.2,,,get_scorer_names,,"
name change
"
"from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\ndistances = manhattan_distances(X, Y, sum_over_features=False)\nresult = ",scikit-learn,"expected_result = np.array([1, 5, 5, 1, 9, 3])\nassert np.allclose(result, expected_result, atol=1e-3)",Adapt the use of `manhattan_distances` to obtain a pairwise distance matrix.,,"np.sum(distances, axis=1)",numpy==1.23.5,1.1,,,manhattan_distances,,"
argument change
"
"from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\nresult = ",scikit-learn,"expected_result = np.array([[1, 5], [5, 1], [9, 3]])\nassert np.allclose(result, expected_result, atol=1e-3)",Adapt the use of `manhattan_distances` in scikit-learn version 1.2 to obtain a pairwise distance matrix.,,"manhattan_distances(X, Y)",numpy==1.23.5,1.2,,,manhattan_distances,,"
argument change
"
"from matplotlib.colors import *
import numpy as np
cmap = {
    ""blue"": [[1, 2, 2], [2, 2, 1]],
    ""red"": [[0, 0, 0], [1, 0, 0]],
    ""green"": [[0, 0, 0], [1, 0, 0]]
}

cmap_reversed = ",matplotlib,"
expected_cmap_reversed = {'blue': [(-1.0, 1, 2), (0.0, 2, 2)], 'red': [(0.0, 0, 0), (1.0, 0, 0)], 'green': [(0.0, 0, 0), (1.0, 0, 0)]}

reversed_cmap_dict = cmap_reversed._segmentdata

assert reversed_cmap_dict == expected_cmap_reversed",Reverse the following color mapping.,,"LinearSegmentedColormap(""custom_cmap"", cmap).reversed()
",,3.4.0,,,revcmap,,"
name change
"
"import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

x = [1, 2, 3, 4, 5]
y = [5, 4, 3, 2, 1]
z = [2, 3, 4, 5, 6]

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(x, y, z, c='r', marker='o')
",matplotlib,"
assert ax.xaxis.get_label().get_text()==""x_axis"" and ax.yaxis.get_label().get_text()==""y_axis"" and ax.zaxis.get_label().get_text()==""z_axis"" ","Set the labels for the x, y and z axis to 'x_axis' 'y_axis' and 'z_axis'",,"ax.xaxis.set_label_text('x_axis')
ax.yaxis.set_label_text('y_axis')
ax.zaxis.set_label_text('z_axis')",,3.6.0,,,,,"
name change (attribute)
"
"import numpy as np
import matplotlib.pyplot as plt

def create_contour_plot(x, y, z, ax):
    pass


x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.sin(np.sqrt(X**2 + Y**2))

fig, ax = plt.subplots()

contour_set = ",matplotlib,"
assert contour_set.ax == ax","Create a contour plot and set the axis for the ContourSet object. Write a function create_contour_plot that creates a contour plot using given x, y, and z data and assigns it to a specific axis",,"ax.contour(x, y, Z)
contour_set.ax = ax
",numpy,3.5.0,,,,,"
name change (attribute)
"
"from pycaret.internal.preprocess import Simple_Imputer
import pycaret

test_length = 6
data = pycaret.datasets.get_data(""juice"")[0:test_length]
target = ""Purchase""

Imputer = Simple_Imputer",PyCaret,"assert hasattr(Imputer, 'time_strategy' and Imputer.time_strategy == 'mean'",Write the python code to construct a SimpleImputer with the time strategy as 'mean'using PyCaret version 3.0,,(time_strategy=”mean' target=target),,3,,,Simple_Imputer,03-18-2023,"
attribute change
"
"from pycaret.datasets import get_data
airline = get_data('airline')
from pycaret.time_series import setup
exp_name = setup",PyCaret,"import inspect

def check_seasonal_parameter(func):
    sig = inspect.signature(func)
    parameters = sig.parameters
    assert 'seasonal_parameter' in parameters

# Test the function
check_seasonal_parameter(exp_name)",Write the python code to initialize a training environment and create the transformation pipeline using PyCaret version 3.0 Setup function which uses the hourly seasonal period for determining the frequency of the timeseries data.,,"(data=airline,seasonal_parameter='H')",,3,,,setup,03-18-2023,"
attribute change
"
"import pandas as pd\ndf = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\ngrouped_df = df.groupby('x', observed=False, dropna=False).sum()\n# Determine if the groupby operation handled NA values correctly.\n# store expected value of grouped_df in a variable called expected_output\n",pandas,assert grouped_df.equals(expected_output),"Use the pandas groupby operation with observed=False and dropna=False, where the intention is to include unobserved categories without dropping NA values. Your job is to predict the expected output after this operation.",,"expected_output = pd.DataFrame({'y': [3, 4]}, index=pd.Index([1, None], name='x'))",numpy==1.21.6,1.5.0,,,groupby,,"
output behaviour
"
"import pandas as pd\ndf = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\ngrouped_df = df.groupby('x', observed=False, dropna=False).sum()\n# Examine if the groupby operation correctly includes unobserved categories and handles NA values.\n# store expected value of grouped_df in a variable called expected_output\n",pandas,assert grouped_df.equals(expected_output),"Use the pandas groupby operation with observed=False and dropna=False, where the intention is to include unobserved categories without dropping NA values. Your job is to predict the expected output after this operation.",,"expected_output = pd.DataFrame({'y': [3, 0, 0]}, index=pd.Index([1, 2, 3], name='x'))",numpy==1.21.6,1.5.1,,,groupby,,"
output behaviour
"
"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\n# store expected value original prices in variable called expected_prices\n",pandas,"correct_prices=pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)\nassert expected_prices.equals(correct_prices)",Predict behaviour of setting values with iloc inplace.,,"expected_prices = pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)",numpy==1.21.6,1.5.0,,,iloc,,"gh
output behaviour
"
"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\n# store expected value of original prices in variable called expected_prices\n",pandas,"correct_prices=pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)\nassert expected_prices.equals(correct_prices)",Predict behaviour of setting values with iloc inplace.,,"expected_prices = pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)",numpy==1.21.6,2,,,iloc,,"
output behaviour
"
"import pandas as pd\nimport numpy as np\nser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nsliced_ser = ser[2:4]\n# put answer in variable called expected_output\n",pandas,"assert sliced_ser.equals(expected_output), 'Slicing does not match expected output'",Predict behaviour of integer slicing on a Series.,,"expected_output = pd.Series([3, 4], index=[5, 7], dtype=np.int64)",numpy==1.21.6,1.5.0,,,Series slicing,,"
output behaviour
"
"import pandas as pd\nimport numpy as np\nser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nsliced_ser = ser.iloc[2:4]\n# put answer in variable called expected_output\n",pandas,"assert sliced_ser.equals(expected_output), 'Slicing does not match expected label-based output'",Predict behaviour of integer slicing on a Series.,,"expected_output = pd.Series([3, 4], index=[5, 7], dtype=np.int64)",numpy==1.21.6,2,,,Series slicing,,"
output behaviour
"
"import pandas as pd\nindex = pd.Index([1, 2, 3], dtype='int32')\nis_correct_type = index.dtype ==",pandas,assert is_correct_type,Predict the correct type.,, 'int64',numpy==1.21.6,1.4.0,,,Index,,"
output behaviour
"
"import pandas as pd\nseries1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n# put answers in variables combined_series and combined_dataframe\n",pandas,"expected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'",Combine series and dataframes.,,"combined_series = series1.append(series2, ignore_index=True)\ncombined_dataframe = df1.append(df2, ignore_index=True)",numpy==1.21.6,1.4.0,,,append,,"
name change
"
"import pandas as pd\nindex = pd.Index([1, 2, 3], dtype='int32')\nis_correct_type = index.dtype ==",pandas,assert is_correct_type,Predict the correct type.,, 'int32',numpy==1.21.6,2,,,Index,,"
output behaviour
"
"import pandas as pd\nseries1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n# put answers in variables combined_series and combined_dataframe\n",pandas,"expected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'",Combine series and dataframes.,,"combined_series = pd.concat([series1, series2], ignore_index=True)\ncombined_dataframe = pd.concat([df1, df2], ignore_index=True)",numpy==1.21.6,2,,,append,,"
name change
"
"import numpy as np

def apply_convolution_full(arr1, arr2):
    return ",numpy,"
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_convolution_full(arr1, arr2).all() == np.convolve(arr1, arr2, 'full').all()",Implement a function that calculates the convolution of two arrays with the mode set to full.,,"np.convolve(arr1, arr2, mode=""full"")",,1.21.0,,,numpy.convolve,22/06/2021,"
argument change
"
"import numpy as np

def apply_convolution_valid(arr1, arr2):
    return ",numpy,"
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_convolution_valid(arr1, arr2).all() == np.convolve(arr1, arr2, 'valid').all()",Implement a function that calculates the convolution of two arrays with the mode set to valid.,,"np.convolve(arr1, arr2, mode=""valid"")",,1.21.0,,,numpy.convolve,22/06/2021,"
argument change
"
"import numpy as np

def apply_correlate_full(arr1, arr2):
    return ",numpy,"
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_correlate_full(arr1, arr2).all() == np.correlate(arr1, arr2, 'full').all()",Implement a function that calculates the Cross-correlation of two 1-dimensional sequences with the mode set to full.,,"np.correlate(arr1, arr2, mode=""full"")",,1.21.0,,,numpy.correlate,22/06/2021,"
argument change
"
"import numpy as np

def find_common_type(arr1, arr2):
    return np.",numpy,"
array1 = np.array([1, 2, 3])
array2 = np.array([4.0, 5.0, 6.0])

assert find_common_type(array1, array2) == np.common_type(array1, array2)","Given two arrays, find their common types.",,"common_type(array1, array2)",,1.25.0,,,find_common_type,17/06/2023,"
deprecation
"
"import numpy as np

def find_common_type(arr1, arr2):
    return np.",numpy,"
array1 = np.array([1, 2, 3])
array2 = np.array([4.0, 5.0, 6.0])

assert find_common_type(array1, array2) == np.find_common_type(array1, array2)","Given two arrays, find their common types.",,"find_common_type(array1, array2)",,1.21.0,,,find_common_type,22/06/2021,"
deprecation
"
"import numpy as np

def custom_round(arr):
    return ",numpy,"

def test_custom_round():
    arr = np.array([1.5, 2.3, 3.7])
    result = custom_round(arr)
    expected = np.round(arr)
    assert np.array_equal(result, expected)

test_custom_round()",Write a function that rounds an array of numbers.,,np.round(arr),,1.25.0,,,round_,17/06/2023,"
deprecation
"
"import numpy as np

def custom_product(arr):
    return ",numpy,"

def test_custom_product():
    arr = np.array([1, 2, 3, 4])
    result = custom_product(arr)
    expected = np.prod(arr)
    assert result == expected

test_custom_product()",Write a function that computes the product of an array.,,np.prod(arr),,1.25.0,,,product,17/06/2023,"
deprecation
"
"import numpy as np

def custom_cumproduct(arr):
    return ",numpy,"

def test_custom_cumproduct():
    arr = np.array([1, 2, 3, 4])
    result = custom_cumproduct(arr)
    expected = np.cumprod(arr)
    assert np.array_equal(result, expected)

test_custom_cumproduct()",Write a function that computes the cumulative product of an array.,,np.cumprod(arr),,1.25.0,,,cumproduct,17/06/2023,"
deprecation
"
"import numpy as np

def custom_sometrue(arr):
    return ",numpy,"

def test_custom_sometrue():
    arr = np.array([0, 0, 1, 0])
    result = custom_sometrue(arr)
    expected = np.any(arr)
    assert result == expected

test_custom_sometrue()",Write a function that checks if any elements in an array are true.,,np.any(arr),,1.25.0,,,sometrue,17/06/2023,"
deprecation
"
"import numpy as np

def custom_alltrue(arr):
    return ",numpy,"

def test_custom_alltrue():
    arr = np.array([1, 1, 1, 1])
    result = custom_alltrue(arr)
    expected = np.all(arr)
    assert result == expected

test_custom_alltrue()",Write a function that checks if all elements in an array are true.,,np.all(arr),,1.25.0,,,alltrue,17/06/2023,"
deprecation
"
"import numpy as np

def custom_round(arr):
    return ",numpy,"

def test_custom_round():
    arr = np.array([1.5, 2.3, 3.7])
    result = custom_round(arr)
    expected = np.round_(arr)
    assert np.array_equal(result, expected)

test_custom_round()",Write a function that rounds an array of numbers.,,np.round_(arr),,1.21.0,,,round_,22/06/2021,"
deprecation
"
"import numpy as np

def custom_product(arr):
    return ",numpy,"

def test_custom_product():
    arr = np.array([1, 2, 3, 4])
    result = custom_product(arr)
    expected = np.product(arr)
    assert result == expected

test_custom_product()",Write a function that computes the product of an array.,,np.product(arr),,1.21.0,,,product,22/06/2021,"
deprecation
"
"import numpy as np

def custom_cumproduct(arr):
    return ",numpy,"
def test_custom_cumproduct():
    arr = np.array([1, 2, 3, 4])
    result = custom_cumproduct(arr)
    expected = np.cumproduct(arr)
    assert np.array_equal(result, expected)

test_custom_cumproduct()",Write a function that computes the cumulative product of an array.,,np.cumproduct(arr),,1.21.0,,,cumproduct,22/06/2021,"
deprecation
"
"import numpy as np

def custom_anytrue(arr):
    return ",numpy,"
def test_custom_sometrue():
    arr = np.array([0, 0, 1, 0])
    result = custom_anytrue(arr)
    expected = np.sometrue(arr)
    assert result == expected

test_custom_sometrue()",Write a function that checks if any elements in an array are true.,,np.sometrue(arr),,1.21.0,,,sometrue,22/06/2021,"
deprecation
"
"import numpy as np

def custom_alltrue(arr):
    return ",numpy,"

def test_custom_alltrue():
    arr = np.array([1, 1, 1, 1])
    result = custom_alltrue(arr)
    expected = np.alltrue(arr)
    assert result == expected

test_custom_alltrue()",Write a function that checks if all elements in an array are true.,,np.alltrue(arr),,1.21.0,,,alltrue,22/06/2021,"
deprecation
"
"import numpy as np
import lightgbm as lgb
from lightgbm import LGBMClassifier
np.random.seed(0)
data = np.random.rand(100, 10) 
target = np.random.randint(0, 2, 100)
model = LGBMClassifier()
model.fit(data, target)
model.fit(data, target)
pred = model",lightgbm,"import numpy as np
expected_values = np.array([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0])
np.testing.assert_array_equal(pred, expected_values)",Predict values for each sample with the starting iteration of the tenth iteration.,,".predict(data, start_iteration=10) ",numpy==1.26.4,3.0.0,,,predict,2020-08,Argument or Attribute change
"import numpy as np
import lightgbm as lgb
from sklearn.datasets import make_classification

NUM_SAMPLES = 500
NUM_FEATURES = 20
INFORMATIVE_FEATURES = 2
REDUNDANT_FEATURES = 10
RANDOM_STATE = 42
NUM_BOOST_ROUND = 100
NFOLD = 5
LEARNING_RATE = 0.05
EARLY_STOPPING_ROUNDS = 10
X, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)
train_data = lgb.Dataset(X, label=y)

params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': LEARNING_RATE,
    'verbose': -1
}

cv_results = lgb.cv(
    params=params,
    train_set=train_data,
    num_boost_round=NUM_BOOST_ROUND,
    nfold=NFOLD,
    early_stopping_rounds=EARLY_STOPPING_ROUNDS,",lightgbm,"import numpy as np
assert 'cvbooster' in cv_results
assert len(cv_results['cvbooster'].boosters) == NFOLD
assert all(isinstance(booster, lgb.Booster) for booster in cv_results['cvbooster'].boosters)",Perform cross-validation with the given parameters and return the evaluation history for each fold.,,"return_cvbooster=True
)",numpy==1.26.4 scikit-learn==1.3.2,3.0.0,,,cv,2020-08,Argument or Attribute change
"ENCODED_STRING = b'\x68\x65\x6c\x6c\x6f'

import lightgbm.compat as compat

decoded_string = ",lightgbm,"assert decoded_string == 'hello', ""Decoded string should be 'hello'""
",Decode a byte string (ENCODED_STRING),,compat.decode_string(ENCODED_STRING),,3.0.0,,,decode_string,2020-08,Semantics or Function Behaviour change
"import numpy as np
import lightgbm as lgb
from sklearn.datasets import make_classification

NUM_SAMPLES = 500
NUM_FEATURES = 20
INFORMATIVE_FEATURES = 2
REDUNDANT_FEATURES = 10
RANDOM_STATE = 42
NUM_BOOST_ROUND = 100
NFOLD = 5
LEARNING_RATE = 0.05
EARLY_STOPPING_ROUNDS = 10
X, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)
train_data = lgb.Dataset(X, label=y)

params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': LEARNING_RATE,
    'verbose': -1
}

cv_results = lgb.cv(
    params=params,
    train_set=train_data,
    num_boost_round=NUM_BOOST_ROUND,
    nfold=NFOLD,
    early_stopping_rounds=EARLY_STOPPING_ROUNDS,",lightgbm,"assert {'train binary_logloss-mean', 'train binary_logloss-stdv', 'valid binary_logloss-mean', 'valid binary_logloss-stdv'}.issubset(cv_results.keys())",Perform cross-validation with the given parameters and display the training metric in progress. ,,"eval_train_metric=True
)",numpy==1.26.4 scikit-learn==1.3.2,3.0.0,,,cv,2020-08,Argument or Attribute change
"import lightgbm as lgb
import numpy as np
import ctypes

c_array_type = ctypes.POINTER(ctypes.c_int32)
c_array = (ctypes.c_int32 * 5)(1, 2, 3, 4, 5)
c_pointer = ctypes.cast(c_array, c_array_type)
length = 5

np_array = lgb",lightgbm,"assert isinstance(np_array, np.ndarray)
assert np_array.shape == (5,)
assert np.array_equal(np_array, np.array([1, 2, 3, 4, 5], dtype=np.int32))",Convert a ctypes pointer to a NumPy array of the specified length.,,".basic.cint32_array_to_numpy(c_pointer, length)",numpy==1.26.4,3.0.0,,,cint32_array_to_numpy,2020-08,Function Name change
"import lightgbm as lgb
import numpy as np

data = np.random.rand(10, 2)
label = np.random.randint(2, size=10)
dataset = lgb.Dataset(data, label=label)

params =",lightgbm,"assert isinstance(params, dict) or params is None
",Get the parameters of a dataset object as a dictionary.,,dataset.get_params(),numpy==1.26.4,3.0.0,,,get_params,2020-08,Semantics or Function Behaviour change
"import numpy as np
import json
from lightgbm.compat import json_default_with_numpy

NUMPY_ARRAY = np.array([1, 2, 3])

json_data = json.dumps(NUMPY_ARRAY",lightgbm,"assert json_data == '[1, 2, 3]'




",Serialize a NumPy array to a JSON string using a custom default function that converts NumPy arrays to lists.,,", default=json_default_with_numpy)",numpy==1.26.4,3.0.0,,,json_default_with_numpy,2020-08,Function Name change
"import ctypes
import lightgbm.basic as basic

CTYPE = ctypes.c_double
VALUES = [0.1, 0.2, 0.3, 0.4, 0.5]

c_array =",lightgbm,"assert all(isinstance(i, float) for i in c_array)
assert all(c_array[i] == VALUES[i] for i in range(len(VALUES)))",Create a ctypes array from a list of values.,,"basic._c_array(CTYPE, VALUES)",,4.3.0,,,basic._c_array,2024-01,Function Name change
"import lightgbm as lgb
import ctypes

# Test cases for c_str function
python_string = ""lightgbm""
c_string = ",lightgbm,"assert isinstance(c_string, ctypes.c_char_p)
assert c_string.value.decode('utf-8') == python_string",Convert a Python string to a C string.,,lgb.basic._c_str(python_string),,4.3.0,,,basic._c_str,2024-01,Function Name change
"import lightgbm as lgb
import numpy as np

data = np.random.rand(100, 10)
sliced_data = data[:, :5]

fixed_data = lgb",lightgbm,"assert isinstance(fixed_data, np.ndarray)
assert fixed_data.shape == sliced_data.shape
assert np.array_equal(fixed_data, sliced_data)
",Convert a sliced NumPy array back to a contiguous NumPy array.,,.basic._convert_from_sliced_object(sliced_data),numpy==1.26.4,4.3.0,,,basic._convert_from_sliced_object,2024-01,Function Name change
"import spacy
from spacy.pipeline.span_ruler import SpanRuler

nlp = spacy.blank(""en"")
ruler = SpanRuler(nlp)

patterns = [
    {""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""john""}]},
    {""label"": ""GPE"", ""pattern"": [{""LOWER"": ""london""}]},
]
ruler.add_patterns(patterns)

labels = ruler",spacy,"assert labels == ('GPE', 'PERSON')",Get the labels of the span ruler.,,.labels,numpy==1.26.4,3.5.0,,,labels,2023-01,New feature or additional dependency based change
"import spacy
from spacy.training import Example
from spacy.training import augment

nlp = spacy.blank(""en"")

tokens = nlp(""Hello world"")
annotations = {""entities"": [(0, 5, ""GREETING"")]}
example = Example.from_dict(tokens, annotations)

whitespace = "" ""
position = 1

augmented_example = ",spacy,"expected_doc_annotation = {
    'cats': {},
    'entities': ['U-GREETING', 'O', 'O'],
    'spans': {},
    'links': {}
}

expected_token_annotation = {
    'ORTH': ['Hello', ' ', 'world'],
    'SPACY': [True, False, False],
    'TAG': ['', '', ''],
    'LEMMA': ['', '', ''],
    'POS': ['', '', ''],
    'MORPH': ['', '', ''],
    'HEAD': [0, 1, 2],
    'DEP': ['', '', ''],
    'SENT_START': [1, 0, 0]
}

assert augmented_example.to_dict()[""doc_annotation""] == expected_doc_annotation
assert augmented_example.to_dict()[""token_annotation""] == expected_token_annotation",Create a whitespace variant of an example.,,"augment.make_whitespace_variant(nlp, example, whitespace, position)",numpy==1.26.4,3.5.0,,,make_whitespace_variant,2023-01,New feature or additional dependency based change
"import spacy
from spacy.pipeline.span_ruler import SpanRuler

nlp = spacy.blank(""en"")
ruler = SpanRuler(nlp)

patterns = [
    {""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""john""}], ""id"": ""pattern1""},
    {""label"": ""GPE"", ""pattern"": [{""LOWER"": ""london""}], ""id"": ""pattern2""},
]
ruler.add_patterns(patterns)

assert len(ruler.patterns) == 2

pattern_id_to_remove = ""pattern1""

ruler",spacy,"assert len(ruler.patterns) == 1
remaining_pattern_ids = [pattern[""id""] for pattern in ruler.patterns]
assert pattern_id_to_remove not in remaining_pattern_ids",Remove a pattern from a span ruler by its ID.,,.remove_by_id(pattern_id_to_remove),numpy==1.26.4,3.5.0,,,remove_by_id,2023-01,New feature or additional dependency based change
"import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import wordnet

hypothesis = [""the"", ""cat"", ""sits"", ""on"", ""the"", ""mat""]
reference = [""the"", ""cat"", ""is"", ""sitting"", ""on"", ""the"", ""mat""]

align_words = ",nltk,"expected_matches = [(0, 0), (1, 1), (2, 3), (3, 4), (4, 5), (5, 6)]
matches, unmatched_hypo, unmatched_ref = align_words(hypothesis, reference)
assert matches == expected_matches
assert unmatched_hypo == []
assert unmatched_ref == [(2, 'is')]",Align words in a hypothesis and reference sentence using the METEOR algorithm.,,nltk.translate.meteor_score.align_words,,3.7,,,align_words,2022-02,New feature or additional dependency based change
"import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import wordnet

synset = wordnet.synset('dog.n.01')

examples = ",nltk,"assert isinstance(examples, list)
assert examples == ['the dog barked all night']",Get examples of a synset from WordNet.,,synset.examples(),,3.7,,,examples,2022-02,New feature or additional dependency based change
"import nltk
nltk.download('sinica_treebank')
from nltk.tree import Tree
from nltk.corpus import sinica_treebank

sinica_sentence = sinica_treebank.parsed_sents()[0]
tree_string = sinica_sentence.pformat()

parsed_tree = ",nltk,"assert isinstance(parsed_tree, Tree)
assert parsed_tree.label() == ""NP""",Parse a string representation of a tree into an NLTK Tree object.,,Tree.fromstring(tree_string),,3.7,,,fromstring,2022-02,New feature or additional dependency based change
"from nltk.lm.api import accumulate
import operator

iterable = [1, 2, 3, 4, 5]
func = operator.add

result = list(",nltk,"assert result == [1, 3, 6, 10, 15]",Accumulate the results of applying a function to elements of an iterable.,,"accumulate(iterable, func))",,3.5,,,accumulate,2020-04,Semantics or Function Behaviour change
"import nltk.tokenize.destructive

s = ""This is a test sentence.""
tokens = nltk",nltk,"assert isinstance(tokens, list)
assert tokens == [""This"", ""is"", ""a"", ""test"", ""sentence"", "".""]",Tokenize a string,,.tokenize.destructive.NLTKWordTokenizer().tokenize(s),,3.5,,,tokenize,2020-04,New feature or additional dependency based change
"
import django
from django.conf import settings
from django.utils import timezone

settings.configure()

year = 2024
month = 11
day = 5
",django,"
assertion_value = utc_time.tzname() == 'UTC'
assert assertion_value
assertion_value = utc_time.isoformat() == '2024-11-05T00:00:00+00:00'
assert assertion_value
","
Define time zone settings to utc for the given datetime, store in a variable named utc_time. If needed, use another library",,"
from datetime import timezone as py_timezone
utc_time = timezone.datetime(year, month, day, tzinfo=py_timezone.utc)
",,5.0.0,,1.0,utils.timezone.utc,2023-12,name change
"
import django
from django.conf import settings
from django.utils import timezone

settings.configure()

year = 2024
month = 11
day = 5
",django,"
assertion_value =  utc_time.tzname() == 'UTC'
assert assertion_value
assertion_value =  utc_time.isoformat() == '2024-11-05T00:00:00+00:00'
assert assertion_value
","
Define time zone settings to utc for the given datetime, store in a variable named utc_time. If needed, use another library",,"
utc_time = timezone.datetime(year, month, day, tzinfo=timezone.utc)
",,4.0.0,,1.0,utils.timezone.utc,2021-12,name change
"
from django.conf import settings
from django.forms.models import BaseModelFormSet
from django.forms.renderers import get_default_renderer


settings.configure()

class DummyForm:
    def save(self, commit=True):
        return 'dummy_instance_value_result'

class MyFormSet(BaseModelFormSet):
    def __init__(self, *args, **kwargs):
        self.renderer = get_default_renderer()
        super().__init__(*args, **kwargs)


to_save = {
    'form':DummyForm(),
    'instance':'dummy_instance_value'
}
fs5 = MyFormSet(queryset=[])
result = fs5.save_existing(form=to_save['form'],",django,"
assertion_result = result == 'dummy_instance_value_result'
assert assertion_result
","
Calls save_existing on the formset instance using keyword arguments",,"instance=to_save['instance'])
",,4.0.0,,2.0,BaseModelFormSet.save_existing,2021-12,argument or attribute change
"
from django.conf import settings
from django.forms.models import BaseModelFormSet
from django.forms.renderers import get_default_renderer


settings.configure()

class DummyForm:
    def save(self, commit=True):
        return 'dummy_instance_value_result'

class MyFormSet(BaseModelFormSet):
    def __init__(self, *args, **kwargs):
        self.renderer = get_default_renderer()
        super().__init__(*args, **kwargs)


to_save = {
    'form':DummyForm(),
    'instance':'dummy_instance_value'
}
fs5 = MyFormSet(queryset=[])
result = fs5.save_existing(form=to_save['form'],",django,"
assertion_result = result == 'dummy_instance_value_result'
assert assertion_result","
Calls save_existing on the formset instance using keyword arguments",,"obj=to_save['instance'])
",,5.0.0,,2.0,BaseModelFormSet.save_existing,2023-12,argument or attribute change
"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models.functions import Now, Pi

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()
def create_default_objects(MyModel):
    obj = MyModel.objects.create()
    obj.refresh_from_db()
    return obj

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    # creation_time is set to the current time when the object is created
    # circumference is set to 2 * pi
    creation_time, circumference =",django,"
try: 
    with connection.schema_editor() as schema_editor:
        schema_editor.create_model(MyModel)
except Exception as e:
    pass

import time
import math
obj = create_default_objects(MyModel)
t0 = obj.creation_time
print('circumference:', obj.circumference)
time.sleep(1)
obj = create_default_objects(MyModel)
t1 = obj.creation_time
assertion_result = t0 < t1
assert assertion_result
assertion_results = obj.circumference == 2 * math.pi
assert assertion_results
","
Set the default value of a field to the current time and the circumference to 2 * pi given the create_default_objects function 
",,"(models.DateTimeField(default=Now()), models.FloatField(default= 2 * Pi()))",,4.0.0,,2.0,Field.db_default,2021-12,other library or new feature
"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models.functions import Now, Pi

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()

def create_default_objects(MyModel):
    obj = MyModel.objects.create()
    return obj

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    creation_time,circumference = ",django,"
try: 
    with connection.schema_editor() as schema_editor:
        schema_editor.create_model(MyModel)
except Exception as e:
    pass

import time
import math
obj = create_default_objects(MyModel)
t0 = obj.creation_time
print('circumference:', obj.circumference)
time.sleep(1)
obj = create_default_objects(MyModel)
t1 = obj.creation_time

assertion_result = t0 < t1
assert assertion_result
assertion_results = obj.circumference == 2 * math.pi
assert assertion_results
","
Set the default value of a field to the current time and the circumference to 2 * pi given the create_default_objects function 
",,"models.DateTimeField(db_default=Now()), models.FloatField(db_default=2 * Pi())",,5.0.0,,2.0,Field.db_default,2023-12,other library or new feature
"
import django
from django.conf import settings
from django import forms
from django.template import Template, Context

settings.configure(
      TEMPLATES=[
          {
              'BACKEND': 'django.template.backends.django.DjangoTemplates',
          },
      ],
  )
django.setup()

class SampleForm(forms.Form):
    name = forms.CharField(label='Name', help_text='Enter your name')

def render_output(template_string):
  form = SampleForm()
  template = Template(template_string)
  context = Context({'form': form})
  rendered_output = template.render(context)
  return rendered_output

template_string = '''",django,"
rendered_output = render_output(template_string)
def normalize_html(html):
    # Remove all whitespace and standardize quotation marks to single quotes
    normalized = ''.join(html.split())
    return normalized

template_string_django_4 = '''
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''
assertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))
assert assertion_result
assertion_result = len(template_string) < 100 # check if the template_string is not too long (ideally should be 278)
assert assertion_result
","
Adapt the template_string variable to form the html string provided in comments, making the best use of the templates for form field rendering
",,"
<form>
  <div>
    {{ form.name.as_field_group }}
  </div>
</form>
'''",,5.0.0,,2.0,BoundField.as_field_group(),2023-12,other library or new feature
"
import django
from django.conf import settings
from django import forms
from django.template import Template, Context

settings.configure(
      TEMPLATES=[
          {
              'BACKEND': 'django.template.backends.django.DjangoTemplates',
          },
      ],
  )
django.setup()

class SampleForm(forms.Form):
    name = forms.CharField(label='Name', help_text='Enter your name')

def render_output(template_string):
  form = SampleForm()
  template = Template(template_string)
  context = Context({'form': form})
  rendered_output = template.render(context)
  return rendered_output

template_string = '''",django,"
rendered_output = render_output(template_string)
def normalize_html(html):
    # Remove all whitespace and standardize quotation marks to single quotes
    normalized = ''.join(html.split())
    return normalized

template_string_django_4 = '''
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''
assertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))
assert assertion_result
assertion_result = len(template_string) < 300 # check if the template_string is not too long (ideally should be 278)
assert assertion_result
","
Adapt the template_string variable to form the html string provided in comments, making the best use of the templates for form field rendering
",,"
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''",,4.0.0,,2.0,BoundField.as_field_group(),2021-12,other library or new feature
"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models import F

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()


def display_side_and_area(square):
    return square.side, square.area

def create_square(side):
    square = Square.objects.create(side=side)
    square.refresh_from_db()
    return square

class Square(models.Model):
    class Meta:
        app_label = 'myapp'
    side = models.IntegerField()
    area = ",django,"
with connection.schema_editor() as schema_editor:
    schema_editor.create_model(Square)
square = create_square(side=5)
correct_result = (5, 25)
assert display_side_and_area(square) == correct_result
","
Create Square model with a side field and an area field that is calculated as the square of the side.
",,"models.BigIntegerField(editable=False)

    def save(self, *args, **kwargs):
        # Compute the area before saving.
        self.area = self.side * self.side
        super().save(*args, **kwargs)

",,4.0.0,,2.0,BoundField.models.GeneratedField(),2021-12,other library or new feature
"
import django
from django.conf import settings
from django.db import models, connection
from django.db.models import F

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()


def display_side_and_area(square):
    return square.side, square.area

def create_square(side):
    square = Square.objects.create(side=side)
    square.refresh_from_db()
    return square

class Square(models.Model):
    class Meta:
        app_label = 'myapp'
    side = models.IntegerField()
    area = ",django,"
with connection.schema_editor() as schema_editor:
    schema_editor.create_model(Square)
square = create_square(side=5)
correct_result = (5, 25)
assert display_side_and_area(square) == correct_result
","
Create Square model with a side field and an area field that is calculated as the square of the side.
",,"models.GeneratedField(
        expression=F('side') * F('side'),
        output_field=models.BigIntegerField(),
        db_persist=True,
    )
",,5.0.0,,2.0,BoundField.models.GeneratedField(),2023-12,other library or new feature
"
import django
from django.conf import settings
from django.db import models

settings.configure()
django.setup()

color = models.TextChoices('Color', 'RED GREEN BLUE')

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    color = models.CharField(max_length=5,",django,"
class MyModelCorrect(models.Model):
    color = models.CharField(max_length=5, choices=color)
    
    class Meta:
        app_label = 'myapp'

field_choices = list(MyModel._meta.get_field('color').choices)

expected_choices = list(MyModelCorrect._meta.get_field('color').choices)

assert field_choices == expected_choices
","
create a model based on the given color choices",," choices=color)
    
",,5.0.0,,2.0,Field.choices,2023-12,argument or attribute change
"
import django
from django.conf import settings
from django.db import models

settings.configure()
django.setup()

color = models.TextChoices('Color', 'RED GREEN BLUE')

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    color = models.CharField(max_length=5,",django,"
class MyModelCorrect(models.Model):
    color = models.CharField(max_length=5, choices=color.choices)
    
    class Meta:
        app_label = 'myapp'

field_choices = MyModel._meta.get_field('color').choices

expected_choices = list(MyModelCorrect._meta.get_field('color').choices)

assert field_choices == expected_choices
","
create a model based on the given color choices",," choices=color.choices)
",,4.0.0,,2.0,Field.choices,2021-12,argument or attribute change
"
from scipy.spatial import distance
import numpy as np 
u = np.asarray([11,12,13,14,15])
v = np.asarray([1,2,3,4,5])
w = np.asarray([0.1,0.3,0.15,0.25,0.2])",scipy,"
assertion_value   = np.allclose(output, distance.wminkowski(u,v,3,w))
assert assertion_value","Compute the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w, and store the results in a variable named output",,"
output = distance.wminkowski(u,v,3,w)",,1.7.3,,1.0,distance.wminkowski,2021-11,name change
"
from scipy.spatial import distance
import numpy as np 
u = np.asarray([11,12,13,14,15])
v = np.asarray([1,2,3,4,5])
w = np.asarray([0.1,0.3,0.15,0.25,0.2])",scipy,"
assertion_value = np.allclose(output, distance.minkowski(u,v,3,w=w))
assert assertion_value","Compute the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w, and store the results in a variable named output",,"
output = distance.minkowski(u,v,3,w=w)",,1.9.2,,1.0,distance.minkowski,2022-10,name change
"
from scipy import linalg
import numpy as np 
A = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],
        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],
        [0.92575218, 0.66167048, 0.81779481, 0.15405207],
        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],
       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],
        [0.11275151, 0.95192602, 0.58950113, 0.78663422],
        [0.64955361, 0.47670695, 0.96824964, 0.74915994],
        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],
       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],
        [0.78721405, 0.09705598, 0.88774379, 0.81623277],
        [0.64821764, 0.62400451, 0.53916194, 0.96522881],
        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])",scipy,"
assertion_value = np.allclose(output, np.stack([linalg.expm(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Compute the matrix exponential of batched matrices, stored in a nd-array A, and store the results in a variable named output",,"
output = np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = linalg.expm(A[i])",,1.8.1,,1.0,linalg.expm,2022-05,argument or attribute change
"
from scipy import linalg
import numpy as np 
A = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],
        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],
        [0.92575218, 0.66167048, 0.81779481, 0.15405207],
        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],
       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],
        [0.11275151, 0.95192602, 0.58950113, 0.78663422],
        [0.64955361, 0.47670695, 0.96824964, 0.74915994],
        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],
       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],
        [0.78721405, 0.09705598, 0.88774379, 0.81623277],
        [0.64821764, 0.62400451, 0.53916194, 0.96522881],
        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])",scipy,"
assert np.allclose(output, linalg.expm(A))","
Compute the matrix exponential of batched matrices, stored in a nd-array A, and store the results in a variable named output",,"
output = linalg.expm(A)",,1.9.2,,1.0,linalg.expm,2022-10,argument or attribute change
"
from scipy import stats
import numpy as np 
A = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,
       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])",scipy,"
assertion_value =  np.allclose(np.asarray(output),np.asarray([-stats.combine_pvalues(1-A,'fisher')[0],(1-stats.combine_pvalues(1-A,'fisher')[1])]))
assert assertion_value","
Combine the p values from various independent tests stored in a 1D array, p_vals,
 using pearson method, make sure that higher values of the statistic now correspond to lower
  p-values and store the results in a variable named output",,"
output = stats.combine_pvalues(A,'pearson')
output = -output[0], 1-output[1]",,1.8.1,,1.0,stats.combine_pvalues,2022-05,output change
"
from scipy import stats
import numpy as np 
A = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,
       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])",scipy,"
assertion_value = np.allclose(np.asarray(output),np.asarray([-stats.combine_pvalues(1-A,'fisher')[0],(1-stats.combine_pvalues(1-A,'fisher')[1])]))
assert assertion_value","
Combine the p values from various independent tests stored in a 1D array, p_vals,
 using pearson method, make sure that higher values of the statistic now correspond to lower
  p-values, and store the results in a variable named output",,"
output = stats.combine_pvalues(A,'pearson')",,1.9.2,,1.0,stats.combine_pvalues,2022-10,output change
"
from scipy import sparse,linalg
import numpy as np 
A = sparse.lil_matrix((3, 3))
A[0, 0] = 4
A[1, 1] = 5
A[1, 2] = 6
output = ",scipy,"
assertion_value = np.allclose(output.todense(), linalg.expm(A).todense())
assert assertion_value","
Compute the matrix exponential of a sparse array A",,linalg.expm(A),,1.8.1,,1.0,linalg,2022-05,name change
"
from scipy import sparse,linalg
import numpy as np 
A = sparse.lil_matrix((3, 3))
A[0, 0] = 4
A[1, 1] = 5
A[1, 2] = 6
output = ",scipy,"
assertion_value = np.allclose(output.todense(), sparse.linalg.expm(A).todense())
assert assertion_value","
Compute the matrix exponential of a sparse array A",,sparse.linalg.expm(A),,1.9.2,,1.0,linalg,2022-10,name change
"
from scipy import stats
import numpy as np 
a = np.array([0, 2*np.pi/3, 5*np.pi/3])
",scipy,"
assertion_value = np.allclose(output,1-np.abs(np.mean(np.exp(1j*a))))
assert assertion_value","
Compute the circular variance: 1-R, where R is the mean resultant vector. Store the results in a variable named output",,"
output = 1-np.abs(np.mean(np.exp(1j*a)))",,1.8.1,,1.0,stats.circvar,2022-05,output change
"
from scipy import stats
import numpy as np 
a = np.array([0, 2*np.pi/3, 5*np.pi/3])
",scipy,"
assertion_value = np.allclose(output,1-np.abs(np.mean(np.exp(1j*a))))
assert assertion_value ","
Compute the circular variance: 1-R, where R is the mean resultant vector. Store the results in a variable named output",,"
output = stats.circvar(a)",,1.9.2,,1.0,stats.circvar,2022-10,output change
"
from scipy.stats import norm
dist = norm(15, 10)
n=5
",scipy,"
import numpy as np
assertion_value = np.allclose(output,dist.moment(order=n))
assert assertion_value","
 Store the results in a variable named output",,"
output = dist.moment(order=n)",,1.11.2,,1.0,rv_continuous.momentr,2023-08,argument or attribute change
"
from scipy.stats import norm
dist = norm(15, 10)
n=5
",scipy,"
import numpy as np
assertion_value = np.allclose(output,dist.moment(order=n))
assert assertion_value","
 Store the results in a variable named output",,"
output = dist.moment(n=n)",,1.9.2,,1.0,rv_continuous.moment,2022-10,argument or attribute change
"
from scipy.linalg import det
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
",scipy,"
assertion_value=np.allclose(output, np.stack([det(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Compute the determinant of batched matrices (batched in the first dimention), store the results in a variable named output",,"
output = np.zeros(A.shape[0])
for i in range(A.shape[0]):
    output[i] = det(A[i])",,1.9.2,,1.0,scipy.linalg.det,2022-10,argument or attribute change
"
from scipy.linalg import det
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
",scipy,"
assertion_value = np.allclose(output, det(A))
assert assertion_value","
Compute the determinant of batched matrices (batched in the first dimention), store the results in a variable named output",,"
output = det(A)",,1.11.2,,1.0,scipy.linalg.det,2023-08,argument or attribute change
"
from scipy.linalg import lu
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
p,u,v =",scipy,"
assertion_value = np.allclose(np.stack([p,u,v],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Compute the lu decomposition of batched square matrices (batched in the first dimention)",," [np.zeros(A.shape) for i in range(3)]
for i in range(A.shape[0]):
    p[i],u[i],v[i] = lu(A[i])",,1.9.2,,1.0,scipy.linalg.lu,2022-10,argument or attribute change
"
from scipy.linalg import lu
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
p,u,v =",scipy,"
assertion_value = np.allclose(np.stack([p,u,v],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Compute the lu decomposition of batched square matrices (batched in the first dimention)",, lu(A),,1.11.2,,1.0,scipy.linalg.lu,2023-08,argument or attribute change
"
import scipy.signal.windows as windows
import numpy as np
window_size=31
window = ",scipy,"
window_numpy = 2*np.arange(window_size)/(window_size-1) - 1 
window_numpy = np.sinc(window_numpy)
window_numpy = window_numpy / np.max(window_numpy)
assertion_value = np.allclose(window,window_numpy)
assert assertion_value","
Using scipy if possible, create a lanczos windows",,windows.lanczos(window_size),-,1.11.2,,1.0,signal.windows.lanczos,2023-08,other library or new feature
"
import scipy.signal.windows as windows
import numpy as np
window_size=31
window = ",scipy,"
window_numpy = 2*np.arange(window_size)/(window_size-1) - 1 
window_numpy = np.sinc(window_numpy)
window_numpy =   window_numpy / np.max(window_numpy)
assertion_value = np.allclose(window,window_numpy)
assert assertion_value","
Using scipy if possible, create a lanczos windows",,"2*np.arange(window_size)/(window_size-1) - 1 
window = np.sinc(window)
window = window / np.max(window)",-,1.9.2,,1.0,signal.windows.lanczos,2022-10,other library or new feature
"
from scipy.ndimage import gaussian_filter1d
import numpy as np
x = np.random.rand(100)
radius = 10
sigma= np.pi
output = ",scipy,"
assertion_value = np.allclose(output,gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma))
assert assertion_value","
Apply a 1D gaussian filter",,"gaussian_filter1d(x, radius=radius, sigma=sigma)",-,1.10.1,,1.0,ndimage.gaussian_filter1d,2023-02,argument or attribute change
"
from scipy.ndimage import gaussian_filter1d
import numpy as np
x = np.random.rand(100)
radius = 10
sigma= np.pi
output = ",scipy,"
assertion_value = np.allclose(output,gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma))
assert assertion_value ","
Apply a 1D gaussian filter",,"gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma)",-,1.9.2,,1.0,ndimage.gaussian_filter1d,2022-10,argument or attribute change
"
from scipy.ndimage import rank_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
rank = 6
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([rank_filter(A[i],rank,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a rank filter on batched images (batched in the first dimention)",,"rank_filter(A,rank,size=size,axes=[1,2])",,1.11.2,,1.0,scipy.ndimage.rank_filter,2023-08,argument or attribute change
"
from scipy.ndimage import rank_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
rank = 6
size = 3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([rank_filter(A[i],rank,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a rank filter on batched images (batched in the first dimention)",,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = rank_filter(A[i],rank,size=size)",,1.9.2,,1.0,scipy.ndimage.rank_filter,2022-10,argument or attribute change
"
from scipy.ndimage import percentile_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
percentile = 90
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([percentile_filter(A[i],percentile=percentile,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a percentile filter on batched images (batched in the first dimention)",,"percentile_filter(A,percentile=percentile,size=size,axes=[1,2])",,1.11.2,,1.0,scipy.ndimage.percentile_filter,2023-08,argument or attribute change
"
from scipy.ndimage import percentile_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
percentile = 90
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([percentile_filter(A[i],percentile=percentile,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a percentile filter on batched images (batched in the first dimention)",,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = percentile_filter(A[i],percentile=percentile,size=size)",,1.9.2,,1.0,scipy.ndimage.percentile_filter,2022-10,argument or attribute change
"
from scipy.ndimage import median_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([ median_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a median filter on batched images (batched in the first dimention)",,"median_filter(A,size=size,axes=[1,2])",,1.11.2,,1.0,scipy.ndimage.median_filter,2023-08,argument or attribute change
"
from scipy.ndimage import median_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([ median_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a median filter on batched images (batched in the first dimention)",,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  median_filter(A[i], size=size)",,1.9.2,,1.0,scipy.ndimage.median_filter,2022-10,argument or attribute change
"
from scipy.ndimage import uniform_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([ uniform_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a uniform filter on batched images (batched in the first dimention)",,"uniform_filter(A,size=size,axes=[1,2])",,1.11.2,,1.0,scipy.ndimage.uniform_filter,2023-08,argument or attribute change
"
from scipy.ndimage import uniform_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([uniform_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a uniform filter on batched images (batched in the first dimention)",,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  uniform_filter(A[i], size=size)",,1.9.2,,1.0,scipy.ndimage.uniform_filter,2022-10,argument or attribute change
"
from scipy.ndimage import minimum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([ minimum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a minimum filter on batched images (batched in the first dimention)",,"minimum_filter(A,size=size,axes=[1,2])",,1.11.2,,1.0,scipy.ndimage.minimum_filter,2023-08,argument or attribute change
"
from scipy.ndimage import minimum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value= np.allclose(output, np.stack([minimum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a minimum filter on batched images (batched in the first dimention)",,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  minimum_filter(A[i], size=size)",,1.9.2,,1.0,scipy.ndimage.minimum_filter,2022-10,argument or attribute change
"
from scipy.ndimage import maximum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([ maximum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a maximum filter on batched images (batched in the first dimention)",,"maximum_filter(A,size=size,axes=[1,2])",,1.11.2,,1.0,scipy.ndimage.maximum_filter,2023-08,argument or attribute change
"
from scipy.ndimage import maximum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =",scipy,"
assertion_value = np.allclose(output, np.stack([ maximum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a maximum filter on batched images (batched in the first dimention)",,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  maximum_filter(A[i], size=size)",,1.9.2,,1.0,scipy.ndimage.maximum_filter,2022-10,argument or attribute change
"
from scipy.ndimage import gaussian_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
sigma=2
output =",scipy,"
assertion_value = np.allclose(output, np.stack([ gaussian_filter(A[i],sigma=sigma) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a gaussian filter on batched images (batched in the first dimention)",,"gaussian_filter(A,sigma=sigma,axes=[1,2])",,1.11.2,,1.0,scipy.ndimage.gaussian_filter,2023-08,argument or attribute change
"
from scipy.ndimage import gaussian_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
sigma=2
output =",scipy,"
assertion_value = np.allclose(output, np.stack([gaussian_filter(A[i],sigma=sigma) for i in range(A.shape[0])],axis=0))
assert assertion_value","
Apply a gaussian filter on batched images (batched in the first dimention)",,"np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  gaussian_filter(A[i],sigma=sigma)",,1.9.2,,1.0,scipy.ndimage.gaussian_filter,2022-10,argument or attribute change
"
import flask

app = flask.Flask('test')
@app.route('/data')
def data(num_set):
    return flask.jsonify({'numbers': num_set})

def eval(app, data_fn, num_set):
    with app.test_request_context():
        response = data_fn(num_set)
        return response.get_data(as_text=False)

",flask,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_set):
    return flask.jsonify({'numbers': num_set})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})
assert assertion_result
","
Complete the app set-up for the json encoding to return a sorted list
 when called with the eval function provided with the variable 
 num_set being a set of numbers, use other libraries if needed",,"
import json
class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app.json_encoder = MyCustomJSONHandler
",werkzeug==2.0.0,2.0.0,,2.0,app.json_encoder,2021-05,argument or attribute change
"
import flask

app = flask.Flask('test')
@app.route('/data')
def data(num_set):
    return flask.jsonify({'numbers':num_set})

def eval(app, data_fn, num_set):
    with app.test_request_context():
        response = data_fn(num_set)
        return response.get_data(as_text=True)

",flask,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_set):
    return flask.jsonify({'numbers': num_set})
class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})
assert assertion_result
","
Complete the app set-up for the json encoding to return a sorted list
 when called with the eval function provided with the variable 
 num_set being a set of numbers, use other libraries if needed",,"
class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",,3.0.0,,2.0,app.json_encoder,2023-09,argument or attribute change
"
from flask import Flask, send_file
from io import BytesIO

app1 = Flask(__name__)

def get_content_disp(app, download_fn):
    with app.test_request_context():
        response = download_fn()
    content_disp = response.headers.get('Content-Disposition')
    return content_disp

@app1.route('/download')
def download():
    data = BytesIO(b'Hello, World!')
    attachment_filename = 'hello.txt'
    return send_file(data, as_attachment=True,
",flask,"
content_disp = get_content_disp(app1, download)
assertion_result = 'filename=hello.txt' in content_disp
assert assertion_result
","
Complete the download definition to download the data in the variable attachment_filename",,"attachment_filename=attachment_filename)
",werkzeug==2.0.0,2.0.0,,1.0,flask.send_file,2021-05,argument or attribute change
"
from flask import Flask, send_file
from io import BytesIO

app1 = Flask(__name__)

def get_content_disp(app, download_fn):
    with app.test_request_context():
        response = download_fn()
    content_disp = response.headers.get('Content-Disposition')
    return content_disp

@app1.route('/download')
def download():
    data = BytesIO(b'Hello, World!')
    attachment_filename = 'hello.txt'
    return send_file(data, as_attachment=True,

",flask,"
content_disp = get_content_disp(app1, download)
assertion_result = 'filename=hello.txt' in content_disp
assert assertion_result
","
Complete the download definition to download the data in the variable attachment_filename",,"download_name=attachment_filename)
",,3.0.0,,1.0,flask.send_file,2023-09,argument or attribute change
"
import json
import tempfile
from flask import Flask

config_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}
with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:
    json.dump(config_data, tmp)
    tmp.flush()
    config_file = tmp.name

app = Flask(__name__)
app.config.",flask,"
assertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'
assert assertion_result","
Load the json file to config the Flask app",,"from_json(config_file)
",werkzeug==2.0.0,2.0.1,,1.0,Flask.config.from_json,2021-05,name change
"
import json
import tempfile
from flask import Flask

config_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}
with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:
    json.dump(config_data, tmp)
    tmp.flush()
    config_file = tmp.name

app = Flask(__name__)
app.config.",flask,"
assertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'
assert assertion_result
","
Load the json file to config the Flask app",,"from_file(config_file, load=json.load)
",,3.0.0,,1.0,Flask.config.from_file,2023-09,name change
"
import flask
import werkzeug

error404 = werkzeug.exceptions.NotFound

def safe_join_fail_404(base_path,sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
",flask,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'

try : 
    joined = safe_join_fail_404(base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined = safe_join_fail_404(base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt'
assert assertion_result","
Complete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed",,"
    joined = flask.safe_join(base_path, sub_path)

    return joined
",werkzeug==2.0.0,2.0.1,,1.0,flask.safe_join,2021-05,name change
"
import flask
import werkzeug

error404 = werkzeug.exceptions.NotFound

def safe_join_fail_404(base_path,sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
",flask,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'

try : 
    joined = safe_join_fail_404(base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined = safe_join_fail_404(base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt'
assert assertion_result","
Complete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed",,"
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    return joined
",,3.0.0,,1.0,flask.safe_join,2023-09,name change
"
import flask

def convert_timedelta_to_seconds(td):",flask,"
import datetime
td = datetime.timedelta(hours=2, minutes=30,microseconds=1)
assertion_results = convert_timedelta_to_seconds(td)==9000
assert assertion_results","
Complete the function convert_timedelta_to_seconds, use other libraries if needed",,"
    return flask.helpers.total_seconds(td)
",werkzeug==2.0.0,2.0.1,,1.0,helpers.total_seconds,2021-05,name change
"
import flask

def convert_timedelta_to_seconds(td):",flask,"
import datetime
td = datetime.timedelta(hours=2, minutes=30,microseconds=1)
assertion_results = convert_timedelta_to_seconds(td)==9000.000001
assert assertion_results","
Complete the function convert_timedelta_to_seconds, use other libraries if needed",,"
    return td.total_seconds()
",,3.0.0,,1.0,helpers.total_seconds,2023-09,name change
"
import jinja2 
def setup_environment(filtername,filter):
    env = jinja2.Environment()
    env.filters[filtername] = filter
    return env

@",jinja2,"
env = setup_environment('greet',greet)
template = env.from_string('''
{{ 'World'| greet }}''')
assertion_results = 'Hi, World!' in template.render(prefix='Hi')
assert assertion_results 
assertion_results = 'Hello, World!' in template.render()
assert assertion_results 
","
Create a custom filter named greet. This filter should: 
accept the context and a parameter name, 
retrieve a variable prefix from the context (defaulting to 'Hello' if not provided), 
return a greeting message combining the prefix and the name.",,"jinja2.contextfilter
def greet(ctx, name):
    prefix = ctx.get('prefix', 'Hello')
    return f'{prefix}, {name}!'

    
",markupsafe==2.0.1,2.11,,1.0,jinja2.contextfilter,2020-01,name change
"
import jinja2 
def setup_environment(filtername,filter):
    env = jinja2.Environment()
    env.filters[filtername] = filter
    return env

@",jinja2,"
env = setup_environment('greet',greet)
template = env.from_string('''
{{ 'World'| greet }}''')
assertion_results = 'Hi, World!' in template.render(prefix='Hi')
assert assertion_results 
assertion_results = 'Hello, World!' in template.render()
assert assertion_results 
","
Create a custom filter named greet. This filter should: 
accept the context and a parameter name, 
retrieve a variable prefix from the context (defaulting to 'Hello' if not provided), 
return a greeting message combining the prefix and the name.",,"jinja2.pass_context
def greet(ctx, name):
    prefix = ctx.get('prefix', 'Hello')
    return f'{prefix}, {name}!'
    
",,3.1,,1.0,jinja2.pass_context,2022-03,name change
"
import re
from jinja2 import Environment, evalcontextfilter
from markupsafe import Markup, escape

def get_output(env, filter_fn):
    env.filters['nl2br'] = filter_fn
    template = env.from_string('{{ text | nl2br }}')
    output = template.render(text='Hello World')
    return output

def nl2br_core(eval_ctx, value):
    br = '<br>Hello</br>'
    if eval_ctx.autoescape:
        value = escape(value)
        br = Markup(br)
    result = re.sub(r'Hello', br, value)
    return Markup(result) if eval_ctx.autoescape else result

@",jinja2,"
env = Environment(autoescape=True)
output = get_output(env,nl2br)
expected = '<br>Hello</br> World'

assert output == expected, f'Expected: {expected!r}, but got: {output!r}'
","
Write the nl2br function which is a custom filter that takes the evaluation context 
and a text value, then replaces every occurrence of the substring 'Hello' 
with the HTML string '<br>Hello</br>', while respecting the autoescape setting.
You can use the nl2br_core function",,"evalcontextfilter
def nl2br(eval_ctx, value):
    return nl2br_core(eval_ctx, value)
",markupsafe==2.0.1,2.11,,1.0,jinja2.evalcontextfilter,2020-01,name change
"
import re
from jinja2 import Environment, pass_eval_context
from markupsafe import Markup, escape

def get_output(env, filter_fn):
    env.filters['nl2br'] = filter_fn
    template = env.from_string('{{ text | nl2br }}')
    output = template.render(text='Hello World')
    return output

def nl2br_core(eval_ctx, value):
    br = '<br>Hello</br>'
    if eval_ctx.autoescape:
        value = escape(value)
        br = Markup(br)
    result = re.sub(r'Hello', br, value)
    return Markup(result) if eval_ctx.autoescape else result

@",jinja2,"
env = Environment(autoescape=True)
output = get_output(env,nl2br)
expected = '<br>Hello</br> World'

assert output == expected, f'Expected: {expected!r}, but got: {output!r}'
","
Write the nl2br function which is a custom filter that takes the evaluation context 
and a text value, then replaces every occurrence of the substring 'Hello' 
with the HTML string '<br>Hello</br>', while respecting the autoescape setting.
You can use the nl2br_core function",,"pass_eval_context
def nl2br(eval_ctx, value):
    return nl2br_core(eval_ctx, value)
",,3.1,,1.0,jinja2.pass_eval_context,2022-03,name change
"
import warnings
from scipy.linalg import det
import numpy as np
warnings.filterwarnings('error')

def check_invertibility(matrices):",scipy,"
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]]
])
assertion_value = check_invertibility(matrices)
assert assertion_value
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]],

    [[0, 0],
     [0, 0]]
])
assertion_value = not check_invertibility(matrices)
assert assertion_value","
complete the following function that check if all the batch of matrices are invertible, using numpy 1.25.1",,"
    return np.all(det(matrices))
",numpy==1.25.1,1.11.1,,2.0,linalg.det,2023-06,argument or attribute change
"
import warnings
from scipy.linalg import det
import numpy as np
warnings.filterwarnings('error')

def check_invertibility(matrices):",scipy,"
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]]
])
assertion_value = check_invertibility(matrices)
assert assertion_value
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]],

    [[0, 0],
     [0, 0]]
])
assertion_value = not check_invertibility(matrices)
assert assertion_value","
complete the following function that check if all the batch of matrices are invertible, using numpy 1.21.6",,"
    return np.alltrue([det(A) for A in matrices])
",numpy==1.21.6,1.9.1,,2.0,linalg.det,2022-10,argument or attribute change
"
import numpy as np
from scipy.stats import hmean

def count_unique_hmean(data):
    # data shape: (n, m)
    # n: number of arrays
    # m: number of elements in each array ",scipy,"
data = np.array([
    [1, 2, 3],
    [2, 2, 2],
    [1, np.nan, 3],
    [4, 5, 6],
    [np.nan, 1, np.nan],
    [1, 2, 3]
])
assertion_value = count_unique_hmean(data) == 5
assert assertion_value
","
Complete the function count_unique_hmean that takes a 2D numpy array as 
input and returns the number of unique harmonic mean values across 
the rows of the array, counting each nan value as unique. We are using numpy 1.25.1",,"
    hmean_values = hmean(np.asarray(data), axis=1)
    unique_vals = np.unique(hmean_values, equal_nan=False).shape[0]
    return unique_vals

",numpy==1.25.1,1.11.1,,2.0,stats.hmean,2023-06,output change
"

import numpy as np
from scipy.stats import hmean

def count_unique_hmean(data):
    # data shape: (n, m)
    # n: number of arrays
    # m: number of elements in each array",scipy,"

data = np.array([
    [1, 2, 3],
    [2, 2, 2],
    [1, np.nan, 3],
    [4, 5, 6],
    [np.nan, 1, np.nan],
    [1, 2, 3]
])
assertion_value = count_unique_hmean(data) == 5
assert assertion_value
","
Complete the function count_unique_hmean that takes a 2D numpy array as 
input and returns the number of unique harmonic mean values across 
the rows of the array, counting each nan value as unique. We are using numpy 1.21.6",,"
    hmean_values = []
    for arr in data:
        if np.isnan(arr).any():
            hm = np.nan
        else:
            hm = hmean(arr)
        hmean_values.append(hm)
    
    hmean_values = np.asarray(hmean_values)
    non_nan_vals = hmean_values[~np.isnan(hmean_values)]
    counts_non_nan = np.unique(non_nan_vals).shape[0]
    nan_count = np.sum(np.isnan(hmean_values))
    return counts_non_nan + nan_count
",numpy==1.21.6,1.8.1,,2.0,stats.hmean,2022-05,output change
"
import numpy as np
from scipy.signal import hilbert

def compute_hilbert_transform(a, b, dtype=np.float64):
    # compute_hilbert_transform should return the Hilbert transform of the
    # a and b arrays stacked vertically, with safe casting and the specified
    # dtype. 
    # raise TypeError if needed
    ",scipy,"

a = np.array([1.0, 2.0, 3.0], dtype=np.float32)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
assertion_value = False
try :
    compute_hilbert_transform(a, b, dtype=np.float32)
except TypeError:
    assertion_value = True
assert assertion_value
b=b.astype(np.float32)
computed = compute_hilbert_transform(a, b, dtype=np.float32)
expected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)
assert assertion_value
a=a.astype(np.float64)
b=b.astype(np.float64)
computed = compute_hilbert_transform(a, b, dtype=np.float64)
expected = expected.astype(dtype=np.complex128)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)
assert assertion_value
","
Complete the function compute_hilbert_transform. We are using numpy 1.25.1",,"
    stacked = np.vstack((a, b), dtype=dtype,
                         casting='safe')
    return hilbert(stacked)
",numpy==1.25.1,1.11.1,,2.0,signal.hilbert,2023-06,argument or attribute change
"
import numpy as np
from scipy.signal import hilbert

def compute_hilbert_transform(a, b, dtype=np.float64):
    # compute_hilbert_transform should return the Hilbert transform of the
    # a and b arrays stacked vertically, with safe casting and the specified
    # dtype.
    # raise TypeError if needed
    ",scipy,"
a = np.array([1.0, 2.0, 3.0], dtype=np.float32)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
assertion_value = False
try :
    compute_hilbert_transform(a, b, dtype=np.float32)
except TypeError:
    assertion_value = True
assert assertion_value
b=b.astype(np.float32)
computed = compute_hilbert_transform(a, b, dtype=np.float32)
expected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)
assert assertion_value

a=a.astype(np.float64)
b=b.astype(np.float64)
computed = compute_hilbert_transform(a, b, dtype=np.float64)
expected = expected.astype(dtype=np.complex128)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)
assert assertion_value
","
Complete the function compute_hilbert_transform. We are using numpy 1.25.1",,"
    if not (np.can_cast(a.dtype, dtype, casting='safe') and np.can_cast(b.dtype, dtype, casting='safe')):
        raise TypeError('Unsafe casting from input dtype to specified dtype')
    
    a_cast = a.astype(dtype, copy=False)
    b_cast = b.astype(dtype, copy=False)
    
    stacked = np.vstack((a_cast, b_cast))
    
    result = hilbert(stacked)
    
    if dtype == np.float32:
        complex_dtype = np.complex64
    elif dtype == np.float64:
        complex_dtype = np.complex128
    else:
        complex_dtype = np.complex128

    return result.astype(complex_dtype)
    ",numpy==1.21.6,1.8.1,,2.0,signal.hilbert,2022-05,argument or attribute change
"
import flask
import json
import numpy as np
app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):",flask,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))
assert assertion_results
"," 
Complete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained 
in the numpy array when called, we are using numpy 1.21.6",,"
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
",numpy==1.21.6 werkzeug==2.0.0,2.0.0,,2.0,app.json_encoder,2021-05,argument or attribute change
"
import flask
import numpy as np

app = flask.Flask('test1')

@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
    ",flask,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_results = eval_app(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval_app(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))
assert assertion_results
"," 
Complete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained 
in the numpy array when called, we are using numpy 1.25.1",,"
        if isinstance(obj, np.ndarray):
            unique_vals = np.unique(obj, equal_nan=False)
            return unique_vals.tolist()
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",numpy==1.25.1,3.0.0,,2.0,app.json_encoder,2023-09,argument or attribute change
"
import flask
import json
import numpy as np
from numpy import fastCopyAndTranspose 
app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):",flask,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))
assert assertion_results
"," 
Complete the app set-up for the json encoding to perform a fast copy and 
transpose when given a numpy array before flattening and converting 
the result to a list,we are using numpy 1.21.6",,"
            res = fastCopyAndTranspose(obj).flatten().tolist()
            return res
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler

",numpy==1.21.6 werkzeug==2.0.0,2.0.0,,2.0,app.json_encoder,2021-05,argument or attribute change
"
import flask
import numpy as np
import warnings
from numpy import fastCopyAndTranspose 
warnings.filterwarnings('error')

app = flask.Flask('test1')

@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):",flask,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_results = eval_app(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval_app(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))
assert assertion_results
"," 
Complete the app set-up for the json encoding to perform a fast copy and 
transpose when given a numpy array before flattening and converting 
the result to a list, we are using numpy 1.25.1",,"
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",numpy==1.25.1,3.0.0,,2.0,app.json_encoder,2023-09,argument or attribute change
"
import flask
import werkzeug
import numpy as np

error404 = werkzeug.exceptions.NotFound

def stack_and_save(arr_list,base_path,sub_path, casting_policy,out_dtype):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # stack the arrays in arr_list with the casting policy and the out_dtype.
    # if the out_dtype is not compatible with the casting policy, raise a TypeError
    # and out_dtype could be np.float32 or np.float64
    # casting policy could be safe or unsafe
    # Return the joined path and the stacked array to be saved ",flask,"

base_path = '/var/www/myapp'
sub_path = '../secret.txt'
a = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
b = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)
arr_list = [a, b]
casting_policy = 'safe'
out_dtype=np.float64
stacked_correct = np.vstack(arr_list).astype(np.float64)
try : 
    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
try : 
    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except TypeError as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
casting_policy = 'unsafe'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)

assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32
assert assertion_result
"," 
Complete the stack_and_save function, we are using numpy 1.21.6",,"
    joined = flask.safe_join(base_path, sub_path)
    casted_list = []

    for arr in arr_list:
        if not np.can_cast(arr.dtype, out_dtype, casting=casting_policy):
            raise TypeError('Cannot cast array')
        casted_list.append(arr.astype(out_dtype, copy=False))
    
    stacked = np.vstack(casted_list)
    return joined, stacked
",numpy==1.21.6 werkzeug==2.0.0,2.0.0,,2.0,flask.safe_join,2021-05,name change
"
import flask
import werkzeug
import numpy as np

error404 = werkzeug.exceptions.NotFound

def stack_and_save(arr_list,base_path,sub_path, casting_policy,out_dtype):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # stack the arrays in arr_list with the casting policy and the out_dtype.
    # if the out_dtype is not compatible with the casting policy, raise a TypeError
    # and out_dtype could be np.float32 or np.float64
    # casting policy could be safe or unsafe
    # Return the joined path and the stacked array to be saved ",flask,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'


a = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
b = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)
arr_list = [a, b]
casting_policy = 'safe'
out_dtype=np.float64
stacked_correct = np.vstack(arr_list).astype(np.float64)
try : 
    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
try : 
    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except TypeError as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
casting_policy = 'unsafe'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)

assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32
assert assertion_result
"," 
Complete the stack_and_save function, we are using numpy 1.25.1",,"
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    stacked = np.vstack(arr_list,casting=casting_policy,dtype=out_dtype)
    return joined, stacked
",numpy==1.25.1,3.0.0,,2.0,flask.safe_join,2023-09,name change
"
import flask
import numpy as np
from scipy import linalg

app = flask.Flask('test1')
@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})
def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] : ",flask,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)
a = np.random.random((6,3,3))

assertion_results = eval_app(app2, data2,a) == eval_app(app, data,a)
assert assertion_results"," 
Complete the app set-up so that, when given a batch of matrix,
the json encoding compute the determinants of each matrix, 
before flattening and converting the result to a list, we are using scipy 1.11.1",,"

            res = linalg.det(obj)
            return res.tolist()
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app) ",scipy==1.11.1,3.0.0,,2.0,app.json_encoder,2023-09,argument or attribute change
" 
import flask
import json
import numpy as np
from scipy import linalg

app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :",flask,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
a = np.random.random((6,3,3))

assertion_results = eval(app2, data2,a) == eval(app, data,a)
assert assertion_results
"," 
Complete the app set-up so that, when given a batch of matrix,
the json encoding compute the determinants of each matrix, 
before flattening and converting the result to a list, we are using scipy 1.8.1 ",,"
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
",scipy==1.8.1 Werkzeug==2.0.0,2.0.0,,2.0,app.json_encoder,2021-05,argument or attribute change
"
import flask
import numpy as np
from scipy.stats import hmean

app = flask.Flask('test1')

@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):",flask,"
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = hmean(obj,axis=1).tolist()
            return res
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)
assertion_results = eval_app(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval_app(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))
assert assertion_results

"," 
Complete the app set-up so that, when given a batch (first dim) of values,
compute the harmonic mean along the second dimension (It should handle nan values),
before flattening and converting the result to a list, we are using scipy 1.11.1",,"
            res = hmean(obj,axis=1).tolist()
            return res
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
",scipy==1.11.1,3.0.0,,2.0,app.json_encoder,2023-09,argument or attribute change
"
import flask
import json
import numpy as np
from scipy.stats import hmean

app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):",flask,"
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = np.zeros((obj.shape[0],1))
            for i_arr in range(obj.shape[0]):
                if np.isnan(obj[i_arr]).any():
                    res[i_arr] = np.nan
                else:
                    res[i_arr]  = hmean(obj[i_arr])
            res = res.flatten().tolist()
            return res
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))
assert assertion_results
"," 
Complete the app set-up so that, when given a batch (first dim) of values,
compute the harmonic mean along the second dimension (It should handle nan values),
before flattening and converting the result to a list, we are using scipy 1.8.1",,"
            res = np.zeros((obj.shape[0],1))
            for i_arr in range(obj.shape[0]):
                if np.isnan(obj[i_arr]).any():
                    res[i_arr] = np.nan
                else:
                    res[i_arr]  = hmean(obj[i_arr])
            res = res.flatten().tolist()
            return res
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
",scipy==1.8.1 Werkzeug==2.0.0,2.0.0,,2.0,app.json_encoder,2021-05,argument or attribute change
"
import flask
import werkzeug
from scipy import linalg

error404 = werkzeug.exceptions.NotFound

def save_exponential(A, base_path, sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # compute the exponential of the batched matrices (m, m) in A (n,m,m)
    # return the save_path and the exponential of the matrices
    ",flask,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'
import numpy as np

a = np.random.random((4,3,3))
expected = np.zeros(a.shape)
for i in range(expected.shape[0]):
    expected[i] = linalg.expm(a[i])

try : 
    joined, results = save_exponential(a,base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'

joined, results = save_exponential(a,base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)
assert assertion_result
"," 
Complete the save_exponential function, we are using scipy 1.11.1",,"
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    output = linalg.expm(A)
    return joined, output
",scipy==1.11.1,3.0.0,,2.0,flask.safe_join,2023-09,name change
" 
import flask
import werkzeug
from scipy import linalg

error404 = werkzeug.exceptions.NotFound

def save_exponential(A, base_path, sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # compute the exponential of the batched matrices (m, m) in A (n,m,m)
    # return the save_path and the exponential of the matrices
    ",flask,"
base_path = '/var/www/myapp'
sub_path = '../secret.txt'
import numpy as np

a = np.random.random((4,3,3))
expected = np.zeros(a.shape)
for i in range(expected.shape[0]):
    expected[i] = linalg.expm(a[i])

try : 
    joined, results = save_exponential(a,base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'

joined, results = save_exponential(a,base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)
assert assertion_result
"," 
Complete the save_exponential function, we are using scipy 1.8.1",,"
    joined = flask.safe_join(base_path, sub_path)
    output = np.zeros(A.shape)
    for i in range(A.shape[0]):
        output[i] = linalg.expm(A[i])
    return joined, output
",scipy==1.8.1 Werkzeug==2.0.0,2.0.0,,2.0,flask.safe_join,2021-05,name change
"    
from sympy.stats import Die, sample
X = Die('X', 6)
def custom_generateRandomSampleDice(X):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

def test_custom_generateRandomSampleDice():
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(""always"", SymPyDeprecationWarning)  # Capture all warnings
        output = custom_generateRandomSampleDice(X)
        expect = [sample(X) for i in range(3)]
        assert isinstance(output, list), ""Test Failed: Output is not a list!""
        assert len(output) == len(expect), ""Test Failed: Output length does not match expected!""
        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""

test_custom_generateRandomSampleDice()","Generate random samples from a six-sided die, return a list.",,[sample(X) for i in range(3)],scipy,1.9,,,stats.sample,2021-06 ,new func/method/class
"
from sympy.matrices.expressions.fourier import DFT

def custom_computeDFT(n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

def test_custom_computeDFT():
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(""always"", SymPyDeprecationWarning)  # Capture all warnings
        output = custom_computeDFT(4)
        expect = DFT(4).as_explicit()
        assert output == expect
        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""

test_custom_computeDFT()",Compute the Discrete Fourier Transform (DFT) matrix of size  n \times n and show it explicitly.,,DFT(n).as_explicit(),,1.9,,,sympy.physics.matrices.mdft,2021-06 ,new func/method/class
"from sympy import laplace_transform, symbols, eye
t, z = symbols('t z')
output = ",sympy,"from sympy import Matrix
expected = (Matrix([
    [1/z,   0],
    [  0, 1/z]
]), 0, True)
assert output == expected
","Compute the Laplace transform of the 2 \times 2 identity matrix I_2 (i.e., eye(2)) with respect to t and return a single tuple. The first element of the tuple should be the transformed matrix, and the second element should be the combined convergence condition. Store the results in a variable named output.
",,"laplace_transform(eye(2), t, z, legacy_matrix=False)",,1.9,,,laplace_transform,2021-06 ,new func/method/class
"import sympy as S
import sympy.physics.quantum
def custom_trace(n):
    return ",sympy,"from sympy.physics.quantum.trace import Tr
expect = Tr(2)
assert custom_trace(2) == expect",Instantiate the trace object with the number n return the resulting trace object.,,S.physics.quantum.trace.Tr(n),,1.11,,,trace,2022-03 ,breaking change
"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_preorder_traversal(expr):
    return ",sympy,"expect = [7]
assert list(custom_preorder_traversal(expr)) == expect",Instantiate the preorder_traversal object with existing expression.,,sympy.preorder_traversal(expr),,1.11,,,preorder_traversal,2022-03 ,breaking change
"from sympy.parsing.mathematica import parse_mathematica
from sympy import Function, Max, Min

expr = ""F[6,4,4]""
def custom_parse_mathematica(expr):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 24
    assert custom_parse_mathematica(expr) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Let F to be a function that returns the maximum value multiplied by the minimum value.,,"parse_mathematica(expr).replace(Function(""F""), lambda *x: Max(*x)*Min(*x))",,1.11,,,parse_mathematica,2022-08 ,new func/method/class
"from sympy.physics.mechanics import Body, PinJoint
parent, child = Body('parent'), Body('child')
pin = ",sympy,"expect1 = parent.frame.x
expect2 = -child.frame.x

assert pin.parent_point.pos_from(parent.masscenter) == expect1
assert pin.child_point.pos_from(child.masscenter) == expect2","Instantiate a PinJoint in the parent to be positioned at parent.frame.x with respect to the mass center, and in the child at -child.frame.x. Store the results in a variable named pin.",,"PinJoint('pin', parent, child, parent_point=parent.frame.x,child_point=-child.frame.x)",,1.12,,,sympy.physics.mechanics.PinJoint,2023-05 ,argument change	
"import sympy as sp
from sympy.physics.mechanics import Body, PinJoint

parent, child = Body('parent'), Body('child')
pin = ",sympy,"
assert isinstance(pin.coordinates, sp.Matrix)
assert isinstance(pin.speeds, sp.Matrix)",Instantiate a PinJoint that connects a parent body to a child body. Store the results in a variable named pin.,,"PinJoint('pin', parent, child)",,1.12,,,sympy.physics.mechanics,2023-05 ,output behaviour
"from sympy import *

output = ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy import is_carmichael
    expect = is_carmichael(561)
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","Check number 561 is carmichael or not, return bool value.  Store the results in a variable named output.",,is_carmichael(561),,1.13,,,"sympy.functions.combinatorial.numbers.carmichael.is_carmichael
",2023-07 ,breaking change
"from sympy import *

output = ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 12
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Compute the sum of the divisors of the integer 6. Store the results in a variable named output.,,"divisor_sigma(6, 1)",,1.13,,,sympy.ntheory.factor_.divisor_sigma,2023-07 ,breaking change
"from sympy import GF
K = GF(6)
a = K(8)
output = ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 2
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","Consider the field of integers modulo 6, create an element corresponding to the number 8, and then convert this modular element into its standard integer representation. Store the results in a variable named output.",,K.to_int(a),,1.13,,,ModularInteger.to_int(),2023-07 ,new func/method/class
"from sympy import symbols
from sympy.physics.mechanics import ReferenceFrame
N = ReferenceFrame('N')
Ixx, Iyy, Izz = symbols('Ixx Iyy Izz')

def custom_generateInertia(N, Ixx, Iyy, Izz):
    from sympy.",sympy,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy.physics.mechanics import inertia
    expect = inertia(N, Ixx, Iyy, Izz)
    assert custom_generateInertia(N, Ixx, Iyy, Izz) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","Create a custom function that accepts an input and returns a symbolic representation of a rigid body’s inertia tensor relative to a specified reference frame, constructed using the symbolic variables for its principal moments of inertia. Based on the provided code, give the complete function and import the necessary libraries.",,"physics.mechanics import inertia
    return inertia(N, Ixx, Iyy, Izz)",,1.13,,,sympy.physics.mechanics,2023-07 ,new func/method/class
"from sympy import *

x, y = symbols('x y')
eq = Eq(x, y)

output = ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = eq.lhs - eq.rhs
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","Use two defined given symbols and an equality between them, then compute the difference between two symbols.",,eq.lhs - eq.rhs,,1.13,,,Eq.rewrite(Add),2023-07 ,new func/method/class
"from sympy import symbols, Poly
x = symbols('x')
p = Poly(x**2 + 2*x + 3)

def custom_generatePolyList(poly):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = [1,2,3]
    assert custom_generatePolyList(p) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Create a custom function that accepts a polynomial object and returns a list of its coefficients based on its internal representation.,,p.rep.to_list(),,1.13,,,DMP.rep attribute,2023-07 ,new func/method/class
"from sympy import symbols
from sympy.physics.mechanics import (
  Particle, PinJoint, PrismaticJoint, RigidBody)
l = symbols(""l"")
wall = RigidBody(""wall"")
cart = RigidBody(""cart"")
pendulum = RigidBody(""Pendulum"")
slider = PrismaticJoint(""s"", wall, cart, joint_axis=wall.x)
pin = PinJoint(""j"", cart, pendulum, joint_axis=cart.z,
               child_point=l * pendulum.y)

def custom_motion(wall,slider, pin):
    from sympy.physics.mechanics import ",sympy,"from sympy import symbols, Function, Derivative, Matrix, sin, cos
t = symbols('t')
l, Pendulum_mass, cart_mass, Pendulum_izz = symbols('l Pendulum_mass cart_mass Pendulum_izz')

q_j = Function('q_j')
u_j = Function('u_j')
u_s = Function('u_s')
M = Matrix([
    [Pendulum_mass*l*u_j(t)**2*sin(q_j(t)) - Pendulum_mass*l*cos(q_j(t))*Derivative(u_j(t), t)
     - (Pendulum_mass + cart_mass)*Derivative(u_s(t), t)],
    [-Pendulum_mass*l*cos(q_j(t))*Derivative(u_s(t), t)
     - (Pendulum_izz + Pendulum_mass*l**2)*Derivative(u_j(t), t)]
])
assert custom_motion(wall,slider, pin) == M","Using Sympy’s mechanics module, create a mechanical system involving three rigid bodies—a wall, a cart, and a pendulum. The wall serves as the inertial (fixed) reference, while the cart is connected to the wall by a sliding joint along the wall’s x-axis. The pendulum is attached to the cart via a rotational joint about the cart’s z-axis, with its connection point offset by a symbolic distance along the pendulum’s y-axis. Write a custom function that sets up this system using the Newtonian formulation (with the wall as the inertial frame), adds the slider and pin joints, and returns the system’s equations of motion. What are the equations of motion generated by your function?",,"System
    system = System.from_newtonian(wall)
    system.add_joints(slider, pin)
    return system.form_eoms()
",,1.13,,,sympy.physics.mechanics.JointsMethod,2023-07 ,new func/method/class
"from sympy.physics.mechanics import *
rigid_body_text = ""rigid_body""
particle_text = ""particle""

def custom_body(rigid_body_text, particle_text):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    exp1, exp2 = custom_body(rigid_body_text, particle_text)
    assert exp1.name == rigid_body_text
    assert exp2.name == particle_text
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","Using the SymPy's mechanics module, define a function that takes two strings as inputs—one representing the name of a rigid body and the other representing the name of a particle—and returns a tuple containing a rigid body and a particle created with those names. 
",,"RigidBody(rigid_body_text), Particle(particle_text)",,1.13,,,sympy.physics.mechanics.Body,2023-07 ,new func/method/class
"from sympy import Indexed

a = Indexed(""A"", 0)
output = ",sympy,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = a.free_symbols
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","Get the given set of free symbols, store the results in output variable.",,a.free_symbols,,1.9,,,expr_free_symbols,2021-06 ,new func/method/class
"from sympy import Matrix

first = [1,2]
second =[3,4]
def custom_create_matrix(first,second):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expected_shape = (2, 2)
    expected_content = [[1, 2], [3, 4]]
    output = custom_create_matrix(first, second)

    assert output.shape == expected_shape

    assert output.tolist() == expected_content
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function named custom_create_matrix that takes two lists (first and second) and returns a matrix composed of these two lists as rows.,,"Matrix([first, second])",,1.9,,,sympy.polys.solvers.RawMatrix,2021-06 ,new func/method/class
"from sympy import Matrix

m = Matrix([[1, 2], [3, 4]])
    
def custom_function(matrix):
    return ",sympy,"

output = custom_function(m)
output[0] = 100

assert m[0, 0] == 1
assert output[0] == 100

","Write a custom function that accepts a Sympy Matrix and returns a flat, read-only copy of its data ",,matrix.flat(),,1.9,,,DenseMatrix._flat,2021-06 ,new func/method/class
"from sympy import Matrix

m = Matrix([[1, 2], [3, 4]])
    
def custom_function(matrix):
    return ",sympy,"output = custom_function(m)
output[(0, 0)] = 100

assert m[0, 0] == 1
assert output[(0, 0)] == 100


",Write a custom function that accepts a Sympy SparseMatrix and returns its dictionary-of-keys representation.,,matrix.todok(),,1.9,,,SparseMatrix._todok,2021-06 ,new func/method/class
"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_bottom_up(expr):
    return ",sympy,"expect = 7

assert custom_bottom_up(expr) == expect",Write a custom function named custom_bottom_up that applies a bottom-up traversal to expr with a lambda function on each node.,,"sympy.bottom_up(expr, lambda x: x.doit())",,1.1,,,sympy.bottom_up,2022-03 ,breaking change
"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_use(expr):
    return ",sympy,"
expect = 7

assert custom_use(expr) == expect","Write a custom function that traverses the expression so that every subexpression is evaluated, and returns final evaluated result. ",," sympy.use(expr, lambda x: x.doit())",,1.1,,,sympy.use,2022-03 ,breaking change
"import sympy

def custom_is_perfect_square(n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_is_perfect_square(4)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that check if the input is a perfect square.,,sympy.ntheory.primetest.is_square(n),,1.11,,,carmichael.is_perfect_square,2023-07 ,new func/method/class
"import sympy

def custom_is_prime(n):

    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_is_prime(13)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that check if the input is prime,,sympy.isprime(n),,1.11,,,carmichael.is_prime,2023-07 ,new func/method/class
"import sympy

def custom_divides(n, p):

    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_divides(10,2)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""","Write a custom function that checks whether the integer p divides the integer n evenly (i.e., with no remainder).",,n % p == 0,,1.11,,,carmichael.divides,2023-07 ,new func/method/class
"from sympy import Matrix, symbols, Array

a1, a2, a3, a4 = symbols('a1 a2 a3 a4')
array_expr = Array([[a1, a2], [a3, a4]])

def custom_array_to_matrix(array):
    from sympy.tensor.array.expressions.",sympy,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy.tensor.array.expressions.from_array_to_matrix import convert_array_to_matrix

    expect = convert_array_to_matrix(array_expr)
    output = custom_array_to_matrix(array_expr)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that convert input array into matrix by import correct sympy modules.,,"from_array_to_matrix import convert_array_to_matrix
    return convert_array_to_matrix(array)
",,1.12,,,sympy.tensor.array.expressions.conv_*,2023-05 ,breaking change
"
import sympy

def custom_jacobi_symbols(a, n):
    return ",sympy,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_jacobi_symbols(1001, 9907)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that compute the Jacobi symbol (a/n).,,"sympy.jacobi_symbol(a, n)",,1.13,,,"sympy.ntheory.residue_ntheory.jacobi_symbol
",2023-07 ,breaking change
"
import sympy

def custom_npartitions(n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 7
    output = custom_npartitions(5)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that compute the number of partitions of n.,,sympy.functions.combinatorial.numbers.partition(n),,1.13,,,sympy.partitions_.npartitions,2023-07 ,breaking change
"
import sympy

def custom_primefactors(n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 3
    output = custom_primefactors(18)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that compute the number of distinct prime factors of n.,,sympy.primeomega(n),,1.13,,,"sympy.ntheory.factor_.primeomega
",2023-07 ,breaking change
"
import sympy

def custom_prime_counting(n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 10
    output = custom_prime_counting(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that compute the prime counting function for n.,,"sympy.primepi(n)
",,1.13,,,sympy.ntheory.generate.primepi,2023-07 ,breaking change
"
import sympy

def custom_totient(n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 8
    output = custom_totient(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that compute Euler's totient function for n (number of integers relatively prime to n).,,sympy.totient(n),,1.13,,,sympy.ntheory.factor_.totient,2023-07 ,breaking change
"import sympy

def custom_mobius(n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_mobius(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that compute the Möbius function for n.,,sympy.mobius(n),,1.13,,,sympy.ntheory.residue_ntheory.mobius,2023-07 ,breaking change
"
import sympy

def custom_legendre(a, n):
    return ",sympy,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_legendre(200, 13)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",Write a custom function that compute the Legendre symbol (a/p).,,"sympy.legendre_symbol(a, n)",,1.13,,,"sympy.ntheory.residue_ntheory.legendre_symbol
",2023-07 ,breaking change
"import seaborn as sns
import pandas as pd


data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})

def custom_pointplot(data):
    return ",seaborn,"

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_pointplot(data)
    
    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]

    if any(""dataframegroupby.apply"" in msg for msg in warning_messages):
        pass  
    elif not any(""deprecated"" in msg and ""removed"" in msg for msg in warning_messages):
        raise AssertionError(""Expected deprecation warning was not raised."")

    for line in output.lines:
        if line.get_linestyle() != ""None"":
            raise AssertionError(""Linestyle is not set to 'none' as expected."")
        break","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, with remove connecting lines",,"sns.pointplot(x='x', y='y', data=data, markers=""o"", linestyles=""none"")",panda,0.13.0,,,seaborn.pointplot(),2023-09 ,new func/method/class
"import seaborn as sns
import pandas as pd


data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})

def custom_pointplot(data):
    return ",seaborn,"
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_pointplot(data)
    
    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]

    if any(""dataframegroupby.apply"" in msg for msg in warning_messages):
        pass  
    elif not any(""deprecated"" in msg and ""removed"" in msg for msg in warning_messages):
        raise AssertionError(""Expected deprecation warning was not raised."")
    
    found_correct_linewidth = False
    for line in output.lines: 
        linewidth = line.get_linewidth()
        if linewidth == 2:
            found_correct_linewidth = True
            break

    if not found_correct_linewidth:
        raise AssertionError(""Error bar linewidth is not set to 2 as expected."")","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, adjust error bar width to 2.",,"sns.pointplot(x='x', y='y', data=data, err_kws={""linewidth"": 2})",panda,0.13.0,,,seaborn.pointplot(),2023-09 ,new func/method/class
"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_violinplot(data):
    return ",seaborn,"
import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_violinplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""bw"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""bw parameter should not be used. Use bw_method and bw_adjust instead."")

    for collection in output.collections:
            if hasattr(collection, ""get_paths""):
                assert sns.violinplot.__defaults__[0] == 1.5
    ","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, scales the bandwidth to 1.5.",,"sns.violinplot(x='x', y='y', data=data, bw_adjust=1.5)",panda,0.13.0,,,seaborn.violinplot,2023-09 ,new func/method/class
"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_violinplot(data):
    return ",seaborn,"
import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_violinplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""bw"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""bw parameter should not be used. Use bw_method and bw_adjust instead."")
    
    collections = [c for c in output.collections if isinstance(c, plt.Line2D)]  # Extract violin plot lines
    
    assert output is not None, ""Violin plot output should not be None.""","Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, choose bandwidth to scott.",,"sns.violinplot(x='x', y='y', data=data, bw_method=""scott"")",panda,0.13.0,,,seaborn.violinplot,2023-09 ,new func/method/class
"import seaborn as sns
import pandas as pd

import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_barplot(data):
    return ",seaborn,"
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    ax = custom_barplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""errcolor"" in msg or ""errwidth"" in msg for msg in warning_messages):
        raise AssertionError(""errcolor and errwidth should not be used. Use err_kws instead."")

    for line in ax.lines:
        if line.get_linewidth() == 2 and line.get_color() == 'red':
            break
    else:
        raise AssertionError(""Error bars are not set with err_kws correctly."")","Write a Seaborn barplot() function that visualizes x and y from a Pandas DataFrame, adjust error bar with color red and linewidth 2.",,"sns.barplot(x='x', y='y', data=data, err_kws={'color': 'red', 'linewidth': 2})",panda,0.13.0,,,seaborn.barplot(),2023-09 ,new func/method/class
"import seaborn as sns
import pandas as pd
import warnings
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_boxenplot(data):
    return ",seaborn,"
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_boxenplot(data)

    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""scale"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""scale should not be used in boxenplot. Use width_method instead."")

    for artist in output.get_children():
        if hasattr(artist, ""get_linestyle"") and artist.get_linestyle() in [""-"", ""--""]:
            break
    else:
        raise AssertionError(""Boxen elements are missing, width_method might not be applied."")","Write a Seaborn boxenplot() function that visualizes x and y from a Pandas DataFrame, make width method to exponential.",,"sns.boxenplot(x='x', y='y', data=data, width_method='exponential')",panda,0.13.0,,,seaborn.boxenplot(),2023-09 ,argument change
"import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

data = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})

def custom_set_axis_labels(data):
    ax = sns.scatterplot(x='x', y='y', data=data)
    ax.",seaborn,"
ax = custom_set_axis_labels(data)
x_expect = ""My X Label""
y_expect = ""My Y Label""
assert ax.get_xlabel() == x_expect and ax.get_ylabel() == y_expect, (
    ""Axis labels not set correctly using ax.set().""
)
","Write a custom function that visualizes x and y from a Pandas DataFrame, set the X label to be ""My X Label"" and Y label to be ""My Y Label"".",,"set(xlabel=""My X Label"", ylabel=""My Y Label"")
    return ax",panda,0.12.0,,,seaborn.scatterplot().set(),2022-09 ,argument change
"import numpy as np

data_array = np.array([1, 2, 3, 4, 5])

def custom_iqr(data):
    from ",seaborn,"computed_iqr = custom_iqr(data_array)
expect = 2
assert computed_iqr == expect
","Write a custom function to compute iqr for input data. If needed, use another library.",,"scipy.stats import iqr
    return iqr(data)
",scipy,0.12.0,,,seaborn.iqr(),2022-09 ,new func/method/class
"import time
import mitmproxy.connection as conn

ip_address = ""127.0.0.1""
i_port = 111
o_port = 222

output_client = ",mitmproxy,"expect_peername = (""127.0.0.1"", 111)
expect_sockname = (""127.0.0.1"", 222)

assert output_client.peername == expect_peername
assert output_client.sockname == expect_sockname
","Using mitmproxy’s connection API, create a client connection by specifying the IP address (ip_address), the input port (i_port), the output port (o_port), and the timestamp (using time.time() for the current time). Store the resulting client connection in the variable output_client.",," conn.Client(
    peername=(ip_address, i_port),
    sockname=(ip_address, o_port),
    timestamp_start=time.time()
)",,9.0.1,,,mitmproxy.connection.Client,2022-11 ,argument change
"import mitmproxy.connection as conn

ip_address = ""192.168.1.1""
server_port = 80

output_server = ",mitmproxy,"expect = (""192.168.1.1"", 80)
assert output_server.address == expect","Using mitmproxy’s connection API, create a server connection with the given parameters: an IP address (ip_address), a server port (server_port), and store the resulting instance in the variable output_server. ",," conn.Server(
    address=(ip_address, server_port)
)",,9.0.1,,,mitmproxy.connection.Server,2022-11 ,argument change
"import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ",mitmproxy,"        
import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_connected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_connected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the server connected event hook. When a server connection is established, the implemented method should be called with a parameter server_conn, and it must print the server’s local address using the attribute server_conn.sockname in the following format: Server connected with local address {server_conn.sockname}.",,"server_connected(self, server_conn):
        print(f""Server connected with local address {server_conn.sockname}"")",,7.0.0,,,"server_connected
",2021-07 ,argument change
"import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ",mitmproxy,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_connect(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_connect(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the server connect event hook. When a server connection is established, the implemented method should be called with a parameter server_conn, and it must print the server’s local address using the attribute server_conn.sockname in the following format: Server connect {server_conn.sockname}.",,"server_connect(self, server_conn):
        print(f""Server connect : {server_conn.sockname}"")",,7.0.0,,,server_connect,2021-07 ,new func/method/class
"import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ",mitmproxy,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_disconnected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_disconnected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the server disconnected event hook. When a server connection is terminated, the implemented method should be called with a parameter server_conn, and it must print the server’s local address using the attribute server_conn.sockname in the following format: Server disconnected: {server_conn.sockname}.",,"server_disconnected(self, server_conn):
        print(f""Server disconnected: {server_conn.sockname}"")",,7.0.0,,,"server_disconnected
",2021-07 ,argument change
"import contextlib

class DummyClientConn:
    def __init__(self, peername):
        self.peername = peername

class ConnectionLogger:
    def ",mitmproxy,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_client_connected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyClientConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.client_connected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the client connected event hook. When a client connection is established, the implemented method should be called with a parameter client_conn, and it must print the client's local address using the attribute client_conn.peername in the following format: Client connected: {client_conn.peername}.",,"client_connected(self, client_conn):
        print(f""Client connected: {client_conn.peername}"")",,7.0.0,,,client_connected,2021-07 ,argument change
"import contextlib

class DummyClientConn:
    def __init__(self, peername):
        self.peername = peername

class ConnectionLogger:
    def ",mitmproxy,"import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_client_disconnected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyClientConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.client_disconnected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()","Using mitmproxy’s connection API, create a Python class named ConnectionLogger that implements the client disconnected event hook. When a client connection is terminated, the implemented method should be called with a parameter client_conn, and it must print the client's local address using the attribute client_conn.peername in the following format: Client disconnected: {client_conn.peername}.",,"client_disconnected(self, client_conn):
        print(f""Client disconnected: {client_conn.peername}"")",,7.0.0,,,client_disconnected,2021-07 ,argument change
"
import contextlib

class DummyLogEntry:
    def __init__(self, msg):
        self.msg = msg

class MyAddon:
    def ",mitmproxy,"import unittest
import io
class TestMyAddonLogging(unittest.TestCase):
    def test_logging_event(self):
        addon = MyAddon()
        dummy_entry = DummyLogEntry(""Test log message"")
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            addon.add_log(dummy_entry)
        print(output.getvalue())
        
        self.assertIn(""Test log message"", output.getvalue())

unittest.main()","Mitmproxy triggers a logging event that passes a log entry object containing a message in its msg attribute. Given the following class definition for MyAddon, extend it by adding a method that handles this log event. Your implementation should use f-string formatting to print the log entry’s message. Provide the complete code for the MyAddon class. ",,"add_log(self, entry):
        print(f""{entry.msg}"")",,7.0.0,,,add_log,2021-07 ,name change
"import types

class DummyCert:
    def __init__(self, hostname):
        self.cert_pem = f""-----BEGIN CERTIFICATE-----\nDummy certificate for {hostname}\n-----END CERTIFICATE-----""
        self.key_pem = f""-----BEGIN PRIVATE KEY-----\nDummy key for {hostname}\n-----END PRIVATE KEY-----""

class DummyCA:
    def __init__(self, path):
        self.path = path

    def get_cert(self, hostname):
        return DummyCert(hostname)
    
certs = types.ModuleType(""certs"")
certs.CA = DummyCA

def generate_cert_new(hostname):

    ca = certs.CA(""dummy/path"")
    cert_obj = ",mitmproxy,"def test_generate_cert_new():
    hostname = ""example.com""
    cert_pem, key_pem = generate_cert_new(hostname)
    
    assert ""BEGIN CERTIFICATE"" in cert_pem, ""Certificate PEM missing header""
    assert ""BEGIN PRIVATE KEY"" in key_pem, ""Key PEM missing header""
    
    assert hostname in cert_pem, ""Hostname not found in certificate PEM""
    
    assert cert_pem.strip() != """", ""Certificate PEM is empty""
    assert key_pem.strip() != """", ""Key PEM is empty""
    
test_generate_cert_new()","Complete the implementation of the generate_cert_new function so that it obtains a certificate object by calling the correct method on the CA, and returns a tuple containing the certificate PEM and key PEM.",,"ca.get_cert(hostname)
    return cert_obj.cert_pem, cert_obj.key_pem",,7.0.0,,,mitmproxy.certs,2021-07 ,output behaviour
"header_name = b""Content-Type""
initial_value = b""text/html""

from mitmproxy.",mitmproxy,"
expect = ""text/html""
assert output.get(header_name) == expect",Update the code by writing the correct import statement for the Headers class from mitmproxy.http. Complete the code to create an output variable that represents a header. The header object takes header_name and initial_value as inputs.,,"http import Headers
output = Headers([(header_name, initial_value)])",,7.0.0,,,mitmproxy.net.http.Headers,2021-07 ,breaking change
"import pytest

@pytest.",pytest,"import pluggy

def test_hookimpl_configuration_with_plugin_manager():
    pm = pluggy.PluginManager(""pytest"")
    
    class DummyPlugin:
        pytest_runtest_call = pytest_runtest_call

    plugin = DummyPlugin()
    pm.register(plugin)
    
    hookimpls = pm.hook.pytest_runtest_call.get_hookimpls()
    
    for impl in hookimpls:
        if impl.plugin is plugin:
            opts = impl.opts 
            assert opts.get(""tryfirst"") is False
            break
    else:
        pytest.fail(""pytest_runtest_call implementation not found in plugin manager."")



test_hookimpl_configuration_with_plugin_manager()","Update the code by writing the correct import statement for the hook implementation decorator from the testing framework. Then, complete the code to define a hook implementation function named pytest_runtest_call that uses this decorator with its execution priority parameter set to false; the function body should contain only the pass statement.",,"hookimpl(tryfirst=False)
def pytest_runtest_call():
    pass",,7.0.0,,,pytest.hookimpl(),2022-02 ,new func/method/class
"import pytest

@pytest.",pytest,"import pluggy

def test_hookwrapper_configuration_with_plugin_manager():
    pm = pluggy.PluginManager(""pytest"")
    
    class DummyPlugin:
        pytest_runtest_setup = pytest_runtest_setup

    plugin = DummyPlugin()
    pm.register(plugin)
    
    hookimpls = pm.hook.pytest_runtest_setup.get_hookimpls()
    for impl in hookimpls:
        if impl.plugin is plugin:
            opts = impl.opts
            assert opts.get(""hookwrapper"") is True, ""Expected hookwrapper=True for a hook wrapper""
            break
    else:
        pytest.fail(""pytest_runtest_setup implementation not found in plugin manager."")


test_hookwrapper_configuration_with_plugin_manager()",Update the code by writing the correct import statement for the hook implementation decorator from the testing framework and then complete the code to define a hook implementation function named pytest_runtest_setup that uses this decorator with its hookwrapper parameter set to True; the function body should contain only a yield statement.,,"hookimpl(hookwrapper=True)
def pytest_runtest_setup():
    yield",,7.0.0,,,pytest.hookimpl(hookwrapper),2022-02 ,new func/method/class
"import pytest
import pathlib

@pytest.hookimpl()
def pytest_ignore_collect(",pytest,"import inspect
def test_pytest_ignore_collect_signature():
    sig = inspect.signature(pytest_ignore_collect)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_ignore_collect_signature()
",Complete code snippet that defines a hook implementation function named pytest_ignore_collect which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,,"collection_path:pathlib.Path):
    pass",,7.0.0,,,pytest_ignore_collect(collection_path: pathlib.Path),2022-02 ,argument change
"import pytest
import pathlib

@pytest.hookimpl()
def pytest_collect_file(",pytest,"
import inspect
def test_pytest_collect_file_signature():
    sig = inspect.signature(pytest_collect_file)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_collect_file_signature()",Complete code snippet that defines a hook implementation function named pytest_collect_file which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,,"file_path:pathlib.Path):
    pass",,7.0.0,,,pytest_collect_file(file_path: pathlib.Path),2022-02 ,argument change
"import pytest
import pathlib

@pytest.hookimpl()
def pytest_pycollect_makemodule(",pytest,"import inspect
def test_pytest_pycollect_makemodule_signature():
    sig = inspect.signature(pytest_pycollect_makemodule)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_pycollect_makemodule_signature()
",Complete code snippet that defines a hook implementation function named pytest_pycollect_makemodule which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,,"module_path:pathlib.Path):
    pass",,7.0.0,,,pytest_pycollect_makemodule(module_path: pathlib.Path),2022-02 ,argument change
"import pytest
import pathlib

@pytest.hookimpl()
def pytest_report_header(",pytest,"
import inspect
def test_pytest_report_header_signature():
    sig = inspect.signature(pytest_report_header)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_report_header_signature()
",Complete code snippet that defines a hook implementation function named pytest_report_header which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,,"start_path:pathlib.Path):
    pass
",,7.0.0,,,pytest_report_header(start_path: pathlib.Path),2022-02 ,argument change
"import pytest
import pathlib

@pytest.hookimpl()
def pytest_report_collectionfinish(",pytest,"
import inspect
def test_pytest_report_collectionfinish_signature():
    sig = inspect.signature(pytest_report_collectionfinish)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_report_collectionfinish_signature()
",Complete code snippet that defines a hook implementation function named pytest_report_collectionfinish which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,,"start_path:pathlib.Path):
    pass",,7.0.0,,,pytest_report_collectionfinish(start_path: pathlib.Path),2022-02 ,argument change
"import pytest

class CustomItem(pytest.Item):
    def __init__(",pytest,"import inspect
signature = inspect.signature(CustomItem.__init__)
assert any(param.kind == param.VAR_KEYWORD for param in signature.parameters.values())",Complete code snippet that defines a custom subclass of pytest.Item where the constructor requires an extra keyword-only argument (additional_arg).,,"self, *, additional_arg, **kwargs):
        super().__init__(**kwargs)
        self.additional_arg = additional_arg",,7.0.0,,,pytest.Item,2022-02 ,argument change
"
import pytest

def foo(a, b):
    return (10 * a - b + 7) // 3

@pytest.mark.parametrize(
    [""a"", ""b"", ""result""],
    [
        [1, 2, 5],
        [2, 3, 8],
        [5, 3, 18],
    ],
)
def test_foo(a, b, result):
    ",pytest,"import dis
import inspect
def test_assert_in_test_foo_bytecode():
    original_test_foo = inspect.unwrap(test_foo)
    instructions = list(dis.get_instructions(original_test_foo))
    has_raise = any(instr.opname == ""RAISE_VARARGS"" for instr in instructions)
    assert has_raise
    
test_assert_in_test_foo_bytecode()","Provide a complete code snippet where a custom function named test_foo(a, b, result) verifies whether foo(a, b) == result. Ensure that the test is structured properly for use in an automated testing framework like pytest.",,"    assert foo(a, b) == result
",,7.2.0,,,pytest.PytestReturnNotNoneWarning,2022-10 ,output behavior
"import pytest

@pytest.",pytest,"def sample_data():
    data = [1, 2, 3]
    yield data

def test_sample_data(sample_data):
    assert sample_data == [1, 2, 3]

if __name__ == ""__main__"":
    pytest.main([""-q"", ""--tb=short""]) ",Complete code snippet by using fixtures in pytest.,,fixture,,6.2.0,,,pytest.yield_fixture,2020-12 ,new func/method/class
"import pytest

@pytest.",pytest,"def test_squared(x, y):
    assert x**x == y

if __name__ == ""__main__"":
    pytest.main([""-q"", ""--tb=short""]) ","Write a pytest function that verifies whether a given function correctly squares an input number using parameterized test cases: (2, 4).",,"mark.parametrize(""x, y"", [(2, 4)])",,7.2.0,,,pytest.mark.parametrize,2022-10 ,new func/method/class
"from falcon import stream

import io
class DummyRequest:
    def __init__(self, data: bytes):
        self.stream = io.BytesIO(data)
        self.content_length = len(data)
        
test_data = b""Hello, Falcon!""
req = DummyRequest(test_data)

def get_bounded_stream(req):
    return ",falcon,"bounded_stream = get_bounded_stream(req)
read_data = bounded_stream.read()
expect = b""Hello, Falcon!""
assert read_data == expect","Provide a complete code snippet that defines a custom function get_bounded_stream, which accepts a req object and wraps the incoming request stream with a controlled reader. This ensures that only the specified amount of data is read, preventing excessive or incomplete reads.",,"stream.BoundedStream(req.stream, req.content_length)",,3.0.0,,,falcon.stream.BoundedStream,2021-04 ,new func/method/class
"import falcon
resp = falcon.Response()

info = 'Falcon'

def custom_body(resp):
    resp.",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    resp = custom_body(resp)
    if w:
        assert issubclass(w[-1].category, DeprecationWarning), ""Expected a DeprecationWarning but got something else!""

expect = 'Falcon'
assert resp.text == expect
"," Complete code snippet that defines a custom function custom_body which accepts a Falcon Response object as input and sets its body to the variable info which is string, and finally return Response object.",,"text = info
    return resp",,3.0.0,,,falcon.Response.body,2021-04 ,argument change
"import falcon
from falcon import HTTPStatus

status = HTTPStatus(falcon.HTTP_200)

info = 'Falcon'

def custom_body(status):
    status.",falcon,"
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    resp = custom_body(status)
    if w:
        assert issubclass(w[-1].category, DeprecationWarning), ""Expected a DeprecationWarning but got something else!""

expect = 'Falcon'
assert resp.text == expect
"," Complete code snippet that defines a custom function custom_body which accepts a Falcon HTTPStatus object as input and sets its body to the variable info which is string, and finally return HTTPStatus object.",,"text = info
    return status",,3.0.0,,,falcon.HTTPStatus.body,2021-04 ,argument change
"from falcon import Response

info = ""Falcon""

def custom_body_length(resp: Response, info):
    resp.",falcon,"class DummyResponse(Response):
    pass

resp = DummyResponse()

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    custom_resp = custom_body_length(resp, info)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), \
                ""Deprecated API used!""
expect = str(len(info))
assert custom_resp.content_length == expect"," Complete code snippet that defines a custom function custom_body_length which accepts a Falcon Response object as input and sets its body length as length of variable info , and finally return Response object.",,"content_length = len(info)
    return resp",,3.0.0,,,falcon.Response.stream_len,2021-04 ,argument change
"
from falcon import Response

info = ""Falcon data""

def custom_data(resp: Response):
    resp.data = info
    return ",falcon,"class DummyResponse(Response):
    pass

resp = DummyResponse()
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    rendered_body = custom_data(resp)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""


expect = info
assert rendered_body == expect"," Complete code snippet that defines a custom function custom_data which accepts a Falcon Response object as input and sets its data as variable info, processes the data property and returns it in the correct format for an HTTP response.",,resp.render_body(),,3.0.0,,,falcon.Response.data.render_body,2021-04 ,new func/method/class
"import falcon
from falcon import HTTPError


def custom_http_error():
    err = HTTPError(falcon.HTTP_400, ""Bad Request"", ""An error occurred"")
    return ",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    result = custom_http_error()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""


expect = b'{""title"": ""Bad Request"", ""description"": ""An error occurred""}'
assert result == expect","Complete the code snippet that defines a custom function custom_http_error, ensuring it correctly raises an HTTP error in Falcon. The function should return a JSON response representing the error. ",,err.to_json(),,3.0.0,,,falcon.HTTPError.to_json(),2021-04 ,name change
"import falcon.testing as testing

info = ""/my/root/path""

def custom_environ():
    return ",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    env = custom_environ()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect = info
assert env.get('SCRIPT_NAME', '') == expect","Complete the code snippet that defines a custom function custom_environ, which should create and return an environment with info variable as the root.",,testing.create_environ(root_path=info),,3.0.0,,,falcon.testing.create_environ(),2021-04 ,argument change
"import io
import warnings
from falcon.stream import BoundedStream

def custom_writable(bstream: BoundedStream):
    return ",falcon,"stream = io.BytesIO(b""initial data"")
bstream = BoundedStream(stream, 1024)

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    writable_val = custom_writable(bstream)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = False 
assert writable_val == expect","Complete the code snippet that defines a custom function custom_writable, which should accepts a BoundedStream object and returns its writable property as Boolean data type.",,bstream.writable(),,3.0.0,,,falcon.stream.BoundedStream.writeable,2021-04 ,argument change
"import falcon.app_helpers as app_helpers

class ExampleMiddleware:
    def process_request(self, req, resp):
        pass

def custom_middleware_variable():
    return ",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    middleware = custom_middleware_variable()
    prepared_mw = app_helpers.prepare_middleware(middleware)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = (list, tuple)
assert isinstance(prepared_mw, expect)
","Complete the code snippet that defines a custom function custom_middleware_variable, which should create an ExampleMiddleware object that should be accepted by the function app_helpers.prepare_middleware().",,[ExampleMiddleware()],,3.0.0,,,falcon.app_helpers.prepare_middleware(),2021-04 ,name change
"import falcon.testing as testing

def custom_environ():
    return ",falcon,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    env = custom_environ()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = ""HTTP/1.1""
assert env.get('SERVER_PROTOCOL', '') == expect","Complete the code snippet that defines a custom function custom_environ, which should set the HTTP version to 1.1 and return the environment object.",,"testing.create_environ(http_version=""1.1"")",,3.0.0,,,falcon.testing.create_environ(http_version=),2021-04 ,ouput behavior
"from falcon import Response

resp = Response()
link = 'http://example.com'
rel = 'preconnect'

def custom_append_link(resp, link, rel):
    resp.",falcon,"response = custom_append_link(resp, link, rel)
expected = ""crossorigin""
assert expected in response.get_header('Link')","Complete the code snippet by defining a custom function named custom_append_link that takes a Falcon Response object, a string link, and a string rel as inputs. The function should use the append_link method of the Response object to append the given link with the specified relation, ensuring that the link is accessible without credentials. The function should then return the updated response.",,"append_link(link, rel, crossorigin='anonymous')
    return resp",,3.0.0,,,falcon.Response.append_link(),2021-04 ,new func/method/class
"import falcon

def custom_falcons():
    return ",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    app_instance = custom_falcons()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = falcon.App
assert isinstance(app_instance, expect)",Complete the code snippet by defining a custom function named custom_falcons that creates a Falcon-based WSGI app and return it.,,falcon.App(),,3.0.0,,,falcon.API,2021-04 ,name change
"from falcon import Response

link_rel = ""next""
link_href = ""http://example.com/next""
resp = Response()

def custom_link(resp: Response, link_rel, link_href):
    resp.",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    custom_resp = custom_link(resp,link_rel,link_href)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expected_link = f'<{link_href}>;'
link_header = custom_resp.get_header(""Link"") or """"
assert expected_link in link_header","Define a function named custom_link that accepts a Falcon Response object, a string indicating the relationship of the link (link_rel), and a string for the link URL (link_href). The function should incorporate the link into the response’s headers—ensuring that the relationship and URL are correctly associated—and then return the modified response.",,"append_link(link_href, link_rel)
    return resp",,3.0.0,,,falcon.Request.add_link(),2021-04 ,name change
"import json
from falcon import Request
from falcon.testing import create_environ

payload = {""key"": ""value""}
body_bytes = json.dumps(payload).encode(""utf-8"")

env = create_environ(
    body=body_bytes,
    headers={'Content-Type': 'application/json'}
)

req = Request(env)

def custom_media(req: Request):
    return ",falcon,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    media = custom_media(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect = payload
assert media == expect","Create a function named custom_media that accepts a Falcon Request object and retrieves the parsed request body (the media) as a Python data structure. The function should then return this parsed content.
",,req.get_media(),,3.0.0,,,falcon.Request.media,2021-04 ,new func/method/class
"import falcon 

error_message = ""Request content is too large""

def raise_too_large_error():
    raise ",falcon,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    try:
        raise_too_large_error()
    except falcon.HTTPPayloadTooLarge as e:
        exception_raised = e
    else:
        exception_raised = None

    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expected_message = error_message
assert str(exception_raised) == expected_message","Define a function named raise_too_large_error that, when called, raises an exception indicating that the request content exceeds acceptable limits, using the provided error_message variable as the error detail.
",,falcon.HTTPPayloadTooLarge(error_message),,2.0.0,,,falcon.HTTPRequestEntityTooLarge,2019-04 ,name change
"
from falcon.uri import parse_query_string

query_string = ""param1=value1&param2=""

def custom_parse_query(qs):
    return ",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    parsed_values = custom_parse_query(query_string)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect1 = 'value1'
expect2 = ''
assert parsed_values.get('param1') == expect1
assert parsed_values.get('param2') == expect2","Define a function named custom_parse_query that accepts a query string as its input and returns its parsed representation. The function should leverage the utility from falcon.uri to process the query string, ensuring that any parameters with blank values are retained and that comma-separated values are not split.",,"parse_query_string(qs, keep_blank=True, csv=False)",,2.0.0,,,falcon.uri.parse_query_string,2019-04 ,argument change
"import json
from falcon import Request
from falcon.testing import create_environ

json_value = json.dumps({""bar"": ""baz""})
query_string = f""foo={json_value}""

env = create_environ(query_string=query_string)
req = Request(env)

def custom_get_param(req: Request):
    return ",falcon,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    result = custom_get_param(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = {""bar"": ""baz""}
assert result == expect","Define a function named custom_get_param that accepts a Falcon Request object. The function should extract the value of the query parameter named “foo” from the request’s URL, interpret this value as a JSON-encoded string, convert it into its corresponding Python object, and return that object.
",,"req.get_param_as_json(""foo"")",,2.0.0,,,falcon.Request.get_param_as_dict(),2019-04 ,new func/method/class
"import falcon

def handle_error(",falcon,"class DummyReq:
    pass

class DummyResp:
    def __init__(self):
        self.media = None
        self.status = None

dummy_req = DummyReq()
dummy_resp = DummyResp()
dummy_ex = Exception(""Test error"")
dummy_params = {}

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    handle_error(dummy_req, dummy_resp, dummy_ex, dummy_params)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect1 = {""error"": ""Test error""}
expect2 = falcon.HTTP_500
assert dummy_resp.media == expect1
assert dummy_resp.status == expect2","Complete a function named handle_error that acts as an error handler in a Falcon application. The function should accept the request, response, exception, and additional parameters. Its purpose is to update the response by setting its media to a JSON object containing an error message (derived from the exception) and updating the HTTP status to indicate an internal server error. ",,"req, resp, ex, params):
    resp.media = {""error"": str(ex)}
    resp.status = falcon.HTTP_500",,2.0.0,,,handle_error,2019-04 ,argument change
"from falcon import Request
from falcon.testing import create_environ

env = create_environ(query_string=""dpr=2"")
req = Request(env)

def custom_get_dpr(req: Request):
    return ",falcon,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    dpr = custom_get_dpr(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = 2
assert dpr == expect","Define a function named custom_get_dpr that accepts a Falcon Request object and retrieves the value of the “dpr” query parameter as an integer. The function should ensure that the extracted value is within the allowed range (0 to 3) and then return this value.
",,"req.get_param_as_int(""dpr"", min_value=0, max_value=3)",,2.0.0,,,falcon.Request.get_param_as_int,2019-04 ,argument change
"from falcon import Request
from falcon.testing import create_environ

env = create_environ()
req = Request(env)

role = 'trial'
user = 'guest'
def custom_set_context(req: Request, role, user):
    req.",falcon,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    context = custom_set_context(req, role, user)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect1 = 'trial'
expect2 = 'guest'

assert context.role == expect1
assert context.user == expect2","Define a function named custom_set_context that takes a Falcon Request object along with two string arguments representing a role and a user. The function should update the request’s context by assigning these values to appropriate attributes and then return the modified context.
",,"context.role = role
    req.context.user = user
    return req.context
",,2.0.0,,,falcon.Request. context_type,2019-04 ,output behavior
"
class CustomRouter:
    def __init__(self):
        self.routes = {}

    def add_route(",falcon,"class DummyResource:
    def on_get(self, req, resp):
        resp.text = ""hello""
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    router = CustomRouter()
    method_map = router.add_route(""/test"", DummyResource())
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = ""/test""
assert expect in router.routes
resource, mapping = router.routes[""/test""]
assert callable(mapping.get(""GET"", None))","Create a class named CustomRouter to manage your application’s routes. The class should maintain an internal dictionary to store routes and their associated resources. Implement an add_route method that accepts a URI template, a resource, and additional keyword arguments. This method should generate a mapping of HTTP methods to resource handlers—using an appropriate Falcon utility—and store the resulting tuple (resource, method mapping) in the routes dictionary. You can import falcon.routing if needed. Finally, the method should return the generated mapping.
",,"self, uri_template, resource, **kwargs):
        from falcon.routing import map_http_methods
        method_map = map_http_methods(resource, kwargs.get('fallback', None))
        self.routes[uri_template] = (resource, method_map)
        return method_map",,2.0.0,,,add_route(),2019-04 ,new func/method/class
"import asyncio
import os
import signal

def custom_add_callback_from_signal(callback, signum):

    loop = ",tornado,"
def test_custom_signal_handler():

    flag = {""executed"": False}
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    def callback():
        flag[""executed""] = True
        loop.stop()

    custom_add_callback_from_signal(callback, signal.SIGUSR1)

    os.kill(os.getpid(), signal.SIGUSR1)

    loop.run_forever()

    return flag[""executed""]

result = test_custom_signal_handler()
assert result","Write a custom function named custom_add_callback_from_signal that registers a signal handler. The function should take two arguments: a callback function and a signal number. When the specified signal is received, the callback should be executed.
",,"asyncio.get_event_loop()
    loop.add_signal_handler(signum, callback)
",,6.3.0,,,IOLoop.add_callback_from_signal,2023-11 ,new func/method/class
"import tornado.wsgi
import tornado.httpserver
import tornado.ioloop
import tornado.httpclient
import concurrent.futures
import socket

# A simple WSGI application that returns ""Hello World""
def simple_wsgi_app(environ, start_response):
    status = ""200 OK""
    headers = [(""Content-Type"", ""text/plain"")]
    start_response(status, headers)
    return [b""Hello World""]

def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def custom_wsgi_container(app, executor):

    return ",tornado,"def test_wsgi_container_executor():

    executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
    
    container = custom_wsgi_container(simple_wsgi_app, executor)
    
    port = find_free_port()
    server = tornado.httpserver.HTTPServer(container)
    server.listen(port)
    
    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}""
    
    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))
    
    server.stop()
    executor.shutdown(wait=True)
    
    return response.body == b""Hello World""

result = test_wsgi_container_executor()
assert result
",Write a custom function that wraps a given WSGI application in a Tornado WSGIContainer using a provided executor so that the app runs on a thread pool.,,"tornado.wsgi.WSGIContainer(app, executor=executor)",,6.3.0,,,tornado.wsgi,2023-04 ,argument change
"import tornado.ioloop
import tornado.web
import tornado.httpserver
import tornado.websocket
import tornado.httpclient
import socket

async def custom_websocket_connect(url, resolver):

    return await ",tornado,"class EchoWebSocketHandler(tornado.websocket.WebSocketHandler):
    def open(self):
        print(""WebSocket opened"")

    def on_message(self, message):
        self.write_message(message)

    def on_close(self):
        print(""WebSocket closed"")

def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def test_websocket_large_message():

    resolver = None

    app = tornado.web.Application([
        (r""/ws"", EchoWebSocketHandler),
    ])
    port = find_free_port()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)

    ws_url = f""ws://localhost:{port}/ws""

    large_message = ""A"" * 100000  # 100k characters

    async def run_test():
        conn = await custom_websocket_connect(ws_url, resolver)
        conn.write_message(large_message)
        echoed = await conn.read_message()
        conn.close()
        return echoed == large_message

    result = tornado.ioloop.IOLoop.current().run_sync(run_test)

    server.stop()
    return result

result = test_websocket_large_message()
assert result
",Write a custom function that establishes a Tornado WebSocket connection using a provided resolver parameter to efficiently handle large fragmented messages.,,"tornado.websocket.websocket_connect(url, resolver=resolver)",,6.3.0,,,tornado.websocket,2023-04 ,argument change
"import tornado.web
import tornado.ioloop
import tornado.httpserver
import tornado.httpclient
import socket

COOKIE_SECRET = ""MY_SECRET_KEY""

class GetCookieHandler(tornado.web.RequestHandler):
    def get(self):
        cookie_value =",tornado,"def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def make_app():
    return tornado.web.Application([
        (r""/get"", GetCookieHandler),
    ], cookie_secret=COOKIE_SECRET)

def test_get_secure_cookie():

    port = find_free_port()
    app = make_app()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)
    
    # Create a signed cookie value for ""testvalue""
    signed_cookie = tornado.web.create_signed_value(COOKIE_SECRET, ""mycookie"", ""testvalue"")
    cookie_header = ""mycookie="" + signed_cookie.decode()

    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}/get""
    
    # Include the signed cookie in the request headers.
    response = tornado.ioloop.IOLoop.current().run_sync(
        lambda: client.fetch(url, headers={""Cookie"": cookie_header})
    )
    server.stop()
    return response.body.decode() == ""testvalue""

result_get = test_get_secure_cookie()
assert result_get",Write a custom test case that sends a signed cookie named “mycookie” to a Tornado RequestHandler and verifies that the correct decoded cookie value is returned.,,"self.get_signed_cookie(""mycookie"")
        if cookie_value:
            self.write(cookie_value.decode())
",,6.3.0,,,tornado.web.RequestHandler.get_secure_cookie,2023-04 ,argument change
"import tornado.web
import tornado.ioloop
import tornado.httpserver
import tornado.httpclient
import socket

COOKIE_SECRET = ""MY_SECRET_KEY""

class SetCookieHandler(tornado.web.RequestHandler):
    def get(self):
        self.",tornado,"def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def make_app():
    return tornado.web.Application([
        (r""/set"", SetCookieHandler),
    ], cookie_secret=COOKIE_SECRET)

def test_set_secure_cookie():

    port = find_free_port()
    app = make_app()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)
    
    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}/set""
    
    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))
    server.stop()
    # Check that a Set-Cookie header is present with the cookie name ""mycookie=""
    set_cookie_headers = response.headers.get_list(""Set-Cookie"")
    return any(""mycookie="" in header for header in set_cookie_headers)

result_set = test_set_secure_cookie()
assert result_set","Write a test case that verifies a Tornado RequestHandler correctly sets a signed cookie named “mycookie” with the value “testvalue”, by checking that the response includes a Set-Cookie header with the expected cookie name and a properly signed value.",,"set_signed_cookie(""mycookie"", ""testvalue"")
        self.write(""Cookie set"")",,6.3.0,,,tornado.web.RequestHandler.set_secure_cookie,2023-04 ,argument change
"import asyncio
import tornado.auth
import asyncio

class DummyAuth(tornado.auth.OAuth2Mixin):
    async def async_get_user_info(self, access_token):
        return ",tornado,"async def custom_auth_test():
    auth = DummyAuth()
    result = await auth.async_get_user_info(""dummy_token"")
    expect = ""dummy_token""
    assert result['token'] == expect

async def main():
    result = await custom_auth_test()

if __name__ == ""__main__"":
    asyncio.run(main())","Create a class named DummyAuth that extends Tornado’s OAuth2Mixin. Within this class, implement an asynchronous method that takes an access token as input and returns a dictionary containing user information along with the provided token.
",,"{""user"": ""test"", ""token"": access_token}",,6.0.0,,,tornado.auth (all callback arguments),2019-03 ,new func/method/class
"import tornado.httputil

class DummyConnection:
    def __init__(self):
        self.buffer = []

    def write(self, chunk):
        self.buffer.append(chunk)

req = tornado.httputil.HTTPServerRequest(method=""GET"", uri=""/"")
req.connection = DummyConnection()

def custom_write(request, text):
    request.",tornado,"written_data = custom_write(req, ""Hello, Tornado!"")
expect = [""Hello, Tornado!""]
assert written_data == expect","Define a function named custom_write that accepts a Tornado HTTPServerRequest object and a text string. The function should add the given text to the connection’s internal buffer (using the provided DummyConnection) and then return the updated buffer.
",,"connection.write(text)
    return request.connection.buffer",,6.0.0,,,HTTPServerRequest.write,2019-03 ,new func/method/class
"import tornado.ioloop

def custom_get_ioloop():
    return ",tornado,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    loop_current = custom_get_ioloop()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning)
    assert loop_current is not None

",Define a function named custom_get_ioloop that returns the current Tornado IOLoop instance using the appropriate Tornado method.,,tornado.ioloop.IOLoop.current(),,5.0.0,,,IOLoop.instance,2018-03 ,new func/method/class
"import plotly.graph_objects as go

x_data = [""A"", ""B"", ""C""]
y_data = [10, 15, 7]

output = ",plotly,"expect = ""v""

assert output.data[0].orientation == expect
",Draw a vertical bar chart figure by using given x_data and y_data. Store the results in output.,,"go.Figure(data=[go.Bar(x=x_data,y=y_data,orientation=""v"")])",,4.8.0,,,bardir,2020-05 ,new func/method/class
"import plotly.graph_objects as go
fig = go.Figure()

fig.",plotly,"expect = ""paper""

assert fig.layout.annotations[0].xref == expect
assert fig.layout.annotations[0].yref == expect
","Add an annotation to a Plotly figure at position x=0.5 and y=0.5 with the text “Example Annotation”. Ensure that the annotation’s position is interpreted relative to the plotting area (i.e., using the “paper” coordinate system). Store the resulting figure in a variable named output.",,"add_annotation(
    x=0.5,
    y=0.5,
    text=""Example Annotation"",
    xref=""paper"",
    yref=""paper"",
    showarrow=False
)",,5.8.0,,,annotation.ref,2022-05 ,new func/method/class
"import plotly.graph_objects as go

x_data = [1, 2, 3]
y_data = [2, 3, 1]
color_set = 'rgba(0, 0, 0, 0.5)'

output = ",plotly,"expect = ""rgba(""
assert output.data[0].error_y.color.startswith(expect)
",Create a scatter plot with error bars using Plotly. Set the error bar color using an RGBA value (given color_set) that includes an alpha channel for opacity. Store the resulting figure in a variable named output.,,"go.Figure(data=go.Scatter(
    x=x_data,
    y=y_data,
    error_y=dict(
        color=color_set
    )
))",,5.10.0,,,opacity,2022-08 ,new func/method/class
"import plotly.graph_objects as go

fig = go.Figure(data=[go.Scatter3d(
    x=[1, 2, 3],
    y=[1, 2, 3],
    z=[1, 2, 3],
    mode='markers'
)])

fig.",plotly,"expect = 1.25

assert fig.layout.scene.camera.eye.x == expect
assert fig.layout.scene.camera.eye.y == expect
assert fig.layout.scene.camera.eye.z == expect","Create a 3D scatter plot using Plotly and update its camera settings. Set the camera’s eye position to x=1.25, y=1.25, z=1.25, store the resulting figure in a variable named fig.",,"update_layout(
    scene_camera=dict(
        eye=dict(x=1.25, y=1.25, z=1.25)
    )
)",,5.10.0,,,gl3d.cameraposition,2023-03 ,argument change
"import plotly

def custom_make_subplots(rows, cols):
    return ",plotly,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_make_subplots(2, 2)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

num_xaxes = sum(1 for key in fig.layout if key.startswith(""xaxis""))
num_yaxes = sum(1 for key in fig.layout if key.startswith(""yaxis""))
expect1 = 4
expect2 = 4
assert num_xaxes == expect1
assert num_yaxes == expect2
","Define a function named custom_make_subplots that takes two parameters, rows and cols, and returns a subplot layout created with the specified number of rows and columns.
",,"plotly.subplots.make_subplots(rows=rows, cols=cols)",,4.0.0,,,plotly.tools.make_subplots,2019-07 ,new func/method/class
"import plotly

x_data = [1, 2, 3]
y_data = [4, 5, 6]

def custom_figure(x_data, y_data):
    import plotly.",plotly,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_figure(x_data, y_data)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
        
expect1 = 1
expect2 = x_data
expect3 = y_data

assert len(fig.data) == expect1
trace = fig.data[0]

assert list(trace.x) == expect2
assert list(trace.y) == expect3","Define a function named custom_figure that accepts two lists representing x and y data. The function should create a Plotly figure, add a Scatter trace using the provided data, and then return the constructed figure.
",,"graph_objects
    fig = plotly.graph_objects.Figure()
    fig.add_trace(plotly.graph_objects.Scatter(x=x_data, y=y_data))
    return fig",,4.0.0,,,plotly.graph_objs,2019-07 ,name change
"
import plotly
def custom_chart_studio_usage():
    import ",plotly,"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    has_plot = custom_chart_studio_usage()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

assert has_plot",Define a function named custom_chart_studio_usage that verifies whether the Plotly module with Chart Studio cloud service offers its primary plotting functionality. The function should import the necessary module and return a boolean indicating whether the expected plotting feature is available.,,"chart_studio.plotly
    return hasattr(chart_studio.plotly, ""plot"")",chart-studio==1.0.0,4.0.0,,,plotly.plotly,2019-07 ,other library
"import plotly
def custom_api_usage():
    import ",plotly,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    module_name = custom_api_usage()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = ""chart_studio.api""
assert module_name == expect","Define a function named custom_api_usage that, using Chart Studio cloud service, retrieves and returns the identifier of the module responsible for API functionalities by accessing its name attribute.
",,"chart_studio.api
    return chart_studio.api.__name__",chart-studio==1.0.0,4.0.0,,,plotly.api,2019-07 ,other library
"import plotly.graph_objs as go

color = 'rgb(255,45,15)'

def custom_scatter(custom_color):
    return ",plotly,"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_scatter(color)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

scatter_trace = fig.data[0]
marker_color = scatter_trace.marker.color
expect = color
assert marker_color == expect","Define a function named custom_scatter that accepts a color value as an argument and uses Plotly’s graph objects to create a figure containing a scatter plot with a single point at coordinates (0, 0). The marker for this point should use the provided color. Finally, the function should return the created figure.",,"go.Figure(data=[go.Scatter(x=[0],y=[0],marker=go.scatter.Marker(color=custom_color)) ])",,3.0.0,,,plotly.graph_objs.Scatter(),2018-07 ,argument change
"import numpy as np
import librosa
from scipy.spatial.distance import cdist
X = np.array([[1, 3, 3, 8, 1]])
Y = np.array([[2, 0, 0, 8, 7, 2]])

# Store the distance matrix in sol_dict['dist_matrix'] and store solution in ""sol_dict['dtw']""
sol_dict = {""dist_matrix"":None, ""dtw"":None}",librosa,"gt_D = np.array([[1., 2., 3., 10., 16., 17.],
 [2., 4., 5., 8., 12., 13.],
 [3., 5., 7., 10., 12., 13.],
 [9., 11., 13., 7., 8., 14.],
 [10, 10., 11., 14., 13., 9.]])
assert np.array_equal(gt_D, sol_dict['dtw'])",Compute the dynamic time warp between arrays X and Y. ,,"sol_dict['dist_matrix'] = cdist(X.T, Y.T, metric='euclidean')
sol_dict['dtw'], _ = librosa.dtw(C=sol_dict['dist_matrix'], metric='invalid')",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.6.0,python==3.7,,librosa.dtw,2018-02,name change
"import numpy as np
import librosa
from scipy.spatial.distance import cdist
X = np.array([[1, 3, 3, 8, 1]])
Y = np.array([[2, 0, 0, 8, 7, 2]])

# Store the distance matrix in sol_dict['dist_matrix'] and store solution in ""sol_dict['dtw']""
sol_dict = {""dist_matrix"":None, ""dtw"":None}",librosa,"gt_D = np.array([[1., 2., 3., 10., 16., 17.],
 [2., 4., 5., 8., 12., 13.],
 [3., 5., 7., 10., 12., 13.],
 [9., 11., 13., 7., 8., 14.],
 [10, 10., 11., 14., 13., 9.]])
assert np.array_equal(gt_D, sol_dict['dtw'])",Compute the dynamic time warp between arrays X and Y. ,,"sol_dict['dist_matrix'] = cdist(X.T, Y.T, metric='euclidean')
sol_dict['dtw'], _ = librosa.sequence.dtw(C=sol_dict['dist_matrix'], metric='invalid')",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.7.0,python==3.7,,librosa.sequence.dtw,2019-07,name change
"import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)

# Store the solution in sol_dict['rms'].
sol_dict = {'rms':None}",librosa,"assert np.array_equal(librosa.feature.rmse(y=y), sol_dict['rms'])",Compute the root mean square value for each frame. ,,sol_dict['rms'] = librosa.feature.rmse(y=y),pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.6.0,python==3.7,,librosa.feature.rmse,2018-02,name change
"import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)

# Store the solution in sol_dict['rms'].
sol_dict = {'rms':None}",librosa,"assert np.array_equal(librosa.feature.rms(y=y), sol_dict['rms'])",Compute the root mean square value for each frame.,,sol_dict['rms'] = librosa.feature.rms(y=y),pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.7.0,python==3.7,,librosa.feature.rms,2019-07,name change
"import librosa
import numpy as np

mut_x = np.ones((8, 12))

# Store the solution in sol_dict['sol'].
sol_dict={""sol"":None}",librosa,"assert np.array_equal(librosa.fill_off_diagonal(mut_x, 0.25), sol_dict['sol'])",Fill the off diagonal with a value of 0 with the constraint region being a Sakoe-Chiba band of radius 0.25. ,,"sol_dict['sol'] = librosa.fill_off_diagonal(mut_x, 0.25)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.6.0,python==3.7,,librosa.fill_off_diagonal,2018-02,name change
"import librosa
import numpy as np

mut_x = np.ones((8, 12))

# Store the solution in sol_dict['sol'].
sol_dict={""sol"":None}",librosa,"assert np.array_equal(librosa.util.fill_off_diagonal(mut_x, 0.25), sol_dict['sol'])",Fill the off diagonal with a value of 0 with the constraint region being a Sakoe-Chiba band of radius 0.25.,,"sol_dict['sol'] = librosa.util.fill_off_diagonal(mut_x, 0.25)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.7.0,python==3.7,,librosa.util.fill_off_diagonal,2019-07,name change
"import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)
y = y.astype(np.float32)

# Extract melspectrogram from waveform y and place the quantity in sol_dict['M_from_y']. After it is computed, ensure it is of type np.float32 and store in  sol_dict['sol']
sol_dict = {""M_from_y"": None, ""sol"":None}",librosa,"sol=librosa.feature.melspectrogram(y=y, sr=sr) 
assert np.array_equal(sol, sol_dict['M_from_y'])
assert sol_dict[""M_from_y""].dtype == np.float64
sol = sol.astype(np.float32)
assert np.array_equal(sol, sol_dict['sol'])
assert sol_dict['sol'].dtype == np.float32","Extract melspectrogram from waveform y. After it is computed, ensure it is of type np.float32",,"sol_dict[""M_from_y""] = librosa.feature.melspectrogram(y=y, sr=sr) 
sol_dict['sol'] = sol_dict[""M_from_y""].astype(np.float32)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.6.0,python==3.7,,librosa.feature.melspectrogram,2018-02,behaviour
"import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)
y = y.astype(np.float32)

# Extract melspectrogram from waveform y and place the quantity in sol_dict['M_from_y']. After it is computed, ensure it is of type np.float32 and store in  sol_dict['sol']
sol_dict = {""M_from_y"": None, ""sol"":None}",librosa,"sol=librosa.feature.melspectrogram(y=y, sr=sr) 
assert np.array_equal(sol, sol_dict['M_from_y'])
assert sol_dict[""M_from_y""].dtype == np.float32
sol = sol.astype(np.float32)
assert np.array_equal(sol, sol_dict['sol'])
assert sol_dict['sol'].dtype == np.float32","Extract melspectrogram from waveform y. After it is computed, ensure it is of type np.float32.",,"sol_dict[""M_from_y""] = librosa.feature.melspectrogram(y=y, sr=sr) 
sol_dict['sol'] = sol_dict[""M_from_y""]",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.7.0,python==3.7,,librosa.feature.melspectrogram,2019-07,behaviour
"import librosa
import numpy as np
import soundfile as sf 

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)

n_fft = 4096
hop_length = n_fft // 2

# Save the stream in sol_dict['stream']. Save each stream block with the format sol_dict['stream_block_{}']. 
sol_dict = {""stream"":None}
for i in range(0, 83):
 sol_dict[""stream_block_{}"".format(i)] = None",librosa,"sol_stream = sf.blocks(filename, blocksize=n_fft + 15 * hop_length,
 overlap=n_fft - hop_length,
 fill_value=0)
solution_dict = {'stream':sol_stream}
for c, block in enumerate(solution_dict['stream']):
 y = librosa.to_mono(block.T)
 D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,
 center=False)
 solution_dict['stream_block_{}'.format(c)] = D
assert len(sol_dict) == len(solution_dict)",Iterate over an audio file using a stream and calculate the STFT on each mono channel.,,"sol_dict['stream'] = sf.blocks(filename, blocksize=n_fft + 15 * hop_length,
 overlap=n_fft - hop_length,
 fill_value=0)
for c, block in enumerate(sol_dict['stream']):
 y = librosa.to_mono(block.T)
 D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,
 center=False)
 sol_dict['stream_block_{}'.format(c)] = D",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,soundfile.blocks,2018-02,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)

n_fft = 4096
hop_length = n_fft // 2

# Save the stream in sol_dict['stream']. Save each stream block with the format sol_dict['stream_block_{}']. 
sol_dict = {""stream"":None}
for i in range(0, 83):
    sol_dict[""stream_block_{}"".format(i)] = None",librosa,"
sol_stream =  librosa.stream(filename, block_length=16,
                        frame_length=n_fft,
                        hop_length=hop_length,
                        mono=True,
                        fill_value=0)
for c, y_block in enumerate(sol_stream):
    assert  np.array_equal(librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length,
                     center=False), sol_dict[""stream_block_{}"".format(c)])",Iterate over an audio file using a stream and calculate the STFT on each mono channel. Frame_length is given by n_fft. Save the stream in sol_dict['stream']. Save each stream block with the format sol_dict['stream_block_{}']. ,,"
sol_dict['stream'] =  librosa.stream(filename, block_length=16,
                        frame_length=n_fft,
                        hop_length=hop_length,
                        mono=True,
                        fill_value=0)
for c, y_block in enumerate(sol_dict['stream']):
    sol_dict['stream_block_{}'.format(c)] = librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length,
                     center=False)",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,0.7.0,python==3.7,,librosa.stream,2019-07,new feature
"import librosa
import numpy as np
from librosa import istft, stft

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
momentum = 0.99
S = np.abs(librosa.stft(y))
random_state = 0
rng = np.random.RandomState(seed=random_state)
n_iter=32
hop_length=None
win_length=None
window='hann'
center=True
dtype=np.float32
length=None
pad_mode='reflect'

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"rng = np.random.RandomState(seed=random_state)
n_fft = 2 * (S.shape[0] - 1)

angles = np.exp(2j * np.pi * rng.rand(*S.shape))

rebuilt = 0.

for _ in range(n_iter):
 tprev = rebuilt

 inverse = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)

 rebuilt = stft(inverse, n_fft=n_fft, hop_length=hop_length,
 win_length=win_length, window=window, center=center,
 pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

sol = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
assert np.allclose(sol, sol_dict['sol'])",Compute an approximate magnitude spectrogram inversion using the Griffin-Lim algorithm.,,"n_fft = 2 * (S.shape[0] - 1)

angles = np.exp(2j * np.pi * rng.rand(*S.shape))

rebuilt = 0.

for _ in range(n_iter):
 tprev = rebuilt

 inverse = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)

 rebuilt = stft(inverse, n_fft=n_fft, hop_length=hop_length,
 win_length=win_length, window=window, center=center,
 pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

sol_dict['sol'] = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.griffinlim,2018-02,new feature
"import librosa
import numpy as np
from librosa import istft, stft

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
momentum = 0.99
S = np.abs(librosa.stft(y))
random_state = 0
rng = np.random.RandomState(seed=random_state)
n_iter=32
hop_length=None
win_length=None
window='hann'
center=True
dtype=np.float32
length=None
pad_mode='reflect'

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"rng = np.random.RandomState(seed=random_state)
sol = librosa.griffinlim(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, random_state)
assert np.allclose(sol, sol_dict['sol'])",Compute an approximate magnitude spectrogram inversion using the Griffin-Lim algorithm.,,"sol_dict['sol'] = librosa.griffinlim(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, random_state)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.griffinlim,2019-07,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
order=2

# Store the solution in sol_dict['lpc_coeff'].
sol_dict = {'lpc_coeff': None}",librosa,"
dtype = y.dtype.type
ar_coeffs = np.zeros(order+1, dtype=dtype)
ar_coeffs[0] = dtype(1)
ar_coeffs_prev = np.zeros(order+1, dtype=dtype)
ar_coeffs_prev[0] = dtype(1)
fwd_pred_error = y[1:]
bwd_pred_error = y[:-1]
den = np.dot(fwd_pred_error, fwd_pred_error) \
      + np.dot(bwd_pred_error, bwd_pred_error)
for i in range(order):
    if den <= 0:
        raise FloatingPointError('numerical error, input ill-conditioned?')
    reflect_coeff = dtype(-2) * np.dot(bwd_pred_error, fwd_pred_error) / dtype(den)
    ar_coeffs_prev, ar_coeffs = ar_coeffs, ar_coeffs_prev
    for j in range(1, i + 2):
        ar_coeffs[j] = ar_coeffs_prev[j] + reflect_coeff * ar_coeffs_prev[i - j + 1]
    fwd_pred_error_tmp = fwd_pred_error
    fwd_pred_error = fwd_pred_error + reflect_coeff * bwd_pred_error
    bwd_pred_error = bwd_pred_error + reflect_coeff * fwd_pred_error_tmp
    q = dtype(1) - reflect_coeff**2
    den = q*den - bwd_pred_error[-1]**2 - fwd_pred_error[0]**2
    fwd_pred_error = fwd_pred_error[1:]
    bwd_pred_error = bwd_pred_error[:-1]

sol = ar_coeffs
assert np.array_equal(sol, sol_dict['lpc_coeff'])",Compute lp coefficents of input array y.,,"
dtype = y.dtype.type
ar_coeffs = np.zeros(order+1, dtype=dtype)
ar_coeffs[0] = dtype(1)
ar_coeffs_prev = np.zeros(order+1, dtype=dtype)
ar_coeffs_prev[0] = dtype(1)
fwd_pred_error = y[1:]
bwd_pred_error = y[:-1]
den = np.dot(fwd_pred_error, fwd_pred_error) \
      + np.dot(bwd_pred_error, bwd_pred_error)
for i in range(order):
    if den <= 0:
        raise FloatingPointError('numerical error, input ill-conditioned?')
    reflect_coeff = dtype(-2) * np.dot(bwd_pred_error, fwd_pred_error) / dtype(den)
    ar_coeffs_prev, ar_coeffs = ar_coeffs, ar_coeffs_prev
    for j in range(1, i + 2):
        ar_coeffs[j] = ar_coeffs_prev[j] + reflect_coeff * ar_coeffs_prev[i - j + 1]
    fwd_pred_error_tmp = fwd_pred_error
    fwd_pred_error = fwd_pred_error + reflect_coeff * bwd_pred_error
    bwd_pred_error = bwd_pred_error + reflect_coeff * fwd_pred_error_tmp
    q = dtype(1) - reflect_coeff**2
    den = q*den - bwd_pred_error[-1]**2 - fwd_pred_error[0]**2
    fwd_pred_error = fwd_pred_error[1:]
    bwd_pred_error = bwd_pred_error[:-1]
sol_dict['lpc_coeff'] = ar_coeffs",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.lpc,2018-02,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
order=2

# Store the solution in sol_dict['lpc_coeff'].
sol_dict = {'lpc_coeff': None}",librosa,"sol = librosa.lpc(y, order)
assert np.array_equal(sol, sol_dict['lpc_coeff'])",Compute lp coefficents of input array y.,,"sol_dict['lpc_coeff'] = librosa.lpc(y, order)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.lpc,2019-07,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

# Store the solution in sol_dict['f_temp'].
sol_dict = {'f_temp': None}",librosa,"from librosa.core.spectrum import stft
sol = stft(oenv, n_fft=384, hop_length=1, center=True, window=""hann"")
assert np.array_equal(sol, sol_dict['f_temp'])",Compute local onset autocorrelation in order to create a fourier tempogram.,,"from librosa.core.spectrum import stft

sol_dict['f_temp'] = stft(oenv, n_fft=384, hop_length=1, center=True, window=""hann"")",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.feature.fourier_tempogram,2018-02,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

# Store the solution in sol_dict['f_temp'].
sol_dict = {'f_temp': None}",librosa,"sol = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)
assert np.array_equal(sol, sol_dict['f_temp'])",Compute local onset autocorrelation using fourier_tempogram.,,"sol_dict['f_temp'] = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.feature.fourier_tempogram,2019-07,new feature
"import librosa
import numpy as np


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
win_length=384
tempo_min = None
tempo_max = None
onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

# Store the solution in sol_dict['plp'].
sol_dict = {'plp': None}",librosa,"from librosa.core.spectrum import stft, istft
ftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=""hann"")
 
tempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))

ftmag = np.abs(ftgram)
peak_values = ftmag.max(axis=0, keepdims=True)
ftgram[ftmag < peak_values] = 0

ftgram[:] /= peak_values

pulse = istft(ftgram, hop_length=1, length=len(onset_env))

np.clip(pulse, 0, None, pulse)
sol = librosa.util.normalize(pulse)
assert np.array_equal(sol, sol_dict['plp'])",Compute the predominant local pulse (PLP) estimation of y.,,"from librosa.core.spectrum import stft, istft
ftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=""hann"")
 
tempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))

ftmag = np.abs(ftgram)
peak_values = ftmag.max(axis=0, keepdims=True)
ftgram[ftmag < peak_values] = 0

ftgram[:] /= peak_values

pulse = istft(ftgram, hop_length=1, length=len(onset_env))

np.clip(pulse, 0, None, pulse)
sol_dict['plp'] = librosa.util.normalize(pulse)",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.beat.plp,2018-02,new feature
"import librosa
import numpy as np


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
win_length=384
tempo_min = None
tempo_max = None
onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

# Store the solution in sol_dict['plp'].
sol_dict = {'plp': None}",librosa,"sol = librosa.beat.plp(onset_envelope=onset_env, sr=sr, tempo_min=tempo_min, tempo_max=tempo_max)
assert np.array_equal(sol, sol_dict['plp'])",Compute the predominant local pulse (PLP) estimation of y.,,"sol_dict['plp'] = librosa.beat.plp(onset_envelope=onset_env, sr=sr, tempo_min=tempo_min, tempo_max=tempo_max)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.beat.plp,2019-07,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
samples = (np.asanyarray(frames) * hop_length + offset).astype(int)

sol = np.asanyarray(samples) / float(sr)
assert np.array_equal(sol, sol_dict['sol'])",Return an array of time values to match the time axis from a feature matrix.,,"if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
samples = (np.asanyarray(frames) * hop_length + offset).astype(int)

sol_dict['sol'] = np.asanyarray(samples) / float(sr)",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.times_like,2018-02,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"sol = librosa.times_like(D, sr=sr)
assert np.array_equal(sol, sol_dict['sol'])",Return an array of time values to match the time axis from a feature matrix.,,"sol_dict['sol'] = librosa.times_like(D, sr=sr)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.times_like,2019-07,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
sol = (np.asanyarray(frames) * hop_length + offset).astype(int)

assert np.array_equal(sol, sol_dict['sol'])",Return an array of sample indices to match the time axis from a feature matrix.,,"if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
sol_dict['sol'] = (np.asanyarray(frames) * hop_length + offset).astype(int)",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.samples_like,2018-02,new feature
"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"sol = librosa.samples_like(D)
assert np.array_equal(sol, sol_dict['sol'])",Return an array of sample indices to match the time axis from a feature matrix.,,sol_dict['sol'] = librosa.samples_like(D),pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.samples_like,2019-07,new feature
"import librosa
import numpy as np

frequency = 440
sr = 22050
length = sr

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"phi = -np.pi * 0.5
sol = np.cos(2 * np.pi * frequency * np.arange(length) / sr + phi)
assert np.array_equal(sol, sol_dict['sol'])",Construct a pure tone (cosine) signal at a given frequency.,,"phi = -np.pi * 0.5
sol_dict['sol'] = np.cos(2 * np.pi * frequency * np.arange(length) / sr + phi)",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.tone,2018-02,new feature
"import librosa
import numpy as np

frequency = 440
sr = 22050
length = sr

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"sol = librosa.tone(frequency, sr=sr, length=length)
assert np.array_equal(sol, sol_dict['sol'])",Construct a pure tone (cosine) signal at a given frequency.,,"sol_dict['sol'] = librosa.tone(frequency, sr=sr, length=length)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.tone,2019-07,new feature
"import librosa
import numpy as np


fmin = 110
fmax = 110*64
duration = 1
sr = 22050
linear = True


# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"import scipy

period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" if linear else ""logarithmic""
sol = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
assert np.array_equal(sol, sol_dict['sol'])",Construct a “chirp” or “sine-sweep” signal. The chirp sweeps from frequency fmin to fmax (in Hz).,,"import scipy
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" if linear else ""logarithmic""
sol_dict['sol'] = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)",pip==24.1 scikit-learn==0.21.0 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.chirp,2018-02,new feature
"import librosa
import numpy as np

fmin = 110
fmax = 110*64
duration = 1
sr = 22050

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",librosa,"sol = librosa.chirp(fmin=fmin, fmax=fmax, duration=duration, sr=sr)
assert np.array_equal(sol, sol_dict['sol'])",Construct a “chirp” or “sine-sweep” signal. The chirp sweeps from frequency fmin to fmax (in Hz). ,,"sol_dict['sol'] = librosa.chirp(fmin=fmin, fmax=fmax, duration=duration, sr=sr)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.chirp,2019-07,new feature
"import librosa
import numpy as np

E = np.eye(3)
factor=-1
axis=-1

# Store the solution in sol_dict['sol'].
sol_dict = {'sol':None}",librosa,"gt = np.array([[1., 1., 1.],
 [0., 0., 0.],
 [0., 0., 0.]])
assert np.array_equal(gt, sol_dict['sol'])",Shear a matrix by a given factor. ,,"E_shear = np.empty_like(E)
for i in range(E.shape[1]):
 E_shear[:, i] = np.roll(E[:, i], factor * i)
sol_dict['sol'] = E_shear",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.util.shear,2018-02,new feature
"import librosa
import numpy as np

E = np.eye(3)
factor=-1
axis=-1

# Store the solution in sol_dict['sol'].
sol_dict = {'sol':None}",librosa,"gt = np.array([[1., 1., 1.],
 [0., 0., 0.],
 [0., 0., 0.]])
assert np.array_equal(gt, sol_dict['sol'])",Shear a matrix by a given factor.,,"sol_dict['sol'] = librosa.util.shear(E, factor=factor, axis=axis)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.1,python==3.7,,librosa.util.shear,2019-07,new feature
"import librosa
import numpy as np
axis=0
x = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"gt = np.array([[False, False, False],
 [False, True, True],
 [False, False, False]])

assert np.array_equal(gt, sol_dict['sol'])",Locate the local minimums of an array. ,,"sol_dict['sol'] = librosa.util.localmax(-x, axis=0)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.util.localmin,2019-07,new feature
"import librosa
import numpy as np
axis=0
x = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"gt = np.array([[False, False, False],
 [False, True, True],
 [False, False, False]])

assert np.array_equal(gt, sol_dict['sol'])",Locate the local minimums of an array. ,,"sol_dict['sol'] = librosa.util.localmin(x, axis=0)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.8.0,python==3.7,,librosa.util.localmin,2020-07,new feature
"from tqdm import tqdm
def infinite():
 i = 0
 while True:
 yield i
 i += 1
 if i == 1000:
 return

# Define the total in sol_dict['total'] and use it.
sol_dict = {""total"":0}",tqdm,assert sol_dict['total'] is None,Iterate over an infinite iterable.,,"sol_dict['total'] = None
progress_bar = tqdm(infinite(), total=sol_dict['total'])
for progress in progress_bar:
 progress_bar.set_description(f""Processing {progress}"")",,4.28,python==3.7,,tqdm,2018-10,argument change
"from tqdm import tqdm
def infinite():
 i = 0
 while True:
 yield i
 i += 1
 if i == 1000:
 return

# Define the total in sol_dict['total'] and use it.
sol_dict = {""total"":0}",tqdm,assert sol_dict['total'] == float('inf'),Iterate over an infinite iterable.,,"sol_dict['total'] = float('inf')
progress_bar = tqdm(infinite(), total=sol_dict['total'])
for progress in progress_bar:
 progress_bar.set_description(f""Processing {progress}"")",,4.29,python==3.7,,tqdm,2019-01,argument change
"import kymatio
import torch
from kymatio import Scattering2D
a = torch.ones((1, 3, 32, 32))

# Put the Scattering object in variable S
# Put the result of Scattering on a in variable S_a",kymatio,"import kymatio
assert isinstance(S_a, torch.Tensor)
assert isinstance(S, kymatio.scattering2d.frontend.torch_frontend.ScatteringTorch2D)",Define and run a 2d scattering transform in Torch.,,"
S = Scattering2D(2, (32, 32), frontend='torch')
S_a = S(a)",torch==1.4.0,0.3.0,python==3.7,,Scattering2D,2022-09,argument change
"import matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots()",matplotlib,"import numpy as np 
assert np.array_equal(ax.get_xticks(), np.array([]))
assert (ax.get_xticks() == np.array([])).all()

assert np.array_equal(ax.get_xticklabels(), np.array([]))
assert (ax.get_xticklabels() == np.array([])).all()",Modify the axis of the figure to not visualize ticks on the x and y axis.,,"ax.set_xticks([], minor=False)
ax.set_yticks([], minor=False)",numpy==1.18.1,3.4.0,python==3.7,,matplotlib.pyplot.axis,2021-03,argument change
"import matplotlib.pyplot as plt
fig, ax = plt.subplots()",matplotlib,"import numpy as np 
assert np.array_equal(ax.get_xticks(), np.array([]))
assert (ax.get_xticks() == np.array([])).all()

assert np.array_equal(ax.get_xticklabels(), np.array([]))
assert (ax.get_xticklabels() == np.array([])).all()","Modify the axis of the figure to not visualize major and minor ticks on the x and y axis, with no labels.",,"ax.set_xticks([], False)
ax.set_yticks([], False)",numpy==1.18.1,3.2.0,python==3.7,,matplotlib.pyplot.axis,2020-03,argument change
"import matplotlib.pyplot as plt
fig, ax = plt.subplots()",matplotlib,"import numpy as np 
assert np.array_equal(ax.get_xticks(), np.array([]))
assert (ax.get_xticks() == np.array([])).all()

assert np.array_equal(ax.get_xticklabels(), np.array([]))
assert (ax.get_xticklabels() == np.array([])).all()","Modify the axis of the figure to not visualize major and minor ticks on the x and y axis, with no labels.",,"ax.set_xticks([], [], minor=False)
ax.set_yticks([], [], minor=False)",numpy==1.18.1,3.5.0,python==3.7,,matplotlib.pyplot.axis,2021-11,argument change
import matplotlib.pyplot as plt,matplotlib,"cycle = plt.rcParams['axes.prop_cycle']
from cycler import cycler
a = cycler('color', ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD'])
assert cycle==a",Use Seaborn style.,,"plt.style.use(""seaborn"")",numpy==1.18.1,3.5.0,python==3.7,,matplotlib.pyplot.style.use,2021-11,argument change
"import matplotlib.pyplot as  plt
",matplotlib,"cycle = plt.rcParams['axes.prop_cycle']
from cycler import cycler
a = cycler('color', ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD'])
assert cycle==a",Use Seaborn style.,,"plt.style.use(""seaborn-v0_8"")
",numpy==1.18.1,3.8.0,python==3.10,,matplotlib.pyplot.style.use,2023-09,argument change
"import librosa
import numpy as np
import scipy

sr=22050
fmin = 440
fmax = 880
duration = 5.0
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" 
y = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
frame_length = 2048
center = True
pad_mode = 'reflect'
win_length = None
hop_length = None
trough_threshold = 0.1

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"sr=22050
fmin = 440
fmax = 880
duration = 5.0
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" 
y = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
frame_length = 2048
center = True
pad_mode = 'reflect'
win_length = None
hop_length = None
trough_threshold = 0.1


# Set the default window length if it is not already specified.
if win_length is None:
 win_length = frame_length // 2


# Set the default hop if it is not already specified.
if hop_length is None:
 hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
 y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))

parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0

# Find local minima.
is_trough = librosa.util.localmax(-yin_frames, axis=0)
is_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]

# Find minima below peak threshold.
is_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)

# Absolute threshold.
# ""The solution we propose is to set an absolute threshold and choose the
# smallest value of tau that gives a minimum of d' deeper than
# this threshold. If none is found, the global minimum is chosen instead.""
global_min = np.argmin(yin_frames, axis=0)
yin_period = np.argmax(is_threshold_trough, axis=0)
no_trough_below_threshold = np.all(~is_threshold_trough, axis=0)
yin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]

# Refine peak by parabolic interpolation.
yin_period = (
 min_period
 + yin_period
 + parabolic_shifts[yin_period, range(yin_frames.shape[1])]
)

# Convert period to fundamental frequency.
sol = sr / yin_period
assert np.allclose(sol, sol_dict['sol'])",Calculate the fundamental frequency (F0) estimation using the YIN algorithm.,,"# Set the default window length if it is not already specified.
if win_length is None:
 win_length = frame_length // 2


# Set the default hop if it is not already specified.
if hop_length is None:
 hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
 y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))

parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0

# Find local minima.
is_trough = librosa.util.localmax(-yin_frames, axis=0)
is_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]

# Find minima below peak threshold.
is_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)

# Absolute threshold.
# ""The solution we propose is to set an absolute threshold and choose the
# smallest value of tau that gives a minimum of d' deeper than
# this threshold. If none is found, the global minimum is chosen instead.""
global_min = np.argmin(yin_frames, axis=0)
yin_period = np.argmax(is_threshold_trough, axis=0)
no_trough_below_threshold = np.all(~is_threshold_trough, axis=0)
yin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]

# Refine peak by parabolic interpolation.
yin_period = (
 min_period
 + yin_period
 + parabolic_shifts[yin_period, range(yin_frames.shape[1])]
)

# Convert period to fundamental frequency.
sol_dict['sol'] = sr / yin_period",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.yin,2019-07,new feature
"import librosa
import numpy as np
import scipy

sr=22050
fmin = 440
fmax = 880
duration = 5.0
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" 
y = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
frame_length = 2048
center = True
pad_mode = 'reflect'
win_length = None
hop_length = None
trough_threshold = 0.1

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"sol = librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)
assert np.allclose(sol, sol_dict['sol'])",Calculate the fundamental frequency (F0) estimation using the YIN algorithm.,,"sol_dict['sol'] = librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.8.0,python==3.7,,librosa.yin,2020-07,new feature
"import librosa
import numpy as np
import scipy


freq=110
sr=22050
y = librosa.tone(freq, duration=1.0)
fmin = 110
fmax = 880
frame_length = 2048
center = False
pad_mode = 'reflect'
win_length = None
hop_length = None
#trough_threshold = 0.1

n_thresholds=100
beta_parameters=(2, 18)
boltzmann_parameter=2
resolution=0.1
max_transition_rate=35.92
switch_prob=0.01
no_trough_prob=0.01
fill_na=np.nan

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"
freq=110
sr=22050
y = librosa.tone(freq, duration=1.0)
fmin = 110
fmax = 880
frame_length = 2048
center = False
pad_mode = 'reflect'
win_length = None
hop_length = None
#trough_threshold = 0.1

n_thresholds=100
beta_parameters=(2, 18)
boltzmann_parameter=2
resolution=0.1
max_transition_rate=35.92
switch_prob=0.01
no_trough_prob=0.01
fill_na=np.nan


# Set the default window length if it is not already specified.
if win_length is None:
    win_length = frame_length // 2


# Set the default hop if it is not already specified.
if hop_length is None:
    hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
    y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))



parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0



# Find Yin candidates and probabilities.
# The implementation here follows the official pYIN software which
# differs from the method described in the paper.
# 1. Define the prior over the thresholds.
thresholds = np.linspace(0, 1, n_thresholds + 1)
beta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])
beta_probs = np.diff(beta_cdf)

yin_probs = np.zeros_like(yin_frames)
for i, yin_frame in enumerate(yin_frames.T):
    # 2. For each frame find the troughs.
    is_trough = librosa.util.localmax(-yin_frame, axis=0)
    is_trough[0] = yin_frame[0] < yin_frame[1]
    (trough_index,) = np.nonzero(is_trough)

    if len(trough_index) == 0:
        continue

    # 3. Find the troughs below each threshold.
    trough_heights = yin_frame[trough_index]
    trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]

    # 4. Define the prior over the troughs.
    # Smaller periods are weighted more.
    trough_positions = np.cumsum(trough_thresholds, axis=0) - 1
    n_troughs = np.count_nonzero(trough_thresholds, axis=0)
    trough_prior = scipy.stats.boltzmann.pmf(
        trough_positions, boltzmann_parameter, n_troughs
    )
    trough_prior[~trough_thresholds] = 0

    # 5. For each threshold add probability to global minimum if no trough is below threshold,
    # else add probability to each trough below threshold biased by prior.
    probs = np.sum(trough_prior * beta_probs, axis=1)
    global_min = np.argmin(trough_heights)
    n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])
    probs[global_min] += no_trough_prob * np.sum(
        beta_probs[:n_thresholds_below_min]
    )

    yin_probs[trough_index, i] = probs

yin_period, frame_index = np.nonzero(yin_probs)

# Refine peak by parabolic interpolation.
period_candidates = min_period + yin_period
period_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]
f0_candidates = sr / period_candidates

n_bins_per_semitone = int(np.ceil(1.0 / resolution))
n_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1

# Construct transition matrix.
max_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)
transition_width = max_semitones_per_frame * n_bins_per_semitone + 1
# Construct the within voicing transition probabilities
transition = librosa.sequence.transition_local(
    n_pitch_bins, transition_width, window=""triangle"", wrap=False
)
# Include across voicing transition probabilities
transition = np.block(
    [
        [(1 - switch_prob) * transition, switch_prob * transition],
        [switch_prob * transition, (1 - switch_prob) * transition],
    ]
)

# Find pitch bin corresponding to each f0 candidate.
bin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)
bin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)

# Observation probabilities.
observation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))
observation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]
voiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)
observation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins

p_init = np.zeros(2 * n_pitch_bins)
p_init[n_pitch_bins:] = 1 / n_pitch_bins

# Viterbi decoding.
states = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)

# Find f0 corresponding to each decoded pitch bin.
freqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))
f0 = freqs[states % n_pitch_bins]
voiced_flag = states < n_pitch_bins
if fill_na is not None:
    f0[~voiced_flag] = fill_na
sol = f0
assert np.allclose(sol, sol_dict['sol'])
assert np.allclose(np.log2(sol_dict['sol']), np.log2(freq), rtol=0, atol=1e-2)",Calculate the fundamental frequency estimation using probabilistic YIN.,,"
# Set the default window length if it is not already specified.
if win_length is None:
    win_length = frame_length // 2


# Set the default hop if it is not already specified.
if hop_length is None:
    hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
    y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))



parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0



# Find Yin candidates and probabilities.
# The implementation here follows the official pYIN software which
# differs from the method described in the paper.
# 1. Define the prior over the thresholds.
thresholds = np.linspace(0, 1, n_thresholds + 1)
beta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])
beta_probs = np.diff(beta_cdf)

yin_probs = np.zeros_like(yin_frames)
for i, yin_frame in enumerate(yin_frames.T):
    # 2. For each frame find the troughs.
    is_trough = librosa.util.localmax(-yin_frame, axis=0)
    is_trough[0] = yin_frame[0] < yin_frame[1]
    (trough_index,) = np.nonzero(is_trough)

    if len(trough_index) == 0:
        continue

    # 3. Find the troughs below each threshold.
    trough_heights = yin_frame[trough_index]
    trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]

    # 4. Define the prior over the troughs.
    # Smaller periods are weighted more.
    trough_positions = np.cumsum(trough_thresholds, axis=0) - 1
    n_troughs = np.count_nonzero(trough_thresholds, axis=0)
    trough_prior = scipy.stats.boltzmann.pmf(
        trough_positions, boltzmann_parameter, n_troughs
    )
    trough_prior[~trough_thresholds] = 0

    # 5. For each threshold add probability to global minimum if no trough is below threshold,
    # else add probability to each trough below threshold biased by prior.
    probs = np.sum(trough_prior * beta_probs, axis=1)
    global_min = np.argmin(trough_heights)
    n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])
    probs[global_min] += no_trough_prob * np.sum(
        beta_probs[:n_thresholds_below_min]
    )

    yin_probs[trough_index, i] = probs

yin_period, frame_index = np.nonzero(yin_probs)

# Refine peak by parabolic interpolation.
period_candidates = min_period + yin_period
period_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]
f0_candidates = sr / period_candidates

n_bins_per_semitone = int(np.ceil(1.0 / resolution))
n_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1

# Construct transition matrix.
max_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)
transition_width = max_semitones_per_frame * n_bins_per_semitone + 1
# Construct the within voicing transition probabilities
transition = librosa.sequence.transition_local(
    n_pitch_bins, transition_width, window=""triangle"", wrap=False
)
# Include across voicing transition probabilities
transition = np.block(
    [
        [(1 - switch_prob) * transition, switch_prob * transition],
        [switch_prob * transition, (1 - switch_prob) * transition],
    ]
)

# Find pitch bin corresponding to each f0 candidate.
bin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)
bin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)

# Observation probabilities.
observation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))
observation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]
voiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)
observation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins

p_init = np.zeros(2 * n_pitch_bins)
p_init[n_pitch_bins:] = 1 / n_pitch_bins

# Viterbi decoding.
states = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)

# Find f0 corresponding to each decoded pitch bin.
freqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))
f0 = freqs[states % n_pitch_bins]
voiced_flag = states < n_pitch_bins
if fill_na is not None:
    f0[~voiced_flag] = fill_na

sol_dict['sol'] = f0",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.pyin,2019-07,new feature
"import librosa
import numpy as np
import scipy

freq=110
fmin=freq
fmax=880
center=False
y = librosa.tone(freq, duration=1.0)


# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"np.random.seed(0)
sol = librosa.pyin(y, fmin=fmin, fmax=fmax, center=center)[0]
assert np.allclose(sol, sol_dict['sol'])
assert np.allclose(np.log2(sol_dict['sol']), np.log2(freq), rtol=0, atol=1e-2)",Calculate the fundamental frequency estimation using probabilistic YIN.,,"sol_dict['sol'] = librosa.pyin(y, fmin=fmin, fmax=fmax, center=center)[0]",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.8.0,python==3.7,,librosa.pyin,2020-07,new feature
"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
fmin=None
n_bins=84
gamma=None
bins_per_octave=12
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=None
dtype=None

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"
filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
fmin=None
n_bins=84
gamma=None
bins_per_octave=12
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=None
dtype=None


# How many octaves are we dealing with?
def dtype_r2c(d, default=np.complex64):
    """"""Find the complex numpy dtype corresponding to a real dtype.

    This is used to maintain numerical precision and memory footprint
    when constructing complex arrays from real-valued data
    (e.g. in a Fourier transform).

    A `float32` (single-precision) type maps to `complex64`,
    while a `float64` (double-precision) maps to `complex128`.


    Parameters
    ----------
    d : np.dtype
        The real-valued dtype to convert to complex.
        If ``d`` is a complex type already, it will be returned.

    default : np.dtype, optional
        The default complex target type, if ``d`` does not match a
        known dtype

    Returns
    -------
    d_c : np.dtype
        The complex dtype

    See Also
    --------
    dtype_c2r
    numpy.dtype

    """"""
    mapping = {
        np.dtype(np.float32): np.complex64,
        np.dtype(np.float64): np.complex128,
        np.dtype(np.float): np.complex,
    }

    # If we're given a complex type already, return it
    dt = np.dtype(d)
    if dt.kind == ""c"":
        return dt

    # Otherwise, try to map the dtype.
    # If no match is found, return the default.
    return np.dtype(mapping.get(dt, default))

n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))
n_filters = min(bins_per_octave, n_bins)

len_orig = len(y)

# Relative difference in frequency between any two consecutive bands
alpha = 2.0 ** (1.0 / bins_per_octave) - 1

if fmin is None:
    # C1 by default
    fmin = librosa.note_to_hz(""C1"")

if tuning is None:
    tuning = librosa.pitch.estimate_tuning(y=y, sr=sr, bins_per_octave=bins_per_octave)

if gamma is None:
    gamma = 24.7 * alpha / 0.108

if dtype is None:
    dtype = dtype_r2c(y.dtype)

# Apply tuning correction
fmin = fmin * 2.0 ** (tuning / bins_per_octave)

# First thing, get the freqs of the top octave
freqs = librosa.time_frequency.cqt_frequencies(n_bins, fmin, bins_per_octave=bins_per_octave)[
    -bins_per_octave:
]

fmin_t = np.min(freqs)
fmax_t = np.max(freqs)

# Determine required resampling quality
Q = float(filter_scale) / alpha
filter_cutoff = (
    fmax_t * (1 + 0.5 * librosa.filters.window_bandwidth(window) / Q) + 0.5 * gamma
)
nyquist = sr / 2.0

auto_resample = False
if not res_type:
    auto_resample = True
    if filter_cutoff < librosa.audio.BW_FASTEST * nyquist:
        res_type = ""kaiser_fast""
    else:
        res_type = ""kaiser_best""

downsample_count1 = max(
    0, int(np.ceil(np.log2(librosa.audio.BW_FASTEST * nyquist / filter_cutoff)) - 1) - 1
)

def num_two_factors(x):
    if x <= 0:
        return 0
    num_twos = 0
    while x % 2 == 0:
        num_twos += 1
        x //= 2

    return num_twos
num_twos=num_two_factors(hop_length)
downsample_count2 = max(0, num_twos - n_octaves + 1)
downsample_count = min(downsample_count1, downsample_count2)


vqt_resp = []

# Make sure our hop is long enough to support the bottom octave

num_twos=num_two_factors(hop_length)


#num_twos = __num_two_factors(hop_length)
if num_twos < n_octaves - 1:
    raise ParameterError(
        ""hop_length must be a positive integer ""
        ""multiple of 2^{0:d} for {1:d}-octave CQT/VQT"".format(
            n_octaves - 1, n_octaves
        )
    )

# Now do the recursive bit
my_y, my_sr, my_hop = y, sr, hop_length
def sparsify_rows(x, quantile=0.01, dtype=None):
    """"""Return a row-sparse matrix approximating the input

    Parameters
    ----------
    x : np.ndarray [ndim <= 2]
        The input matrix to sparsify.

    quantile : float in [0, 1.0)
        Percentage of magnitude to discard in each row of ``x``

    dtype : np.dtype, optional
        The dtype of the output array.
        If not provided, then ``x.dtype`` will be used.

    Returns
    -------
    x_sparse : ``scipy.sparse.csr_matrix`` [shape=x.shape]
        Row-sparsified approximation of ``x``

        If ``x.ndim == 1``, then ``x`` is interpreted as a row vector,
        and ``x_sparse.shape == (1, len(x))``.

    Raises
    ------
    ParameterError
        If ``x.ndim > 2``

        If ``quantile`` lies outside ``[0, 1.0)``
    """"""

    if x.ndim == 1:
        x = x.reshape((1, -1))

    elif x.ndim > 2:
        raise ParameterError(
            ""Input must have 2 or fewer dimensions. ""
            ""Provided x.shape={}."".format(x.shape)
        )

    if not 0.0 <= quantile < 1:
        raise ParameterError(""Invalid quantile {:.2f}"".format(quantile))

    if dtype is None:
        dtype = x.dtype

    x_sparse = scipy.sparse.lil_matrix(x.shape, dtype=dtype)

    mags = np.abs(x)
    norms = np.sum(mags, axis=1, keepdims=True)

    mag_sort = np.sort(mags, axis=1)
    cumulative_mag = np.cumsum(mag_sort / norms, axis=1)

    threshold_idx = np.argmin(cumulative_mag < quantile, axis=1)

    for i, j in enumerate(threshold_idx):
        idx = np.where(mags[i] >= mag_sort[i, j])
        x_sparse[i, idx] = x[i, idx]

    return x_sparse.tocsr()

def cqt_filter_fft(
    sr,
    fmin,
    n_bins,
    bins_per_octave,
    filter_scale,
    norm,
    sparsity,
    hop_length=None,
    window=""hann"",
    gamma=0.0,
    dtype=np.complex,
):
    """"""Generate the frequency domain constant-Q filter basis.""""""

    basis, lengths = librosa.filters.constant_q(
        sr,
        fmin=fmin,
        n_bins=n_bins,
        bins_per_octave=bins_per_octave,
        filter_scale=filter_scale,
        norm=norm,
        pad_fft=True,
        window=window,
    )

    # Filters are padded up to the nearest integral power of 2
    n_fft = basis.shape[1]

    if hop_length is not None and n_fft < 2.0 ** (1 + np.ceil(np.log2(hop_length))):

        n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))

    # re-normalize bases with respect to the FFT window length
    basis *= lengths[:, np.newaxis] / float(n_fft)

    # FFT and retain only the non-negative frequencies
    fft = librosa.get_fftlib()
    fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, : (n_fft // 2) + 1]

    # sparsify the basis
    fft_basis = sparsify_rows(fft_basis, quantile=sparsity, dtype=dtype)

    return fft_basis, n_fft, lengths


def cqt_response(y, n_fft, hop_length, fft_basis, mode, dtype=None):
    """"""Compute the filter response with a target STFT hop.""""""

    # Compute the STFT matrix
    D = librosa.stft(
        y, n_fft=n_fft, hop_length=hop_length, window=""ones"", pad_mode=mode, dtype=dtype
    )

    # And filter response energy
    return fft_basis.dot(D)

# Iterate down the octaves
for i in range(n_octaves):
    # Resample (except first time)
    if i > 0:
        if len(my_y) < 2:
            raise ParameterError(
                ""Input signal length={} is too short for ""
                ""{:d}-octave CQT/VQT"".format(len_orig, n_octaves)
            )

        my_y = librosa.audio.resample(my_y, 2, 1, res_type=res_type, scale=True)

        my_sr /= 2.0
        my_hop //= 2

    fft_basis, n_fft, _ = cqt_filter_fft(
        my_sr,
        fmin_t * 2.0 ** -i,
        n_filters,
        bins_per_octave,
        filter_scale,
        norm,
        sparsity,
        window=window,
        gamma=gamma,
        dtype=dtype,
    )

    # Re-scale the filters to compensate for downsampling
    fft_basis[:] *= np.sqrt(2 ** i)

    # Compute the vqt filter response and append to the stack
    vqt_resp.append(
        cqt_response(my_y, n_fft, my_hop, fft_basis, pad_mode, dtype=dtype)
    )

def trim_stack(cqt_resp, n_bins, dtype):
    """"""Helper function to trim and stack a collection of CQT responses""""""

    max_col = min(c_i.shape[-1] for c_i in cqt_resp)
    cqt_out = np.empty((n_bins, max_col), dtype=dtype, order=""F"")

    # Copy per-octave data into output array
    end = n_bins
    for c_i in cqt_resp:
        # By default, take the whole octave
        n_oct = c_i.shape[0]
        # If the whole octave is more than we can fit,
        # take the highest bins from c_i
        if end < n_oct:
            cqt_out[:end] = c_i[-end:, :max_col]
        else:
            cqt_out[end - n_oct : end] = c_i[:, :max_col]

        end -= n_oct

    return cqt_out

V = trim_stack(vqt_resp, n_bins, dtype)

if scale:
    lengths = librosa.filters.constant_q_lengths(
        sr,
        fmin,
        n_bins=n_bins,
        bins_per_octave=bins_per_octave,
        window=window,
        filter_scale=filter_scale,
    )
    V /= np.sqrt(lengths[:, np.newaxis])

sol = V
assert np.allclose(sol, sol_dict['sol'])",Compute the variable-Q transform of an audio signal.,,"
# How many octaves are we dealing with?
def dtype_r2c(d, default=np.complex64):
    """"""Find the complex numpy dtype corresponding to a real dtype.

    This is used to maintain numerical precision and memory footprint
    when constructing complex arrays from real-valued data
    (e.g. in a Fourier transform).

    A `float32` (single-precision) type maps to `complex64`,
    while a `float64` (double-precision) maps to `complex128`.


    Parameters
    ----------
    d : np.dtype
        The real-valued dtype to convert to complex.
        If ``d`` is a complex type already, it will be returned.

    default : np.dtype, optional
        The default complex target type, if ``d`` does not match a
        known dtype

    Returns
    -------
    d_c : np.dtype
        The complex dtype

    See Also
    --------
    dtype_c2r
    numpy.dtype

    """"""
    mapping = {
        np.dtype(np.float32): np.complex64,
        np.dtype(np.float64): np.complex128,
        np.dtype(np.float): np.complex,
    }

    # If we're given a complex type already, return it
    dt = np.dtype(d)
    if dt.kind == ""c"":
        return dt

    # Otherwise, try to map the dtype.
    # If no match is found, return the default.
    return np.dtype(mapping.get(dt, default))

n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))
n_filters = min(bins_per_octave, n_bins)

len_orig = len(y)

# Relative difference in frequency between any two consecutive bands
alpha = 2.0 ** (1.0 / bins_per_octave) - 1

if fmin is None:
    # C1 by default
    fmin = librosa.note_to_hz(""C1"")

if tuning is None:
    tuning = librosa.pitch.estimate_tuning(y=y, sr=sr, bins_per_octave=bins_per_octave)

if gamma is None:
    gamma = 24.7 * alpha / 0.108

if dtype is None:
    dtype = dtype_r2c(y.dtype)

# Apply tuning correction
fmin = fmin * 2.0 ** (tuning / bins_per_octave)

# First thing, get the freqs of the top octave
freqs = librosa.time_frequency.cqt_frequencies(n_bins, fmin, bins_per_octave=bins_per_octave)[
    -bins_per_octave:
]

fmin_t = np.min(freqs)
fmax_t = np.max(freqs)

# Determine required resampling quality
Q = float(filter_scale) / alpha
filter_cutoff = (
    fmax_t * (1 + 0.5 * librosa.filters.window_bandwidth(window) / Q) + 0.5 * gamma
)
nyquist = sr / 2.0

auto_resample = False
if not res_type:
    auto_resample = True
    if filter_cutoff < librosa.audio.BW_FASTEST * nyquist:
        res_type = ""kaiser_fast""
    else:
        res_type = ""kaiser_best""

downsample_count1 = max(
    0, int(np.ceil(np.log2(librosa.audio.BW_FASTEST * nyquist / filter_cutoff)) - 1) - 1
)

def num_two_factors(x):
    if x <= 0:
        return 0
    num_twos = 0
    while x % 2 == 0:
        num_twos += 1
        x //= 2

    return num_twos
num_twos=num_two_factors(hop_length)
downsample_count2 = max(0, num_twos - n_octaves + 1)
downsample_count = min(downsample_count1, downsample_count2)


vqt_resp = []

# Make sure our hop is long enough to support the bottom octave

num_twos=num_two_factors(hop_length)


#num_twos = __num_two_factors(hop_length)
if num_twos < n_octaves - 1:
    raise ParameterError(
        ""hop_length must be a positive integer ""
        ""multiple of 2^{0:d} for {1:d}-octave CQT/VQT"".format(
            n_octaves - 1, n_octaves
        )
    )

# Now do the recursive bit
my_y, my_sr, my_hop = y, sr, hop_length
def sparsify_rows(x, quantile=0.01, dtype=None):
    """"""Return a row-sparse matrix approximating the input

    Parameters
    ----------
    x : np.ndarray [ndim <= 2]
        The input matrix to sparsify.

    quantile : float in [0, 1.0)
        Percentage of magnitude to discard in each row of ``x``

    dtype : np.dtype, optional
        The dtype of the output array.
        If not provided, then ``x.dtype`` will be used.

    Returns
    -------
    x_sparse : ``scipy.sparse.csr_matrix`` [shape=x.shape]
        Row-sparsified approximation of ``x``

        If ``x.ndim == 1``, then ``x`` is interpreted as a row vector,
        and ``x_sparse.shape == (1, len(x))``.

    Raises
    ------
    ParameterError
        If ``x.ndim > 2``

        If ``quantile`` lies outside ``[0, 1.0)``
    """"""

    if x.ndim == 1:
        x = x.reshape((1, -1))

    elif x.ndim > 2:
        raise ParameterError(
            ""Input must have 2 or fewer dimensions. ""
            ""Provided x.shape={}."".format(x.shape)
        )

    if not 0.0 <= quantile < 1:
        raise ParameterError(""Invalid quantile {:.2f}"".format(quantile))

    if dtype is None:
        dtype = x.dtype

    x_sparse = scipy.sparse.lil_matrix(x.shape, dtype=dtype)

    mags = np.abs(x)
    norms = np.sum(mags, axis=1, keepdims=True)

    mag_sort = np.sort(mags, axis=1)
    cumulative_mag = np.cumsum(mag_sort / norms, axis=1)

    threshold_idx = np.argmin(cumulative_mag < quantile, axis=1)

    for i, j in enumerate(threshold_idx):
        idx = np.where(mags[i] >= mag_sort[i, j])
        x_sparse[i, idx] = x[i, idx]

    return x_sparse.tocsr()

def cqt_filter_fft(
    sr,
    fmin,
    n_bins,
    bins_per_octave,
    filter_scale,
    norm,
    sparsity,
    hop_length=None,
    window=""hann"",
    gamma=0.0,
    dtype=np.complex,
):
    """"""Generate the frequency domain constant-Q filter basis.""""""

    basis, lengths = librosa.filters.constant_q(
        sr,
        fmin=fmin,
        n_bins=n_bins,
        bins_per_octave=bins_per_octave,
        filter_scale=filter_scale,
        norm=norm,
        pad_fft=True,
        window=window,
    )

    # Filters are padded up to the nearest integral power of 2
    n_fft = basis.shape[1]

    if hop_length is not None and n_fft < 2.0 ** (1 + np.ceil(np.log2(hop_length))):

        n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))

    # re-normalize bases with respect to the FFT window length
    basis *= lengths[:, np.newaxis] / float(n_fft)

    # FFT and retain only the non-negative frequencies
    fft = librosa.get_fftlib()
    fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, : (n_fft // 2) + 1]

    # sparsify the basis
    fft_basis = sparsify_rows(fft_basis, quantile=sparsity, dtype=dtype)

    return fft_basis, n_fft, lengths


def cqt_response(y, n_fft, hop_length, fft_basis, mode, dtype=None):
    """"""Compute the filter response with a target STFT hop.""""""

    # Compute the STFT matrix
    D = librosa.stft(
        y, n_fft=n_fft, hop_length=hop_length, window=""ones"", pad_mode=mode, dtype=dtype
    )

    # And filter response energy
    return fft_basis.dot(D)

# Iterate down the octaves
for i in range(n_octaves):
    # Resample (except first time)
    if i > 0:
        if len(my_y) < 2:
            raise ParameterError(
                ""Input signal length={} is too short for ""
                ""{:d}-octave CQT/VQT"".format(len_orig, n_octaves)
            )

        my_y = librosa.audio.resample(my_y, 2, 1, res_type=res_type, scale=True)

        my_sr /= 2.0
        my_hop //= 2

    fft_basis, n_fft, _ = cqt_filter_fft(
        my_sr,
        fmin_t * 2.0 ** -i,
        n_filters,
        bins_per_octave,
        filter_scale,
        norm,
        sparsity,
        window=window,
        gamma=gamma,
        dtype=dtype,
    )

    # Re-scale the filters to compensate for downsampling
    fft_basis[:] *= np.sqrt(2 ** i)

    # Compute the vqt filter response and append to the stack
    vqt_resp.append(
        cqt_response(my_y, n_fft, my_hop, fft_basis, pad_mode, dtype=dtype)
    )

def trim_stack(cqt_resp, n_bins, dtype):
    """"""Helper function to trim and stack a collection of CQT responses""""""

    max_col = min(c_i.shape[-1] for c_i in cqt_resp)
    cqt_out = np.empty((n_bins, max_col), dtype=dtype, order=""F"")

    # Copy per-octave data into output array
    end = n_bins
    for c_i in cqt_resp:
        # By default, take the whole octave
        n_oct = c_i.shape[0]
        # If the whole octave is more than we can fit,
        # take the highest bins from c_i
        if end < n_oct:
            cqt_out[:end] = c_i[-end:, :max_col]
        else:
            cqt_out[end - n_oct : end] = c_i[:, :max_col]

        end -= n_oct

    return cqt_out

V = trim_stack(vqt_resp, n_bins, dtype)

if scale:
    lengths = librosa.filters.constant_q_lengths(
        sr,
        fmin,
        n_bins=n_bins,
        bins_per_octave=bins_per_octave,
        window=window,
        filter_scale=filter_scale,
    )
    V /= np.sqrt(lengths[:, np.newaxis])
sol_dict['sol'] = V",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.vqt,2019-07,new feature
"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
fmin=None
n_bins=84
gamma=None
bins_per_octave=12
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=None
dtype=None

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"sol = librosa.vqt(y, sr=sr)
assert np.allclose(sol, sol_dict['sol'])",Compute the variable-Q transform of an audio signal.,,"sol_dict['sol'] = librosa.vqt(y, sr=sr)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.8.0,python==3.7,,librosa.vqt,2020-07,new feature
"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

C = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))
n_iter=32
hop_length=512
fmin=None
bins_per_octave=36
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=""kaiser_fast""
dtype=None
length=None
momentum=0.99
init=None
rng = np.random.RandomState(seed=0)

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

C = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))
n_iter=32
hop_length=512
fmin=None
bins_per_octave=36
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=""kaiser_fast""
dtype=None
length=None
momentum=0.99
init=None
rng = np.random.RandomState(seed=0)

if fmin is None:
 fmin = librosa.note_to_hz(""C1"")

# using complex64 will keep the result to minimal necessary precision
angles = np.empty(C.shape, dtype=np.complex64)
if init == ""random"":
 # randomly initialize the phase
 angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))
elif init is None:
 # Initialize an all ones complex matrix
 angles[:] = 1.0

# And initialize the previous iterate to 0
rebuilt = 0.0

for _ in range(n_iter):
 # Store the previous iterate
 tprev = rebuilt

 # Invert with our current estimate of the phases
 inverse = librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 length=length,
 res_type=res_type,
 )

 # Rebuild the spectrogram
 rebuilt = librosa.constantq.cqt(
 inverse,
 sr=sr,
 bins_per_octave=bins_per_octave,
 n_bins=C.shape[0],
 hop_length=hop_length,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 res_type=res_type,
 )

 # Update our phase estimates
 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

# Return the final phase estimates
sol = librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 tuning=tuning,
 filter_scale=filter_scale,
 fmin=fmin,
 window=window,
 length=length,
 res_type=res_type,

)

assert np.allclose(sol, sol_dict['sol'])",Compute the approximate constant-Q magnitude spectrogram inversion using the “fast” Griffin-Lim algorithm.,,"if fmin is None:
 fmin = librosa.note_to_hz(""C1"")

# using complex64 will keep the result to minimal necessary precision
angles = np.empty(C.shape, dtype=np.complex64)
if init == ""random"":
 # randomly initialize the phase
 angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))
elif init is None:
 # Initialize an all ones complex matrix
 angles[:] = 1.0

# And initialize the previous iterate to 0
rebuilt = 0.0

for _ in range(n_iter):
 # Store the previous iterate
 tprev = rebuilt

 # Invert with our current estimate of the phases
 inverse = librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 length=length,
 res_type=res_type
 )

 # Rebuild the spectrogram
 rebuilt = librosa.constantq.cqt(
 inverse,
 sr=sr,
 bins_per_octave=bins_per_octave,
 n_bins=C.shape[0],
 hop_length=hop_length,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 res_type=res_type,
 )

 # Update our phase estimates
 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

# Return the final phase estimates
sol_dict['sol']= librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 tuning=tuning,
 filter_scale=filter_scale,
 fmin=fmin,
 window=window,
 length=length,
 res_type=res_type,
)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.griffinlim_cqt,2019-07,new feature
"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]


C = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))
bins_per_octave=36
init=None

# Store the solution in sol_dict['sol'].
sol_dict = {""sol"":None}",librosa,"sol = librosa.griffinlim_cqt(C, sr=sr, bins_per_octave=bins_per_octave, init=init)
assert np.allclose(sol, sol_dict['sol'])",Compute the approximate constant-Q magnitude spectrogram inversion using the “fast” Griffin-Lim algorithm.,,"sol_dict['sol'] = librosa.griffinlim_cqt(C, sr=sr, bins_per_octave=bins_per_octave, init=init)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.8.0,python==3.7,,librosa.griffinlim_cqt,2020-07,new feature
"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

S = np.abs(librosa.stft(y))**2
M = librosa.feature.melspectrogram(y=y, sr=sr, S=S)
n_fft=2048
hop_length=512
win_length=None
window='hann'
center=True
pad_mode='reflect'
power=2.0
n_iter=32
length=None
dtype=np.float32

# Store the solution in sol_dict['sol'].
sol_dict = {'sol':None}
np.random.seed(seed=0)",librosa,"n_fft=2048
hop_length=512
win_length=None
window='hann'
center=True
pad_mode='reflect'
power=2.0
n_iter=32
length=None
dtype=np.float32
np.random.seed(seed=0)

import numpy as np
import scipy.optimize

def _nnls_obj(x, shape, A, B):
 # Scipy's lbfgs flattens all arrays, so we first reshape
 # the iterate x
 x = x.reshape(shape)

 # Compute the difference matrix
 diff = np.dot(A, x) - B

 # Compute the objective value
 value = 0.5 * np.sum(diff**2)

 # And the gradient
 grad = np.dot(A.T, diff)

 # Flatten the gradient
 return value, grad.flatten()


def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):
 # If we don't have an initial point, start at the projected
 # least squares solution
 if x_init is None:
 x_init = np.linalg.lstsq(A, B, rcond=None)[0]
 np.clip(x_init, 0, None, out=x_init)

 # Adapt the hessian approximation to the dimension of the problem
 kwargs.setdefault('m', A.shape[1])

 # Construct non-negative bounds
 bounds = [(0, None)] * x_init.size
 shape = x_init.shape

 # optimize
 x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,
 args=(shape, A, B),
 bounds=bounds,
 **kwargs)
 # reshape the solution
 return x.reshape(shape)


def nnls(A, B, **kwargs):
 # If B is a single vector, punt up to the scipy method
 if B.ndim == 1:
 return scipy.optimize.nnls(A, B)[0]

 n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))

 # Process in blocks:
 if B.shape[-1] <= n_columns:
 return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)

 x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)
 np.clip(x, 0, None, out=x)
 x_init = x

 for bl_s in range(0, x.shape[-1], n_columns):
 bl_t = min(bl_s + n_columns, B.shape[-1])
 x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],
 x_init=x_init[:, bl_s:bl_t],
 **kwargs)
 return x

rng = np.random.seed(seed=0)
def mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):
 # Construct a mel basis with dtype matching the input data
 mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],
 **kwargs)

 # Find the non-negative least squares solution, and apply
 # the inverse exponent.
 # We'll do the exponentiation in-place.
 inverse = nnls(mel_basis, M)
 return np.power(inverse, 1./power, out=inverse)


stft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)
def griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', 
 center=True, dtype=np.float32, length=None, pad_mode='reflect',
 momentum=0.99, random_state=None):
 #rng = np.random.RandomState(seed=0)
 rng = np.random
 n_fft = 2 * (S.shape[0] - 1)

 angles = np.exp(2j * np.pi * rng.rand(*S.shape))

 rebuilt = 0.

 for _ in range(n_iter):
 tprev = rebuilt
 inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
 rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16
 # Return the final phase estimates
 return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
sol = griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length,
 pad_mode=pad_mode)
assert np.allclose(sol, sol_dict['sol'])",Invert a mel power spectrogram to audio using Griffin-Lim.,,"import numpy as np
import scipy.optimize




def _nnls_obj(x, shape, A, B):
 # Scipy's lbfgs flattens all arrays, so we first reshape
 # the iterate x
 x = x.reshape(shape)

 # Compute the difference matrix
 diff = np.dot(A, x) - B

 # Compute the objective value
 value = 0.5 * np.sum(diff**2)

 # And the gradient
 grad = np.dot(A.T, diff)

 # Flatten the gradient
 return value, grad.flatten()


def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):
 # If we don't have an initial point, start at the projected
 # least squares solution
 if x_init is None:
 x_init = np.linalg.lstsq(A, B, rcond=None)[0]
 np.clip(x_init, 0, None, out=x_init)

 # Adapt the hessian approximation to the dimension of the problem
 kwargs.setdefault('m', A.shape[1])

 # Construct non-negative bounds
 bounds = [(0, None)] * x_init.size
 shape = x_init.shape

 # optimize
 x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,
 args=(shape, A, B),
 bounds=bounds,
 **kwargs)
 # reshape the solution
 return x.reshape(shape)


def nnls(A, B, **kwargs):
 # If B is a single vector, punt up to the scipy method
 if B.ndim == 1:
 return scipy.optimize.nnls(A, B)[0]

 n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))

 # Process in blocks:
 if B.shape[-1] <= n_columns:
 return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)

 x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)
 np.clip(x, 0, None, out=x)
 x_init = x

 for bl_s in range(0, x.shape[-1], n_columns):
 bl_t = min(bl_s + n_columns, B.shape[-1])
 x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],
 x_init=x_init[:, bl_s:bl_t],
 **kwargs)
 return x

rng = np.random.seed(seed=0)
def mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):
 # Construct a mel basis with dtype matching the input data
 mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],
 **kwargs)

 # Find the non-negative least squares solution, and apply
 # the inverse exponent.
 # We'll do the exponentiation in-place.
 inverse = nnls(mel_basis, M)
 return np.power(inverse, 1./power, out=inverse)


stft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)
def griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', 
 center=True, dtype=np.float32, length=None, pad_mode='reflect',
 momentum=0.99, random_state=None):
 #rng = np.random.RandomState(seed=0)
 rng = np.random
 n_fft = 2 * (S.shape[0] - 1)

 angles = np.exp(2j * np.pi * rng.rand(*S.shape))

 rebuilt = 0.

 for _ in range(n_iter):
 tprev = rebuilt
 inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
 rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16
 # Return the final phase estimates
 return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
sol_dict['sol'] = griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length,
 pad_mode=pad_mode)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.feature.inverse.mel_to_audio,2018-02,new feature
"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

S = np.abs(librosa.stft(y))**2
M = librosa.feature.melspectrogram(y=y, sr=sr, S=S)

# Store the solution in sol_dict['sol'].
sol_dict = {'sol':None}
np.random.seed(seed=0)",librosa,"np.random.seed(seed=0)
sol = librosa.feature.inverse.mel_to_audio(M)
assert np.allclose(sol, sol_dict['sol'])",Invert a mel power spectrogram to audio using Griffin-Lim. ,,sol_dict['sol'] = librosa.feature.inverse.mel_to_audio(M),pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.feature.inverse.mel_to_audio,2019-07,new feature
"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
mfcc = librosa.feature.mfcc(y=y, sr=sr)

# Store the solution in sol_dict['sol'].
sol_dict = {'sol':None}
np.random.seed(seed=0)",librosa,"def mfcc_to_mel(mfcc, n_mels=128, dct_type=2, norm='ortho', ref=1.0):
 '''Invert Mel-frequency cepstral coefficients to approximate a Mel power
 spectrogram.

 This inversion proceeds in two steps:

 1. The inverse DCT is applied to the MFCCs
 2. `core.db_to_power` is applied to map the dB-scaled result to a power spectrogram


 Parameters
 ----------
 mfcc : np.ndarray [shape=(n_mfcc, n)]
 The Mel-frequency cepstral coefficients

 n_mels : int > 0
 The number of Mel frequencies

 dct_type : None or {1, 2, 3}
 Discrete cosine transform (DCT) type
 By default, DCT type-2 is used.

 norm : None or 'ortho'
 If `dct_type` is `2 or 3`, setting `norm='ortho'` uses an orthonormal
 DCT basis.

 Normalization is not supported for `dct_type=1`.

 ref : number or callable
 Reference power for (inverse) decibel calculation


 Returns
 -------
 M : np.ndarray [shape=(n_mels, n)]
 An approximate Mel power spectrum recovered from `mfcc`
 '''

 logmel = scipy.fftpack.idct(mfcc, axis=0, type=dct_type, norm=norm, n=n_mels)

 return librosa.db_to_power(logmel, ref=ref)
np.random.seed(seed=0)
sol = mfcc_to_mel(mfcc)
assert np.allclose(sol, sol_dict['sol'])",Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.,,"def mfcc_to_mel(mfcc, n_mels=128, dct_type=2, norm='ortho', ref=1.0):
 '''Invert Mel-frequency cepstral coefficients to approximate a Mel power
 spectrogram.

 This inversion proceeds in two steps:

 1. The inverse DCT is applied to the MFCCs
 2. `core.db_to_power` is applied to map the dB-scaled result to a power spectrogram


 Parameters
 ----------
 mfcc : np.ndarray [shape=(n_mfcc, n)]
 The Mel-frequency cepstral coefficients

 n_mels : int > 0
 The number of Mel frequencies

 dct_type : None or {1, 2, 3}
 Discrete cosine transform (DCT) type
 By default, DCT type-2 is used.

 norm : None or 'ortho'
 If `dct_type` is `2 or 3`, setting `norm='ortho'` uses an orthonormal
 DCT basis.

 Normalization is not supported for `dct_type=1`.

 ref : number or callable
 Reference power for (inverse) decibel calculation


 Returns
 -------
 M : np.ndarray [shape=(n_mels, n)]
 An approximate Mel power spectrum recovered from `mfcc`
 '''

 logmel = scipy.fftpack.idct(mfcc, axis=0, type=dct_type, norm=norm, n=n_mels)

 return librosa.db_to_power(logmel, ref=ref)
sol_dict['sol'] = mfcc_to_mel(mfcc)",pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.6.0,python==3.7,,librosa.feature.inverse.mel_to_audio,2018-02,new feature
"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
mfcc = librosa.feature.mfcc(y=y, sr=sr)

# Store the solution in sol_dict['sol'].
sol_dict = {'sol':None}
np.random.seed(seed=0)",librosa,"np.random.seed(seed=0)
sol = librosa.feature.inverse.mfcc_to_mel(mfcc)
assert np.allclose(sol, sol_dict['sol'])",Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.,,sol_dict['sol'] = librosa.feature.inverse.mfcc_to_mel(mfcc),pip==24.1 numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile==0.10.2,0.7.0,python==3.7,,librosa.feature.inverse.mfcc_to_mel,2019-07,new feature
"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
    random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
    return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",pillow,"
def create(imIn1, imIn2, mode=None):
    if imIn1.shape != imIn2.shape:
        return None
    return np.empty_like(imIn1, dtype=np.uint8)

def imaging_overlay(imIn1, imIn2):
    imOut = create(imIn1, imIn2)
    if imOut is None:
        return None
    
    ysize, xsize, _ = imOut.shape
    for y in range(ysize):
        for x in range(xsize):
            for c in range(3):  # Loop over RGB channels
                in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
                if in1 < 128:
                    imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
                else:
                    imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
    
    return imOut

sol = imaging_overlay(np.array(img1), np.array(img2))
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",Superimpose two images on top of each other using the Overlay algorithm.,,"
def create(imIn1, imIn2, mode=None):
    if imIn1.shape != imIn2.shape:
        return None
    return np.empty_like(imIn1, dtype=np.uint8)

def imaging_overlay(imIn1, imIn2):
    imOut = create(imIn1, imIn2)
    if imOut is None:
        return None
    
    ysize, xsize, _ = imOut.shape
    for y in range(ysize):
        for x in range(xsize):
            for c in range(3):  # Loop over RGB channels
                in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
                if in1 < 128:
                    imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
                else:
                    imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
    
    return imOut

sol_dict['sol'] = imaging_overlay(np.array(img1), np.array(img2))",numpy==1.16,7.0.0,python==3.7,,PIL.ImageChops.overlay,2020-01,new feature
"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
    random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
    return Image.fromarray(random_data)

width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)


# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",pillow,"
def create(imIn1, imIn2, mode=None):
    if imIn1.shape != imIn2.shape:
        return None
    return np.empty_like(imIn1, dtype=np.uint8)

def imaging_softlight(imIn1, imIn2):
    if imIn1.shape != imIn2.shape:
        return None
  
    imOut = create(imIn1, imIn2)
    ysize, xsize, _ = imOut.shape
    for y in range(ysize):
        for x in range(xsize):
            for c in range(3):  # Loop over RGB channels
                in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
                imOut[y, x, c] = int((((255 - in1) * (in1 * in2)) // 65536) +  (in1 * (255 - ((255 - in1) * (255 - in2) // 255))) // 255)
    return imOut
sol = imaging_softlight(np.array(img1), np.array(img2))
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",Superimpose two images on top of each other using the Soft Light algorithm.,,"
def create(imIn1, imIn2, mode=None):
    if imIn1.shape != imIn2.shape:
        return None
    return np.empty_like(imIn1, dtype=np.uint8)

def imaging_softlight(imIn1, imIn2):
    if imIn1.shape != imIn2.shape:
        return None
    
    imOut = create(imIn1, imIn2)
    ysize, xsize, _ = imOut.shape
    for y in range(ysize):
        for x in range(xsize):
            for c in range(3):  # Loop over RGB channels
                in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
                imOut[y, x, c] = int((((255 - in1) * (in1 * in2)) // 65536) +  (in1 * (255 - ((255 - in1) * (255 - in2) // 255))) // 255)
    return imOut
sol_dict['sol'] = imaging_softlight(np.array(img1), np.array(img2))",numpy==1.16,7.0.0,python==3.7,,PIL.ImageChops.soft_light,2020-01,new feature
"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
    random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
    return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)


# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",pillow,"
def create(imIn1, imIn2, mode=None):
    if imIn1.shape != imIn2.shape:
        return None
    return np.empty_like(imIn1, dtype=np.uint8)

def imaging_hardlight(imIn1, imIn2):
    imOut = create(imIn1, imIn2)
    if imOut is None:
        return None
    
    ysize, xsize, _ = imOut.shape
    for y in range(ysize):
        for x in range(xsize):
            for c in range(3):  # Loop over RGB channels
                in1, in2 = int(imIn2[y, x, c]), int(imIn1[y, x, c])
                if in1 < 128:
                    imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
                else:
                    imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
    
    return imOut

sol = imaging_hardlight(np.array(img1), np.array(img2))
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",Superimpose two images on top of each other using the Hard Light algorithm. ,,"
def create(imIn1, imIn2, mode=None):
    if imIn1.shape != imIn2.shape:
        return None
    return np.empty_like(imIn1, dtype=np.uint8)

def imaging_hardlight(imIn1, imIn2):
    imOut = create(imIn1, imIn2)
    if imOut is None:
        return None
    
    ysize, xsize, _ = imOut.shape
    for y in range(ysize):
        for x in range(xsize):
            for c in range(3):  # Loop over RGB channels
                in1, in2 = int(imIn2[y, x, c]), int(imIn1[y, x, c])
                if in1 < 128:
                    imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
                else:
                    imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
    
    return imOut

sol_dict['sol'] = imaging_hardlight(np.array(img1), np.array(img2))",numpy==1.16,7.0.0,python==3.7,,PIL.ImageChops.hard_light,2020-01,new feature
"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",pillow,"sol = ImageChops.overlay(img1, img2)
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",Superimpose two images on top of each other using the Overlay algorithm.,,"sol_dict['sol'] = ImageChops.overlay(img1, img2)",numpy==1.16,7.1.0,python==3.7,,PIL.ImageChops.overlay,2020-04,new feature
"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",pillow,"sol = ImageChops.soft_light(img1, img2)
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",Superimpose two images on top of each other using the Soft Light algorithm.,,"sol_dict['sol'] = ImageChops.soft_light(img1, img2)",numpy==1.16,7.1.0,python==3.7,,PIL.ImageChops.soft_light,2020-04,new feature
"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)

# Store the solution in sol_dict['sol'].
sol_dict = {'sol': None}",pillow,"sol = ImageChops.hard_light(img1, img2)
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",Superimpose two images on top of each other using the Hard Light algorithm.,,"sol_dict['sol'] = ImageChops.hard_light(img1, img2)",numpy==1.16,7.1.0,python==3.7,,PIL.ImageChops.hard_light,2020-04,new feature
