Unnamed: 0,library,version,name_of_class_or_func,type_of_change,problem,starting_code,solution,test,additional_dependencies,release_date,python_version,count as ,env_id
0,torch,1.9.0,log_ndtr,"
other library
","Calculate the logarithm of the cumulative distribution function of the standard normal distribution using available functions. If not available in PyTorch, use another library.","import torch\ninput_tensor = torch.linspace(-10, 10, steps=20)\n# put answer in variable called output\n",import numpy as np\nfrom scipy.stats import norm\noutput = torch.from_numpy(norm.logcdf(input_tensor.numpy())),"input_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.from_numpy(norm.logcdf(input_tensor.numpy()))\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",scipy==1.7.3 numpy==1.21.6,,,,
1,torch,1.12.0,log_ndtr,"
new func/method/class
",Calculate the logarithm of the cumulative distribution function of the standard normal distribution using PyTorch's special functions.,"import torch\ninput_tensor = torch.linspace(-10, 10, steps=20)\n# put answer in variable called output\n",output = torch.special.log_ndtr(input_tensor),"input_tensor = torch.linspace(-10, 10, steps=20)\nexpected_result = torch.special.log_ndtr(input_tensor)\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",,,,,
2,torch,1.9.0,gammaln,"
other library
","Calculate the natural logarithm of the absolute value of the gamma function using PyTorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",import numpy as np\nfrom scipy.special import gammaln as scipy_gammaln\noutput = torch.from_numpy(scipy_gammaln(input_tensor.numpy())),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([torch.inf,-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",scipy==1.7.3 numpy==1.21.6,,,,
3,torch,1.9.0,erf,"
other library
","Calculate the error function using PyTorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",import numpy as np\nfrom scipy.special import erf as scipy_erf\noutput = torch.from_numpy(scipy_erf(input_tensor.numpy())),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",scipy==1.7.3 numpy==1.21.6,,,,
4,torch,1.9.0,erfc,"
other library
","Calculate the complementary error function using PyTorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",import numpy as np\nfrom scipy.special import erfc as scipy_erfc\noutput = torch.from_numpy(scipy_erfc(input_tensor.numpy())),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",scipy==1.7.3 numpy==1.21.6,,,,
5,torch,1.9.0,bessel_i0,"
other library
","Calculate the modified Bessel function of the first kind, order 0 using PyTorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",import numpy as np\nfrom scipy.special import i0 as scipy_i0\noutput = torch.from_numpy(scipy_i0(input_tensor.numpy())),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",scipy==1.7.3 numpy==1.21.6,,,,
6,torch,1.9.0,bessel_i1,"
other library
","Calculate the modified Bessel function of the first kind, order 1 using PyTorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\n# put answer in variable called output\n",import numpy as np\nfrom scipy.special import i1 as scipy_i1\noutput = torch.from_numpy(scipy_i1(input_tensor.numpy())),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",scipy==1.7.3 numpy==1.21.6,,,,
7,torch,1.10.0,gammaln,"
new func/method/class
","Calculate the natural logarithm of the absolute value of the gamma function using pytorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch.special.gammaln(input_tensor),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([torch.inf,-0.0545,0.1092,1.0218,2.3770,4.0476,5.9637,8.0806,10.3675,12.8018])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",,,,,
8,torch,1.10.0,erf,"
new func/method/class
","Calculate the error function using pytorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch.special.erf(input_tensor),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000,0.8839,0.9983,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000,1.0000])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",,,,,
9,torch,1.10.0,erfc,"
new func/method/class
","Calculate the complementary error function using pytorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch.special.erfc(input_tensor),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.1610e-01,1.6740e-03,2.4285e-06,3.2702e-10,3.9425e-15,4.1762e-21,3.8452e-28,3.0566e-36,1.4013e-45])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",,,,,
10,torch,1.10.0,bessel_i0,"
new func/method/class
","Calculate the modified Bessel function of the first kind, order 0 using pytorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch.special.i0(input_tensor),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([1.0000e+00,1.3333e+00,2.6721e+00,6.4180e+00,1.6648e+01,4.4894e+01,1.2392e+02,3.4740e+02,9.8488e+02,2.8157e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",,,,,
11,torch,1.10.0,bessel_i1,"
new func/method/class
","Calculate the modified Bessel function of the first kind, order 1 using pytorch's special functions if available in this version, otherwise you may use another library.","import torch\ninput_tensor = torch.linspace(0, 10, steps=10)\noutput = ",torch.special.i1(input_tensor),"input_tensor = torch.linspace(0, 10, steps=10)\nexpected_result = torch.tensor([0.0000e+00,6.4581e-01,1.9536e+00,5.3391e+00,1.4628e+01,4.0623e+01,1.1420e+02,3.2423e+02,9.2770e+02,2.6710e+03])\nassert torch.allclose(output, expected_result, rtol=1e-3, atol=1e-3)",,,,,
12,torch,1.10.0,invert_mask_v1_1,"
output behaviour
","You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to implement a function invert_mask to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask.","import torch\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([3, 1, 2])\nmask= ",tensor1 < tensor2\nmask = ~mask,"expected_mask=torch.tensor([False, True, True])\nassert torch.all(torch.eq(mask, expected_mask))",,,,,
13,torch,1.13,invert_mask_v1_2,"
output behaviour
","You are given two tensors, `tensor1` and `tensor2`, both of shape `(n,)`. Your task is to implement a function invert_mask to create a boolean mask indicating whether each element of `tensor1` is less than the corresponding element of `tensor2`, and then invert this mask.","import torch\ntensor1 = torch.tensor([1, 2, 3])\ntensor2 = torch.tensor([3, 1, 2])\nmask= ",tensor1 < tensor2\nmask = ~(mask.bool()),"expected_mask=torch.tensor([False, True, True])\nassert torch.all(torch.eq(mask, expected_mask))",,,,,
14,torch,1.13,torch.stft,"
argument change
",You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.,import torch\naudio_signal = torch.rand(1024)\nn_fft=128\nstft_result = ,"torch.stft(audio_signal, n_fft=n_fft, return_complex=False)","expected_shape = (65, 33, 2)\nassert stft_result.shape == expected_shape",,,,,
15,torch,2,torch.stft,"
argument change
",You are given an audio signal represented as a 1D tensor `audio_signal`. Your task is to compute the Short-Time Fourier Transform (STFT) of the signal. Do not return a complex data type.,import torch\naudio_signal = torch.rand(1024)\nn_fft=128\nstft_result = ,"torch.view_as_real(torch.stft(audio_signal, n_fft=n_fft, return_complex=True))","expected_shape = (65, 33, 2)\nassert stft_result.shape == expected_shape",,,,,
16,torch,1.13,torch.istft,"
argument change
","You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.","import torch
# Sample rate (samples per second)
fs = 8000  
# Duration of the signal in seconds
t = 1  
# Time axis for the signal
time = torch.linspace(0, t, steps=int(fs * t))
# Frequency of the sine wave in Hz
frequency = 440  
# Generate a sine wave
signal = torch.sin(2 * torch.pi * frequency * time)
n_fft = 1024  # Number of FFT points
hop_length = 256  # Number of samples between successive frames
win_length = 1024  # Window length
# Compute STFT
spectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=True)
# Perform ISTFT
reconstructed_signal = ","torch.istft(spectrogram, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)",expected_shape=signal.shape\nassert expected_shape == reconstructed_signal.shape,,,,,
17,torch,2,torch.istft,"
argument change
","You are given a spectrogram represented as a 3D tensor `spectrogram` with dimensions `(65, 33, 2)`, where the first dimension represents the frequency bins, the second dimension represents the time frames, and the third dimension represents the real and imaginary parts of the complex values. Your task is to compute the Inverse Short-Time Fourier Transform (ISTFT) of the spectrogram using PyTorch's `torch.istft` function.","import torch
# Sample rate (samples per second)
fs = 8000  
# Duration of the signal in seconds
t = 1  
# Time axis for the signal
time = torch.linspace(0, t, steps=int(fs * t))
# Frequency of the sine wave in Hz
frequency = 440  
# Generate a sine wave
signal = torch.sin(2 * torch.pi * frequency * time)
n_fft = 1024  # Number of FFT points
hop_length = 256  # Number of samples between successive frames
win_length = 1024  # Window length
# Compute STFT
spectrogram = torch.stft(signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), normalized=False, return_complex=False)
# Perform ISTFT
reconstructed_signal = ","torch.istft(torch.view_as_complex(spectrogram), n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=torch.hann_window(win_length), length=signal.shape[0], normalized=False)",expected_shape=signal.shape\nassert expected_shape == reconstructed_signal.shape,,,,,
18,geopandas,0.10.0,sjoin,name change,Write a function that performs a spatial join.,"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_join(gdf1, gdf2):
    return ","gpd.sjoin(gdf1, gdf2, predicate='within')","
gdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
polygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]
gdf2 = gpd.GeoDataFrame({'geometry': polygons})

assert spatial_join(gdf1, gdf2).equals(gpd.sjoin(gdf1, gdf2, predicate='within'))",rtree,2021-10,,,
19,geopandas,0.9.0,sjoin,name change,Write a function that performs a spatial join.,"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_join(gdf1, gdf2):
    return ","gpd.sjoin(gdf1, gdf2, op='within')","
gdf1 = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
polygons = [Polygon([(0, 0), (0, 4), (4, 4), (4, 0)]), Polygon([(4, 4), (4, 8), (8, 8), (8, 4)])]
gdf2 = gpd.GeoDataFrame({'geometry': polygons})
expected_result = gpd.sjoin(gdf1, gdf2, op='within')
assert spatial_join(gdf1, gdf2).equals(expected_result)",rtree,2021-02,,,
20,geopandas,0.10.0,cascaded_union,name change,Write a function that performs a union.,"import geopandas as gpd
from shapely.geometry import box

def perform_union(gdf):
    return ",gdf.geometry.unary_union,"
gdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})
expected_result = gdf.geometry.unary_union
assert perform_union(gdf).equals(expected_result)",,2021-10,,,
21,geopandas,0.9.0,cascaded_union,name change,Write a function that performs a union.,"import geopandas as gpd
from shapely.geometry import box

def perform_union(gdf):
    return ",gdf.geometry.cascaded_union,"
gdf = gpd.GeoDataFrame({'geometry': [box(0, 0, 2, 5), box(0, 0, 2, 1)]})
expected_result = gdf.geometry.cascaded_union
assert perform_union(gdf) == expected_result",,2021-02,,,
22,geopandas,0.10.0,points_from_xy,name change,Write a function that creates a GeoSeries from x and y coordinates.,"import geopandas as gpd
def create_geoseries(x, y):
    return ","gpd.GeoSeries.from_xy(x, y)","
x, y = [1, 2], [3, 4]
print(create_geoseries(x,y))
expected_result = gpd.GeoSeries.from_xy(x, y)
assert create_geoseries(x, y).equals(expected_result)",,2021-10,,,
23,geopandas,0.9.0,points_from_xy,name change,Write a function that creates a GeoSeries from x and y coordinates.,"import geopandas as gpd
def create_geoseries(x, y):
    return ","gpd.points_from_xy(x, y)","
x, y = [1, 2], [3, 4]
print(create_geoseries(x,y))
expected_result = gpd.points_from_xy(x, y)
assert create_geoseries(x, y).equals(expected_result)",,2021-02,,,
24,geopandas,0.13.0,query_bulk,name change,Write a function that performs a spatial query.,"import geopandas as gpd
from shapely.geometry import Point, Polygon, box

def spatial_query(gdf, other):
    combined_geometry = other.unary_union
    return ",gdf.sindex.query(combined_geometry),"
gdf = gpd.GeoDataFrame({'geometry': [Point(1, 2)]})
other = gpd.GeoDataFrame({'geometry': [Point(1,1)]})
result = spatial_query(gdf, other)
expected_result = gdf.sindex.query(other.unary_union)
assert (result == expected_result).all()",rtree,2023-05,,,
25,geopandas,0.10.0,query_bulk,name change,Write a function that performs a spatial query.,"import geopandas as gpd
from shapely.geometry import Point, Polygon

def spatial_query(gdf, other):
    return ",gdf.sindex.query_bulk(other),"
gdf = gpd.GeoDataFrame({'geometry': [Point(1, 1), Point(2, 2), Point(3, 3)]})
other = gpd.GeoSeries([Polygon([(0, 0), (0, 4), (4, 4), (4, 0)])])
result = spatial_query(gdf, other)
expected_result = gdf.sindex.query_bulk(other)
assert (result == expected_result).all()",rtree,2021-10,,,
26,nltk,3.6.4,usage,"
deprecation
",Write a function that displays usage information of an object.,"import nltk
import io
import contextlib
def show_usage(obj) -> str:
    with io.StringIO() as buf, contextlib.redirect_stdout(buf):","
       help(obj)
       return buf.getvalue()","
assert ""Help on package nltk"" in show_usage(nltk)",,,,,
27,nltk,3.6.3,usage,"
deprecation
",Write a function that displays usage information of an object.,"import nltk
import io
import contextlib

def show_usage(obj) -> str:
    with io.StringIO() as buf, contextlib.redirect_stdout(buf):","
        nltk.usage(obj)
        return buf.getvalue()","
assert ""LazyModule supports the following operations"" in show_usage(nltk.corpus)
",,,,,
28,networkx,2.8,,"
argument change
","
Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.
","import networkx as nx
def modularity_communities(G):
    return nx.community.greedy_modularity_communities(G,", cutoff=5),"
G = nx.karate_club_graph()
assert len(modularity_communities(G)) > 0",,,,,
29,networkx,2.7,,"
argument change
","
Write a function that uses NetworkX's greedy_modularity_communities with the number of communities set at 5.
","import networkx as nx
def modularity_communities(G):
    return nx.community.greedy_modularity_communities(G,", n_communities=5),"
G = nx.karate_club_graph()
assert len(modularity_communities(G)) > 0",numpy,,,,
30,networkx,2.8,,"
name change
","
Write a function that calculates the diameters' extreme distance of a graph.
","import networkx as nx
def bounding_distance(G):
    return nx.diameter","(G, usebounds=True)","
G = nx.path_graph(5)
assert bounding_distance(G) is not None",,,,,
31,networkx,2.6,,"
name change
","
Write a function that calculates the diameters' extreme distance of a graph.
","import networkx as nx
def bounding_distance(G):
    return nx.algorithms.distance_measures.","extrema_bounding(G, ""diameter"")","
G = nx.path_graph(5)
assert bounding_distance(G) is not None",,,,,
32,networkx,2.5,,"
name change
","
Write a function that returns the naive greedy modularity communities for a graph.
","import networkx as nx
def naive_modularity_communities(G):
    return nx.community.",naive_greedy_modularity_communities(G),"
G = nx.karate_club_graph()
assert len(list(naive_modularity_communities(G))) > 0",,,,,
33,networkx,2.4,,"
name change
","
Write a function that returns the naive greedy modularity communities for a graph.
","import networkx as nx
def naive_modularity_communities(G):
    return nx.community.",_naive_greedy_modularity_communities(G),"
G = nx.karate_club_graph()
assert len(list(naive_modularity_communities(G))) > 0",,,,,
34,networkx,2.5,,"
name change (attribute)
","
Write a function that returns the nodes as a list of NetworkX graph.
","import networkx as nx
def get_nodes(G):
   return ",list(G.nodes),"
G = nx.karate_club_graph()
assert get_nodes(G) is not None and len(get_nodes(G)) > 0",,,,,
35,networkx,1.11,,"
name change (attribute)
","
Write a function that accesses the nodes as a list of NetworkX graph.
","import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def get_nodes(G):
    return ",list(G.node),"
G = nx.karate_club_graph()
assert get_nodes(G) is not None and len(get_nodes(G)) > 0",,,,,
36,networkx,2.5,,"
name change
","
Write a function that accesses the first edge of a NetworkX graph.
","import networkx as nx
def get_first_edge(G):
    return ",list(G.edges)[0],"
G = nx.karate_club_graph()
assert get_first_edge(G) is not None",,,,,
37,networkx,1.11,,"
name change
","
Write a function that accesses the first edge of a NetworkX graph.
","import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def get_first_edge(G):
    return ",list(G.edge)[0],"
G = nx.karate_club_graph()
assert get_first_edge(G) is not None",,,,,
38,networkx,2.5,,"
name change
","
Write a function that computes the shortest path lengths and predecessors on shortest paths in weighted graphs using NetworkX.


","import networkx as nx
def shortest_path(G, source):
    return nx.","bellman_ford_predecessor_and_distance(G, source)","
G = nx.path_graph(5)
assert shortest_path(G, 0) is not None",,,,,
39,networkx,1.11,,"
name change
","
Write a function that computes the shortest path lengths and predecessors on shortest paths in weighted graphs using NetworkX.


","import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def shortest_path(G, source):
    return nx.","bellman_ford(G, source)","
G = nx.path_graph(5)
assert shortest_path(G, 0) is not None",,,,,
40,networkx,2,add_edge,"
input argument change

","
Write a function that adds an edge to a NetworkX graph with a color red.
","import collections.abc
import sys
import math
import fractions
sys.modules['collections.Mapping'] = collections.abc.Mapping
sys.modules['collections.Set'] = collections.abc.Set
sys.modules['collections.Iterable'] = collections.abc.Iterable
fractions.gcd = math.gcd

import networkx as nx
def add_colored_edge(G, u, v):
   G.","add_edge(u, v, color='red')","
G = nx.Graph()
add_colored_edge(G, 1, 2)
assert G[1][2]['color'] == 'red'",,,,,
41,networkx,1.9,add_edge,"
input argument change
","
Write a function that adds an edge to a NetworkX graph with a color red.
","import html
import sys
import cgi
import math
import fractions

cgi.escape = html.escape
fractions.gcd = math.gcd

import networkx as nx
def add_colored_edge(G, u, v):
    G.","add_edge(u, v, {'color': 'red'})","
G = nx.Graph()
add_colored_edge(G, 1, 2)
assert G[1][2]['color'] == 'red'",,,,,
42,geopy,2.0.0,GoogleV3.timezone,"
breaking change
",Write a function that retrieves timezone information using GoogleV3.,"from geopy.geocoders import GoogleV3
def get_timezone(location):
    geolocator = GoogleV3()","
    return geolocator.reverse_timezone(location)","
location = (40.748817, -73.985428)
print(get_timezone(location))
assert get_timezone(location) is not None",pytz,,,,
43,geopy,1.9.0,GoogleV3.timezone,"
breaking change
",Write a function that retrieves timezone information using GoogleV3.,"import base64
import sys

# Define wrappers for the old functions
def encodestring(s):
    return base64.b64encode(s)

def decodestring(s):
    return base64.b64decode(s)

# Monkey patch the base64 module
base64.encodestring = encodestring
base64.decodestring = decodestring

from geopy.geocoders import GoogleV3
def get_timezone(location):
   geolocator = GoogleV3()","
   return geolocator.timezone(location)","
location = (40.748817, -73.985428)
assert get_timezone(location) is not None",pytz,,,,
44,gradio,3.24.0,-,"
argument change
",Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},"import gradio as gr
def render_quadratic_formula():
     pass


interface = gr.Interface(fn=render_quadratic_formula, inputs=[], outputs = ""text"")

def render_quadratic_formula():
    formula =","""$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$""
    return formula","
assert render_quadratic_formula().startswith(""$"") and render_quadratic_formula().endswith(""$"") ",-,,,,
45,gradio,3.36.0,-,"
argument change
",Write a function that renders the quadratic formula in LaTeX using Gradio's Chatbot. The quadratic formula is given by: x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a},"import gradio as gr
def render_quadratic_formula():
    formula = ""x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}""
    return formula

interface = gr.Chatbot","(fn=render_quadratic_formula, latex_delimiters=(""$$"", ""$$""))
","
assert not render_quadratic_formula().startswith(""$"") and not render_quadratic_formula().endswith(""$"") and ""$"" in interface.latex_delimiters[0] and  ""$"" in interface.latex_delimiters[1]",-,,,,
46,gradio,3.36.0,-,"
argument change
",Write a function that displays an image using Gradio where you cannot share the image.,"import gradio as gr
def display_image():
    return ""https://image_placeholder.com/42""

iface = gr.Interface","(fn=display_image, inputs=[], outputs=gr.Image(show_share_button=False))
","
assert iface.output_components[0].show_share_button==False",-,,,,
47,gradio,3.0.0,-,"
argument change
",Write a function that displays an image using Gradio where you cannot share the image.,"import gradio as gr
def display_image():
    return ""https://image_placeholder.com/42""

iface = gr.Interface","(fn=display_image, inputs=[], outputs=gr.Image())
","
assert type(gr.Image()) == type(iface.output_components[0])",-,,,,
48,gradio,2.9.2,-,"
argument change
",Write a function that takes an image input and returns a textbox output.,"import gradio as gr
def process_image(image):
    return ""Processed""

iface = gr.Interface","(fn=process_image, inputs=gr.inputs.Image(), outputs=gr.outputs.Textbox())","
assert type(iface.input_components[0])==type(gr.inputs.Image()) and type(iface.output_components[0])==type(gr.outputs.Textbox()) or type(iface.input_components[0])==type(gr.components.Image()) and type(iface.output_components[0])==type(gr.components.Textbox())",black,,,,
49,gradio,3.24.0,-,"
argument change
",Write a function that takes an image input and returns a label output.,"import gradio as gr
def process_image(image):
    return ""Processed""

iface = gr.Interface","(fn=process_image, inputs=gr.Image(), outputs=gr.Label())","
assert type(iface.input_components[0])==type(gr.Image()) and type(iface.output_components[0])==type(gr.Label())",-,,,,
50,gradio,3.20.0,-,"
No new feature
",Write a function that generates an interactive bar plot. Overwrite the method gradio_plot and make sure to import dependencies if needed.,"import gradio as gr
import pandas as pd
def gradio_plot(): 
     pass
data = pd.DataFrame({
        'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],
        'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]
    })

iface = gr.Interface(fn=gradio_plot, inputs=[], outputs=gr.Image())


","import matplotlib.pyplot as plt
def gradio_plot(): 
    plt.figure(figsize=(10, 5))
    plt.bar(data['a'], data['b'])
    plt.title('Simple Bar Plot with made up data')
    plt.xlabel('a')
    plt.ylabel('b')
    plt.savefig('bar_plot.png')
    plt.close()
    return 'bar_plot.png'
","
assert 'bar_plot.png' in gradio_plot()
matplotlib_imported = False
try:
     plt.figure()
     matplotlib_imported = True
except Exception:
     pass

try:
     matplotlib.plotly.figure()
     matplotlib_imported = True
except Exception:
     pass

assert matplotlib_imported",pandas matplotlib,,,,
51,gradio,3.17.0,-,"
new feature
",Write a function that generates an interactive bar plot. Set the new gr.Interface object as a variable named iface.,"import gradio as gr
import pandas as pd
data = pd.DataFrame({
        'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],
        'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]
    })
def gradio_plot():
    ","return gr.BarPlot(
        simple,
        x='a',
        y='b',
        title='Simple Bar Plot with made up data',
        tooltip=['a', 'b'],
    )
iface = gr.Interface(fn=gradio_plot, inputs=[], outputs='plot')","
assert type(iface.output_components[0]) == gr.Plot",pandas,,,,
52,gradio,3.15.0,-,"
argument change
",Write a function that returns the selected options from a list of options. Users can select multiple options.,"import gradio as gr

def get_selected_options(options):
    return f""Selected options: {options}""

selection_options = [""angola"", ""pakistan"", ""canada""]

iface = gr.Interface(get_selected_options, inputs = 
","gr.CheckboxGroup([""angola"", ""pakistan"", ""canada""]), outputs = 'text')","
assert type(iface.input_components[0]) == gr.CheckboxGroup",,,,,
53,gradio,3.17.0,-,"
argument change
",Write a function that returns the selected options from a list of options. Users can select multiple options.,"import gradio as gr

def get_selected_options(options):
    return f""Selected options: {options}""

selection_options = [""angola"", ""pakistan"", ""canada""]

iface = gr.Interface(get_selected_options, inputs =
","gr.Dropdown(selection_options, multiselect=True), outputs = 'text')","
assert (type(iface.input_components[0]) == gr.Dropdown and iface.input_components[0].multiselect == True ) or type(iface.input_components[0]) == gr.CheckboxGroup",,,,,
54,scikit-learn,1.1,GradientBoostingClassifier,"
output behaviour
",Train a Gradient Boosting Classifier from scikit-learn for a binary classification task and get the number of features used in fit.,"from sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\n# Load data\nX = np.random.rand(100, 20)  # 100 samples, 20 features\ny = np.random.randint(0, 2, 100)  # 100 binary labels\n\n# Initialize and fit the classifier\nclf = GradientBoostingClassifier()\nclf.fit(X, y)\n","n_features_used = clf.n_features_in_\nprint('Number of features used:', n_features_used)","expected_n_features=20\nassert n_features_used == expected_n_features, f'Test: Number of features should be 20, and it is: {n_features_used}'",numpy==1.23.5,,,,
55,scikit-learn,1.1,GradientBoostingClassifier,"
argument change
",You are tasked with developing a solution that uses Gradient Boosting Classifier from scikit-learn for a binary classification task with the mean squared error as ther criterion.,from sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize the classifier\nclassifier = GradientBoostingClassifier(criterion=,'squared_error'),"expected_clf=GradientBoostingClassifier\nassert isinstance(classifier, expected_clf)",numpy==1.23.5,,,,
56,scikit-learn,1.2,CCA,"
attribute change
","Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.","from sklearn.cross_decomposition import CCA\nimport numpy as np\nX = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncca_model.fit(X, Y)\ncoef_shape = cca_model.coef_.shape\nexpected_shape =","(10, 5)",\ncorrect_shape=coef_shape\nassert expected_shape == correct_shape,numpy==1.23.5,,,,
57,scikit-learn,1.3,CCA,"
attribute change
","Given dummy data, determine the shape of the coef_ attribute of a CCA model fitted with this data.","from sklearn.cross_decomposition import CCA\nimport numpy as np\nX = np.random.rand(100, 10)\nY = np.random.rand(100, 5)\ncca_model = CCA()\ncca_model.fit(X, Y)\ncoef_shape = cca_model.coef_.shape\nexpected_shape =","(5, 10)",\ncorrect_shape=coef_shape\nassert expected_shape == correct_shape,numpy==1.23.5,,,,
58,scikit-learn,1.1,make_sparse_coded_signal,"
output behaviour
",Generate a sparse coded signal where the data is transposed.,"from sklearn.datasets import make_sparse_coded_signal\nn_samples=100\nn_features=50\n\nn_components=20\nn_nonzero_coefs=10\ny, d, c = ","make_sparse_coded_signal(n_samples=n_samples, n_features=n_features,n_components=n_components,n_nonzero_coefs=n_nonzero_coefs)","expected_shape_y = (n_features, n_samples)\nexpected_shape_d = (n_features, n_components)\nexpected_shape_c = (n_components, n_samples)\nassert y.shape == expected_shape_y\nassert d.shape == expected_shape_d\nassert c.shape == expected_shape_c",numpy==1.23.5,,,,
59,scikit-learn,1.3,make_sparse_coded_signal,"
output behaviour
",Generate a sparse coded signal where the data is transposed.,"from sklearn.datasets import make_sparse_coded_signal\nn_samples=100\nn_features=50\nn_components=20\nn_nonzero_coefs=10\\ny, d, c = ","make_sparse_coded_signal(n_samples=n_samples, n_features=n_features, n_components=n_components,n_nonzero_coefs=n_nonzero_coefs,data_transposed=True)","expected_shape_y = (n_features, n_samples)\nexpected_shape_d = (n_features, n_components)\nexpected_shape_c = (n_components, n_samples)\nassert y.shape == expected_shape_y\nassert d.shape == expected_shape_d\nassert c.shape == expected_shape_c",numpy==1.23.5,,,,
60,scikit-learn,1.1,FastICA,"
argument change
",Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.,"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\ndata, _ = load_digits(return_X_y=True)\nica = ","FastICA(n_components=7,random_state=0,whiten=True)\ntransformed_data = ica.fit_transform(data)","expected_shape = (1797, 7)\nassert transformed_data.shape == expected_shape",numpy==1.23.5,,,,
61,scikit-learn,1.3,FastICA,"
argument change
",Apply Fast Independent Component Analysis (FastICA) with a specific whiten parameter setting. Store transformed data in a variable transformed_data.,"from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\ndata, _ = load_digits(return_X_y=True)\nica = ","FastICA(n_components=7,random_state=0,whiten='arbitrary-variance')\ntransformed_data = ica.fit_transform(data)","expected_shape = (1797, 7)\nassert transformed_data.shape == expected_shape",numpy==1.23.5,,,,
62,scikit-learn,1.1,SimpleImputer,"
argument change
","Impute missing values in a dataset using SimpleImputer, including the `verbose` parameter if available. Verify that the output dimensions are as expected.","from sklearn.impute import SimpleImputer\nimport numpy as np\ndata = np.array([[1, 2, 3], [4, None, 6], [7, 8, None]], dtype=float)\nimputer = ",SimpleImputer()\nimputed_data = imputer.fit_transform(data),"expected_type=SimpleImputer\nassert isinstance(imputer, expected_type)",numpy==1.23.5,,,,
63,scikit-learn,1.3,get_scorer_names,"
name change
","Retrieve and list all available scorer names, ensuring they are returned in a list format.",from sklearn import metrics\nscorer_names = ,metrics.get_scorer_names(),"conditions = isinstance(scorer_names, list) and len(scorer_names) > 0\nassert conditions",numpy==1.23.5,,,,
64,scikit-learn,1.2,get_scorer_names,"
name change
","Retrieve and list all available scorer names, ensuring they are returned in a list format.",from sklearn import metrics\nscorer_names = ,list(metrics.SCORERS.keys()),"conditions = isinstance(scorer_names, list) and len(scorer_names) > 0\nassert conditions",numpy==1.23.5,,,,
65,scikit-learn,1.1,manhattan_distances,"
argument change
",Adapt the use of `manhattan_distances` to obtain a pairwise distance matrix.,"from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\ndistances = manhattan_distances(X, Y, sum_over_features=False)\nresult = ","np.sum(distances, axis=1)","expected_result = np.array([1, 5, 5, 1, 9, 3])\nassert np.allclose(result, expected_result, atol=1e-3)",numpy==1.23.5,,,,
66,scikit-learn,1.2,manhattan_distances,"
argument change
",Adapt the use of `manhattan_distances` in scikit-learn version 1.2 to obtain a pairwise distance matrix.,"from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[1, 1], [4, 4]])\nresult = ","manhattan_distances(X, Y)","expected_result = np.array([[1, 5], [5, 1], [9, 3]])\nassert np.allclose(result, expected_result, atol=1e-3)",numpy==1.23.5,,,,
67,matplotlib,3.4.0,revcmap,"
name change
",Reverse the following color mapping.,"from matplotlib.colors import *
import numpy as np
cmap = {
    ""blue"": [[1, 2, 2], [2, 2, 1]],
    ""red"": [[0, 0, 0], [1, 0, 0]],
    ""green"": [[0, 0, 0], [1, 0, 0]]
}

cmap_reversed = ","LinearSegmentedColormap(""custom_cmap"", cmap).reversed()
","
expected_cmap_reversed = {'blue': [(-1.0, 1, 2), (0.0, 2, 2)], 'red': [(0.0, 0, 0), (1.0, 0, 0)], 'green': [(0.0, 0, 0), (1.0, 0, 0)]}

reversed_cmap_dict = cmap_reversed._segmentdata

assert reversed_cmap_dict == expected_cmap_reversed",,,,,
68,matplotlib,3.6.0,,"
name change (attribute)
","Set the labels for the x, y and z axis to 'x_axis' 'y_axis' and 'z_axis'","import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

x = [1, 2, 3, 4, 5]
y = [5, 4, 3, 2, 1]
z = [2, 3, 4, 5, 6]

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(x, y, z, c='r', marker='o')
","ax.xaxis.set_label_text('x_axis')
ax.yaxis.set_label_text('y_axis')
ax.zaxis.set_label_text('z_axis')","
assert ax.xaxis.get_label().get_text()==""x_axis"" and ax.yaxis.get_label().get_text()==""y_axis"" and ax.zaxis.get_label().get_text()==""z_axis"" ",,,,,
69,matplotlib,3.5.0,,"
name change (attribute)
","Create a contour plot and set the axis for the ContourSet object. Write a function create_contour_plot that creates a contour plot using given x, y, and z data and assigns it to a specific axis","import numpy as np
import matplotlib.pyplot as plt

def create_contour_plot(x, y, z, ax):
    pass


x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.sin(np.sqrt(X**2 + Y**2))

fig, ax = plt.subplots()

contour_set = ","ax.contour(x, y, Z)
contour_set.ax = ax
","
assert contour_set.ax == ax",numpy,,,,
70,PyCaret,3,Simple_Imputer,"
attribute change
",Write the python code to construct a SimpleImputer with the time strategy as 'mean'using PyCaret version 3.0,"from pycaret.internal.preprocess import Simple_Imputer
import pycaret

test_length = 6
data = pycaret.datasets.get_data(""juice"")[0:test_length]
target = ""Purchase""

Imputer = Simple_Imputer",(time_strategy=‚Äùmean' target=target),"assert hasattr(Imputer, 'time_strategy' and Imputer.time_strategy == 'mean'",,03-18-2023,,,
71,PyCaret,3,setup,"
attribute change
",Write the python code to initialize a training environment and create the transformation pipeline using PyCaret version 3.0 Setup function which uses the hourly seasonal period for determining the frequency of the timeseries data.,"from pycaret.datasets import get_data
airline = get_data('airline')
from pycaret.time_series import setup
exp_name = setup","(data=airline,seasonal_parameter='H')","import inspect

def check_seasonal_parameter(func):
    sig = inspect.signature(func)
    parameters = sig.parameters
    assert 'seasonal_parameter' in parameters

# Test the function
check_seasonal_parameter(exp_name)",,03-18-2023,,,
72,pandas,1.5.0,groupby,"
output behaviour
","Use the pandas groupby operation with observed=False and dropna=False, where the intention is to include unobserved categories without dropping NA values. Your job is to predict the expected output after this operation.","import pandas as pd\ndf = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\ngrouped_df = df.groupby('x', observed=False, dropna=False).sum()\n# Determine if the groupby operation handled NA values correctly.\n# store expected value of grouped_df in a variable called expected_output\n","expected_output = pd.DataFrame({'y': [3, 4]}, index=pd.Index([1, None], name='x'))",assert grouped_df.equals(expected_output),numpy==1.21.6,,,,
73,pandas,1.5.1,groupby,"
output behaviour
","Use the pandas groupby operation with observed=False and dropna=False, where the intention is to include unobserved categories without dropping NA values. Your job is to predict the expected output after this operation.","import pandas as pd\ndf = pd.DataFrame({'x': pd.Categorical([1, None], categories=[1, 2, 3]), 'y': [3, 4]})\ngrouped_df = df.groupby('x', observed=False, dropna=False).sum()\n# Examine if the groupby operation correctly includes unobserved categories and handles NA values.\n# store expected value of grouped_df in a variable called expected_output\n","expected_output = pd.DataFrame({'y': [3, 0, 0]}, index=pd.Index([1, 2, 3], name='x'))",assert grouped_df.equals(expected_output),numpy==1.21.6,,,,
74,pandas,1.5.0,iloc,"gh
output behaviour
",Predict behaviour of setting values with iloc inplace.,"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\n# store expected value original prices in variable called expected_prices\n","expected_prices = pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)","correct_prices=pd.Series([11.1, 12.2], index=['book1', 'book2'], dtype=np.float64)\nassert expected_prices.equals(correct_prices)",numpy==1.21.6,,,,
75,pandas,2,iloc,"
output behaviour
",Predict behaviour of setting values with iloc inplace.,"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\ndf.iloc[:, 0] = new_prices\n# store expected value of original prices in variable called expected_prices\n","expected_prices = pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)","correct_prices=pd.Series([98.0, 99.0], index=['book1', 'book2'], dtype=np.float64)\nassert expected_prices.equals(correct_prices)",numpy==1.21.6,,,,
76,pandas,1.5.0,Series slicing,"
output behaviour
",Predict behaviour of integer slicing on a Series.,"import pandas as pd\nimport numpy as np\nser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nsliced_ser = ser[2:4]\n# put answer in variable called expected_output\n","expected_output = pd.Series([3, 4], index=[5, 7], dtype=np.int64)","assert sliced_ser.equals(expected_output), 'Slicing does not match expected output'",numpy==1.21.6,,,,
77,pandas,2,Series slicing,"
output behaviour
",Predict behaviour of integer slicing on a Series.,"import pandas as pd\nimport numpy as np\nser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\nsliced_ser = ser.iloc[2:4]\n# put answer in variable called expected_output\n","expected_output = pd.Series([3, 4], index=[5, 7], dtype=np.int64)","assert sliced_ser.equals(expected_output), 'Slicing does not match expected label-based output'",numpy==1.21.6,,,,
78,pandas,1.4.0,Index,"
output behaviour
",Predict the correct type.,"import pandas as pd\nindex = pd.Index([1, 2, 3], dtype='int32')\nis_correct_type = index.dtype ==", 'int64',assert is_correct_type,numpy==1.21.6,,,,
79,pandas,1.4.0,append,"
name change
",Combine series and dataframes.,"import pandas as pd\nseries1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n# put answers in variables combined_series and combined_dataframe\n","combined_series = series1.append(series2, ignore_index=True)\ncombined_dataframe = df1.append(df2, ignore_index=True)","expected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'",numpy==1.21.6,,,,
80,pandas,2,Index,"
output behaviour
",Predict the correct type.,"import pandas as pd\nindex = pd.Index([1, 2, 3], dtype='int32')\nis_correct_type = index.dtype ==", 'int32',assert is_correct_type,numpy==1.21.6,,,,
81,pandas,2,append,"
name change
",Combine series and dataframes.,"import pandas as pd\nseries1 = pd.Series([1, 2])\nseries2 = pd.Series([3, 4])\ndf1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n# put answers in variables combined_series and combined_dataframe\n","combined_series = pd.concat([series1, series2], ignore_index=True)\ncombined_dataframe = pd.concat([df1, df2], ignore_index=True)","expected_series_values = [1, 2, 3, 4]\nexpected_dataframe_values = [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert list(combined_series) == expected_series_values, 'Combined series values are incorrect'\nassert combined_dataframe.values.tolist() == expected_dataframe_values, 'Combined dataframe values are incorrect'",numpy==1.21.6,,,,
82,numpy,1.21.0,numpy.convolve,"
argument change
",Implement a function that calculates the convolution of two arrays with the mode set to full.,"import numpy as np

def apply_convolution_full(arr1, arr2):
    return ","np.convolve(arr1, arr2, mode=""full"")","
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_convolution_full(arr1, arr2).all() == np.convolve(arr1, arr2, 'full').all()",,22/06/2021,,,
83,numpy,1.21.0,numpy.convolve,"
argument change
",Implement a function that calculates the convolution of two arrays with the mode set to valid.,"import numpy as np

def apply_convolution_valid(arr1, arr2):
    return ","np.convolve(arr1, arr2, mode=""valid"")","
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_convolution_valid(arr1, arr2).all() == np.convolve(arr1, arr2, 'valid').all()",,22/06/2021,,,
84,numpy,1.21.0,numpy.correlate,"
argument change
",Implement a function that calculates the Cross-correlation of two 1-dimensional sequences with the mode set to full.,"import numpy as np

def apply_correlate_full(arr1, arr2):
    return ","np.correlate(arr1, arr2, mode=""full"")","
arr1 = np.array([1, 2, 3])
arr2 = np.array([0, 1, 0.5])
assert apply_correlate_full(arr1, arr2).all() == np.correlate(arr1, arr2, 'full').all()",,22/06/2021,,,
85,numpy,1.25.0,find_common_type,"
deprecation
","Given two arrays, find their common types.","import numpy as np

def find_common_type(arr1, arr2):
    return np.","common_type(array1, array2)","
array1 = np.array([1, 2, 3])
array2 = np.array([4.0, 5.0, 6.0])

assert find_common_type(array1, array2) == np.common_type(array1, array2)",,17/06/2023,,,
86,numpy,1.21.0,find_common_type,"
deprecation
","Given two arrays, find their common types.","import numpy as np

def find_common_type(arr1, arr2):
    return np.","find_common_type(array1, array2)","
array1 = np.array([1, 2, 3])
array2 = np.array([4.0, 5.0, 6.0])

assert find_common_type(array1, array2) == np.find_common_type(array1, array2)",,22/06/2021,,,
87,numpy,1.25.0,round_,"
deprecation
",Write a function that rounds an array of numbers.,"import numpy as np

def custom_round(arr):
    return ",np.round(arr),"

def test_custom_round():
    arr = np.array([1.5, 2.3, 3.7])
    result = custom_round(arr)
    expected = np.round(arr)
    assert np.array_equal(result, expected)

test_custom_round()",,17/06/2023,,,
88,numpy,1.25.0,product,"
deprecation
",Write a function that computes the product of an array.,"import numpy as np

def custom_product(arr):
    return ",np.prod(arr),"

def test_custom_product():
    arr = np.array([1, 2, 3, 4])
    result = custom_product(arr)
    expected = np.prod(arr)
    assert result == expected

test_custom_product()",,17/06/2023,,,
89,numpy,1.25.0,cumproduct,"
deprecation
",Write a function that computes the cumulative product of an array.,"import numpy as np

def custom_cumproduct(arr):
    return ",np.cumprod(arr),"

def test_custom_cumproduct():
    arr = np.array([1, 2, 3, 4])
    result = custom_cumproduct(arr)
    expected = np.cumprod(arr)
    assert np.array_equal(result, expected)

test_custom_cumproduct()",,17/06/2023,,,
90,numpy,1.25.0,sometrue,"
deprecation
",Write a function that checks if any elements in an array are true.,"import numpy as np

def custom_sometrue(arr):
    return ",np.any(arr),"

def test_custom_sometrue():
    arr = np.array([0, 0, 1, 0])
    result = custom_sometrue(arr)
    expected = np.any(arr)
    assert result == expected

test_custom_sometrue()",,17/06/2023,,,
91,numpy,1.25.0,alltrue,"
deprecation
",Write a function that checks if all elements in an array are true.,"import numpy as np

def custom_alltrue(arr):
    return ",np.all(arr),"

def test_custom_alltrue():
    arr = np.array([1, 1, 1, 1])
    result = custom_alltrue(arr)
    expected = np.all(arr)
    assert result == expected

test_custom_alltrue()",,17/06/2023,,,
92,numpy,1.21.0,round_,"
deprecation
",Write a function that rounds an array of numbers.,"import numpy as np

def custom_round(arr):
    return ",np.round_(arr),"

def test_custom_round():
    arr = np.array([1.5, 2.3, 3.7])
    result = custom_round(arr)
    expected = np.round_(arr)
    assert np.array_equal(result, expected)

test_custom_round()",,22/06/2021,,,
93,numpy,1.21.0,product,"
deprecation
",Write a function that computes the product of an array.,"import numpy as np

def custom_product(arr):
    return ",np.product(arr),"

def test_custom_product():
    arr = np.array([1, 2, 3, 4])
    result = custom_product(arr)
    expected = np.product(arr)
    assert result == expected

test_custom_product()",,22/06/2021,,,
94,numpy,1.21.0,cumproduct,"
deprecation
",Write a function that computes the cumulative product of an array.,"import numpy as np

def custom_cumproduct(arr):
    return ",np.cumproduct(arr),"
def test_custom_cumproduct():
    arr = np.array([1, 2, 3, 4])
    result = custom_cumproduct(arr)
    expected = np.cumproduct(arr)
    assert np.array_equal(result, expected)

test_custom_cumproduct()",,22/06/2021,,,
95,numpy,1.21.0,sometrue,"
deprecation
",Write a function that checks if any elements in an array are true.,"import numpy as np

def custom_anytrue(arr):
    return ",np.sometrue(arr),"
def test_custom_sometrue():
    arr = np.array([0, 0, 1, 0])
    result = custom_anytrue(arr)
    expected = np.sometrue(arr)
    assert result == expected

test_custom_sometrue()",,22/06/2021,,,
96,numpy,1.21.0,alltrue,"
deprecation
",Write a function that checks if all elements in an array are true.,"import numpy as np

def custom_alltrue(arr):
    return ",np.alltrue(arr),"

def test_custom_alltrue():
    arr = np.array([1, 1, 1, 1])
    result = custom_alltrue(arr)
    expected = np.alltrue(arr)
    assert result == expected

test_custom_alltrue()",,22/06/2021,,,
97,lightgbm,3.0.0,predict,Argument or Attribute change,Predict values for each sample with the starting iteration of the tenth iteration.,"import numpy as np
import lightgbm as lgb
from lightgbm import LGBMClassifier
np.random.seed(0)
data = np.random.rand(100, 10) 
target = np.random.randint(0, 2, 100)
model = LGBMClassifier()
model.fit(data, target)
model.fit(data, target)
pred = model",".predict(data, start_iteration=10) ","import numpy as np
expected_values = np.array([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0])
np.testing.assert_array_equal(pred, expected_values)",numpy==1.26.4,2020-08,,,
98,lightgbm,3.0.0,cv,Argument or Attribute change,Perform cross-validation with the given parameters and return the evaluation history for each fold.,"import numpy as np
import lightgbm as lgb
from sklearn.datasets import make_classification

NUM_SAMPLES = 500
NUM_FEATURES = 20
INFORMATIVE_FEATURES = 2
REDUNDANT_FEATURES = 10
RANDOM_STATE = 42
NUM_BOOST_ROUND = 100
NFOLD = 5
LEARNING_RATE = 0.05
EARLY_STOPPING_ROUNDS = 10
X, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)
train_data = lgb.Dataset(X, label=y)

params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': LEARNING_RATE,
    'verbose': -1
}

cv_results = lgb.cv(
    params=params,
    train_set=train_data,
    num_boost_round=NUM_BOOST_ROUND,
    nfold=NFOLD,
    early_stopping_rounds=EARLY_STOPPING_ROUNDS,","return_cvbooster=True
)","import numpy as np
assert 'cvbooster' in cv_results
assert len(cv_results['cvbooster'].boosters) == NFOLD
assert all(isinstance(booster, lgb.Booster) for booster in cv_results['cvbooster'].boosters)",numpy==1.26.4 scikit-learn==1.3.2,2020-08,,,
99,lightgbm,3.0.0,decode_string,Semantics or Function Behaviour change,Decode a byte string (ENCODED_STRING),"ENCODED_STRING = b'\x68\x65\x6c\x6c\x6f'

import lightgbm.compat as compat

decoded_string = ",compat.decode_string(ENCODED_STRING),"assert decoded_string == 'hello', ""Decoded string should be 'hello'""
",,2020-08,,,
100,lightgbm,3.0.0,cv,Argument or Attribute change,Perform cross-validation with the given parameters and display the training metric in progress. ,"import numpy as np
import lightgbm as lgb
from sklearn.datasets import make_classification

NUM_SAMPLES = 500
NUM_FEATURES = 20
INFORMATIVE_FEATURES = 2
REDUNDANT_FEATURES = 10
RANDOM_STATE = 42
NUM_BOOST_ROUND = 100
NFOLD = 5
LEARNING_RATE = 0.05
EARLY_STOPPING_ROUNDS = 10
X, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)
train_data = lgb.Dataset(X, label=y)

params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'learning_rate': LEARNING_RATE,
    'verbose': -1
}

cv_results = lgb.cv(
    params=params,
    train_set=train_data,
    num_boost_round=NUM_BOOST_ROUND,
    nfold=NFOLD,
    early_stopping_rounds=EARLY_STOPPING_ROUNDS,","eval_train_metric=True
)","assert {'train binary_logloss-mean', 'train binary_logloss-stdv', 'valid binary_logloss-mean', 'valid binary_logloss-stdv'}.issubset(cv_results.keys())",numpy==1.26.4 scikit-learn==1.3.2,2020-08,,,
101,lightgbm,3.0.0,cint32_array_to_numpy,Function Name change,Convert a ctypes pointer to a NumPy array of the specified length.,"import lightgbm as lgb
import numpy as np
import ctypes

c_array_type = ctypes.POINTER(ctypes.c_int32)
c_array = (ctypes.c_int32 * 5)(1, 2, 3, 4, 5)
c_pointer = ctypes.cast(c_array, c_array_type)
length = 5

np_array = lgb",".basic.cint32_array_to_numpy(c_pointer, length)","assert isinstance(np_array, np.ndarray)
assert np_array.shape == (5,)
assert np.array_equal(np_array, np.array([1, 2, 3, 4, 5], dtype=np.int32))",numpy==1.26.4,2020-08,,,
102,lightgbm,3.0.0,get_params,Semantics or Function Behaviour change,Get the parameters of a dataset object as a dictionary.,"import lightgbm as lgb
import numpy as np

data = np.random.rand(10, 2)
label = np.random.randint(2, size=10)
dataset = lgb.Dataset(data, label=label)

params =",dataset.get_params(),"assert isinstance(params, dict) or params is None
",numpy==1.26.4,2020-08,,,
103,lightgbm,3.0.0,json_default_with_numpy,Function Name change,Serialize a NumPy array to a JSON string using a custom default function that converts NumPy arrays to lists.,"import numpy as np
import json
from lightgbm.compat import json_default_with_numpy

NUMPY_ARRAY = np.array([1, 2, 3])

json_data = json.dumps(NUMPY_ARRAY",", default=json_default_with_numpy)","assert json_data == '[1, 2, 3]'




",numpy==1.26.4,2020-08,,,
104,lightgbm,4.3.0,basic._c_array,Function Name change,Create a ctypes array from a list of values.,"import ctypes
import lightgbm.basic as basic

CTYPE = ctypes.c_double
VALUES = [0.1, 0.2, 0.3, 0.4, 0.5]

c_array =","basic._c_array(CTYPE, VALUES)","assert all(isinstance(i, float) for i in c_array)
assert all(c_array[i] == VALUES[i] for i in range(len(VALUES)))",,2024-01,,,
105,lightgbm,4.3.0,basic._c_str,Function Name change,Convert a Python string to a C string.,"import lightgbm as lgb
import ctypes

# Test cases for c_str function
python_string = ""lightgbm""
c_string = ",lgb.basic._c_str(python_string),"assert isinstance(c_string, ctypes.c_char_p)
assert c_string.value.decode('utf-8') == python_string",,2024-01,,,
106,lightgbm,4.3.0,basic._convert_from_sliced_object,Function Name change,Convert a sliced NumPy array back to a contiguous NumPy array.,"import lightgbm as lgb
import numpy as np

data = np.random.rand(100, 10)
sliced_data = data[:, :5]

fixed_data = lgb",.basic._convert_from_sliced_object(sliced_data),"assert isinstance(fixed_data, np.ndarray)
assert fixed_data.shape == sliced_data.shape
assert np.array_equal(fixed_data, sliced_data)
",numpy==1.26.4,2024-01,,,
107,spacy,3.5.0,labels,New feature or additional dependency based change,Get the labels of the span ruler.,"import spacy
from spacy.pipeline.span_ruler import SpanRuler

nlp = spacy.blank(""en"")
ruler = SpanRuler(nlp)

patterns = [
    {""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""john""}]},
    {""label"": ""GPE"", ""pattern"": [{""LOWER"": ""london""}]},
]
ruler.add_patterns(patterns)

labels = ruler",.labels,"assert labels == ('GPE', 'PERSON')",numpy==1.26.4,2023-01,,,
108,spacy,3.5.0,make_whitespace_variant,New feature or additional dependency based change,Create a whitespace variant of an example.,"import spacy
from spacy.training import Example
from spacy.training import augment

nlp = spacy.blank(""en"")

tokens = nlp(""Hello world"")
annotations = {""entities"": [(0, 5, ""GREETING"")]}
example = Example.from_dict(tokens, annotations)

whitespace = "" ""
position = 1

augmented_example = ","augment.make_whitespace_variant(nlp, example, whitespace, position)","expected_doc_annotation = {
    'cats': {},
    'entities': ['U-GREETING', 'O', 'O'],
    'spans': {},
    'links': {}
}

expected_token_annotation = {
    'ORTH': ['Hello', ' ', 'world'],
    'SPACY': [True, False, False],
    'TAG': ['', '', ''],
    'LEMMA': ['', '', ''],
    'POS': ['', '', ''],
    'MORPH': ['', '', ''],
    'HEAD': [0, 1, 2],
    'DEP': ['', '', ''],
    'SENT_START': [1, 0, 0]
}

assert augmented_example.to_dict()[""doc_annotation""] == expected_doc_annotation
assert augmented_example.to_dict()[""token_annotation""] == expected_token_annotation",numpy==1.26.4,2023-01,,,
109,spacy,3.5.0,remove_by_id,New feature or additional dependency based change,Remove a pattern from a span ruler by its ID.,"import spacy
from spacy.pipeline.span_ruler import SpanRuler

nlp = spacy.blank(""en"")
ruler = SpanRuler(nlp)

patterns = [
    {""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""john""}], ""id"": ""pattern1""},
    {""label"": ""GPE"", ""pattern"": [{""LOWER"": ""london""}], ""id"": ""pattern2""},
]
ruler.add_patterns(patterns)

assert len(ruler.patterns) == 2

pattern_id_to_remove = ""pattern1""

ruler",.remove_by_id(pattern_id_to_remove),"assert len(ruler.patterns) == 1
remaining_pattern_ids = [pattern[""id""] for pattern in ruler.patterns]
assert pattern_id_to_remove not in remaining_pattern_ids",numpy==1.26.4,2023-01,,,
111,nltk,3.7,align_words,New feature or additional dependency based change,Align words in a hypothesis and reference sentence using the METEOR algorithm.,"import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import wordnet

hypothesis = [""the"", ""cat"", ""sits"", ""on"", ""the"", ""mat""]
reference = [""the"", ""cat"", ""is"", ""sitting"", ""on"", ""the"", ""mat""]

align_words = ",nltk.translate.meteor_score.align_words,"expected_matches = [(0, 0), (1, 1), (2, 3), (3, 4), (4, 5), (5, 6)]
matches, unmatched_hypo, unmatched_ref = align_words(hypothesis, reference)
assert matches == expected_matches
assert unmatched_hypo == []
assert unmatched_ref == [(2, 'is')]",,2022-02,,,
112,nltk,3.7,examples,New feature or additional dependency based change,Get examples of a synset from WordNet.,"import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import wordnet

synset = wordnet.synset('dog.n.01')

examples = ",synset.examples(),"assert isinstance(examples, list)
assert examples == ['the dog barked all night']",,2022-02,,,
113,nltk,3.7,fromstring,New feature or additional dependency based change,Parse a string representation of a tree into an NLTK Tree object.,"import nltk
nltk.download('sinica_treebank')
from nltk.tree import Tree
from nltk.corpus import sinica_treebank

sinica_sentence = sinica_treebank.parsed_sents()[0]
tree_string = sinica_sentence.pformat()

parsed_tree = ",Tree.fromstring(tree_string),"assert isinstance(parsed_tree, Tree)
assert parsed_tree.label() == ""NP""",,2022-02,,,
114,nltk,3.5,accumulate,Semantics or Function Behaviour change,Accumulate the results of applying a function to elements of an iterable.,"from nltk.lm.api import accumulate
import operator

iterable = [1, 2, 3, 4, 5]
func = operator.add

result = list(","accumulate(iterable, func))","assert result == [1, 3, 6, 10, 15]",,2020-04,,,
115,nltk,3.5,tokenize,New feature or additional dependency based change,Tokenize a string,"import nltk.tokenize.destructive

s = ""This is a test sentence.""
tokens = nltk",.tokenize.destructive.NLTKWordTokenizer().tokenize(s),"assert isinstance(tokens, list)
assert tokens == [""This"", ""is"", ""a"", ""test"", ""sentence"", "".""]",,2020-04,,,
116,django,5.0.0,utils.timezone.utc,Function Name Change,"
Define time zone settings to utc for the given datetime, store in a variable named utc_time. If needed, use another library","
import django
from django.conf import settings
from django.utils import timezone

settings.configure()

year = 2024
month = 11
day = 5
","
from datetime import timezone as py_timezone
utc_time = timezone.datetime(year, month, day, tzinfo=py_timezone.utc)
","
assertion_value = utc_time.tzname() == 'UTC'
assert assertion_value
assertion_value = utc_time.isoformat() == '2024-11-05T00:00:00+00:00'
assert assertion_value
",,Dec 4 2023,,1.0,
117,django,4.0.0,utils.timezone.utc,Function Name Change,"
Define time zone settings to utc for the given datetime, store in a variable named utc_time. If needed, use another library","
import django
from django.conf import settings
from django.utils import timezone

settings.configure()

year = 2024
month = 11
day = 5
","
utc_time = timezone.datetime(year, month, day, tzinfo=timezone.utc)
","
assertion_value =  utc_time.tzname() == 'UTC'
assert assertion_value
assertion_value =  utc_time.isoformat() == '2024-11-05T00:00:00+00:00'
assert assertion_value
",,Dec 7 2021,,1.0,
118,django,4.0.0,BaseModelFormSet.save_existing,Argument or Attribute Change,"
Calls save_existing on the formset instance using keyword arguments","
from django.conf import settings
from django.forms.models import BaseModelFormSet
from django.forms.renderers import get_default_renderer


settings.configure()

class DummyForm:
    def save(self, commit=True):
        return 'dummy_instance_value_result'

class MyFormSet(BaseModelFormSet):
    def __init__(self, *args, **kwargs):
        self.renderer = get_default_renderer()
        super().__init__(*args, **kwargs)


to_save = {
    'form':DummyForm(),
    'instance':'dummy_instance_value'
}
fs5 = MyFormSet(queryset=[])
result = fs5.save_existing(form=to_save['form'],","instance=to_save['instance'])
","
assertion_result = result == 'dummy_instance_value_result'
assert assertion_result
",,Dec 7 2021,,2.0,
119,django,5.0.0,BaseModelFormSet.save_existing,Argument or Attribute Change,"
Calls save_existing on the formset instance using keyword arguments","
from django.conf import settings
from django.forms.models import BaseModelFormSet
from django.forms.renderers import get_default_renderer


settings.configure()

class DummyForm:
    def save(self, commit=True):
        return 'dummy_instance_value_result'

class MyFormSet(BaseModelFormSet):
    def __init__(self, *args, **kwargs):
        self.renderer = get_default_renderer()
        super().__init__(*args, **kwargs)


to_save = {
    'form':DummyForm(),
    'instance':'dummy_instance_value'
}
fs5 = MyFormSet(queryset=[])
result = fs5.save_existing(form=to_save['form'],","obj=to_save['instance'])
","
assertion_result = result == 'dummy_instance_value_result'
assert assertion_result",,Dec 4 2023,,2.0,
120,django,4.0.0,Field.db_default,New Feature or Additional Dependency-Based Change,"
Set the default value of a field to the current time and the circumference to 2 * pi given the create_default_objects function 
","
import django
from django.conf import settings
from django.db import models, connection
from django.db.models.functions import Now, Pi

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()
def create_default_objects(MyModel):
    obj = MyModel.objects.create()
    obj.refresh_from_db()
    return obj

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    # creation_time is set to the current time when the object is created
    # circumference is set to 2 * pi
    creation_time, circumference =","(models.DateTimeField(default=Now()), models.FloatField(default= 2 * Pi()))","
try: 
    with connection.schema_editor() as schema_editor:
        schema_editor.create_model(MyModel)
except Exception as e:
    pass

import time
import math
obj = create_default_objects(MyModel)
t0 = obj.creation_time
print('circumference:', obj.circumference)
time.sleep(1)
obj = create_default_objects(MyModel)
t1 = obj.creation_time
assertion_result = t0 < t1
assert assertion_result
assertion_results = obj.circumference == 2 * math.pi
assert assertion_results
",,Dec 7 2021,,2.0,
121,django,5.0.0,Field.db_default,New Feature or Additional Dependency-Based Change,"
Set the default value of a field to the current time and the circumference to 2 * pi given the create_default_objects function 
","
import django
from django.conf import settings
from django.db import models, connection
from django.db.models.functions import Now, Pi

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()

def create_default_objects(MyModel):
    obj = MyModel.objects.create()
    return obj

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    creation_time,circumference = ","models.DateTimeField(db_default=Now()), models.FloatField(db_default=2 * Pi())","
try: 
    with connection.schema_editor() as schema_editor:
        schema_editor.create_model(MyModel)
except Exception as e:
    pass

import time
import math
obj = create_default_objects(MyModel)
t0 = obj.creation_time
print('circumference:', obj.circumference)
time.sleep(1)
obj = create_default_objects(MyModel)
t1 = obj.creation_time

assertion_result = t0 < t1
assert assertion_result
assertion_results = obj.circumference == 2 * math.pi
assert assertion_results
",,Dec 4 2023,,2.0,
122,django,5.0.0,BoundField.as_field_group(),New Feature or Additional Dependency-Based Change,"
Adapt the template_string variable to form the html string provided in comments, making the best use of the templates for form field rendering
","
import django
from django.conf import settings
from django import forms
from django.template import Template, Context

settings.configure(
      TEMPLATES=[
          {
              'BACKEND': 'django.template.backends.django.DjangoTemplates',
          },
      ],
  )
django.setup()

class SampleForm(forms.Form):
    name = forms.CharField(label='Name', help_text='Enter your name')

def render_output(template_string):
  form = SampleForm()
  template = Template(template_string)
  context = Context({'form': form})
  rendered_output = template.render(context)
  return rendered_output

template_string = '''","
<form>
  <div>
    {{ form.name.as_field_group }}
  </div>
</form>
'''","
rendered_output = render_output(template_string)
def normalize_html(html):
    # Remove all whitespace and standardize quotation marks to single quotes
    normalized = ''.join(html.split())
    return normalized

template_string_django_4 = '''
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''
assertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))
assert assertion_result
assertion_result = len(template_string) < 100 # check if the template_string is not too long (ideally should be 278)
assert assertion_result
",,Dec 4 2023,,2.0,
123,django,4.0.0,BoundField.as_field_group(),New Feature or Additional Dependency-Based Change,"
Adapt the template_string variable to form the html string provided in comments, making the best use of the templates for form field rendering
","
import django
from django.conf import settings
from django import forms
from django.template import Template, Context

settings.configure(
      TEMPLATES=[
          {
              'BACKEND': 'django.template.backends.django.DjangoTemplates',
          },
      ],
  )
django.setup()

class SampleForm(forms.Form):
    name = forms.CharField(label='Name', help_text='Enter your name')

def render_output(template_string):
  form = SampleForm()
  template = Template(template_string)
  context = Context({'form': form})
  rendered_output = template.render(context)
  return rendered_output

template_string = '''","
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''","
rendered_output = render_output(template_string)
def normalize_html(html):
    # Remove all whitespace and standardize quotation marks to single quotes
    normalized = ''.join(html.split())
    return normalized

template_string_django_4 = '''
<form>
  <div>
    {{ form.name.label_tag }}
    {% if form.name.help_text %}
      <div class=""helptext"" id=""{{ form.name.auto_id }}_helptext"">
        {{ form.name.help_text|safe }}
      </div>
    {% endif %}
    {{ form.name.errors }}
    {{ form.name }}
  </div>
</form>
'''
assertion_result = normalize_html(rendered_output) == normalize_html(render_output(template_string))
assert assertion_result
assertion_result = len(template_string) < 300 # check if the template_string is not too long (ideally should be 278)
assert assertion_result
",,Dec 7 2021,,2.0,
124,django,4.0.0,BoundField.models.GeneratedField(),New Feature or Additional Dependency-Based Change,"
Create Square model with a side field and an area field that is calculated as the square of the side.
","
import django
from django.conf import settings
from django.db import models, connection
from django.db.models import F

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()


def display_side_and_area(square):
    return square.side, square.area

def create_square(side):
    square = Square.objects.create(side=side)
    square.refresh_from_db()
    return square

class Square(models.Model):
    class Meta:
        app_label = 'myapp'
    side = models.IntegerField()
    area = ","models.BigIntegerField(editable=False)

    def save(self, *args, **kwargs):
        # Compute the area before saving.
        self.area = self.side * self.side
        super().save(*args, **kwargs)

","
with connection.schema_editor() as schema_editor:
    schema_editor.create_model(Square)
square = create_square(side=5)
correct_result = (5, 25)
assert display_side_and_area(square) == correct_result
",,Dec 7 2021,,2.0,
125,django,5.0.0,BoundField.models.GeneratedField(),New Feature or Additional Dependency-Based Change,"
Create Square model with a side field and an area field that is calculated as the square of the side.
","
import django
from django.conf import settings
from django.db import models, connection
from django.db.models import F

settings.configure(
    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},
)
django.setup()


def display_side_and_area(square):
    return square.side, square.area

def create_square(side):
    square = Square.objects.create(side=side)
    square.refresh_from_db()
    return square

class Square(models.Model):
    class Meta:
        app_label = 'myapp'
    side = models.IntegerField()
    area = ","models.GeneratedField(
        expression=F('side') * F('side'),
        output_field=models.BigIntegerField(),
        db_persist=True,
    )
","
with connection.schema_editor() as schema_editor:
    schema_editor.create_model(Square)
square = create_square(side=5)
correct_result = (5, 25)
assert display_side_and_area(square) == correct_result
",,Dec 4 2023,,2.0,
126,django,5.0.0,Field.choices,Semantics or Function Behavior Change,"
create a model based on the given color choices","
import django
from django.conf import settings
from django.db import models

settings.configure()
django.setup()

color = models.TextChoices('Color', 'RED GREEN BLUE')

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    color = models.CharField(max_length=5,"," choices=color)
    
","
class MyModelCorrect(models.Model):
    color = models.CharField(max_length=5, choices=color)
    
    class Meta:
        app_label = 'myapp'

field_choices = list(MyModel._meta.get_field('color').choices)

expected_choices = list(MyModelCorrect._meta.get_field('color').choices)

assert field_choices == expected_choices
",,Dec 4 2023,,2.0,
127,django,4.0.0,Field.choices,Semantics or Function Behavior Change,"
create a model based on the given color choices","
import django
from django.conf import settings
from django.db import models

settings.configure()
django.setup()

color = models.TextChoices('Color', 'RED GREEN BLUE')

class MyModel(models.Model):
    class Meta:
        app_label = 'myapp'
    color = models.CharField(max_length=5,"," choices=color.choices)
","
class MyModelCorrect(models.Model):
    color = models.CharField(max_length=5, choices=color.choices)
    
    class Meta:
        app_label = 'myapp'

field_choices = MyModel._meta.get_field('color').choices

expected_choices = list(MyModelCorrect._meta.get_field('color').choices)

assert field_choices == expected_choices
",,Dec 7 2021,,2.0,
128,scipy,1.7.3,distance.wminkowski,"
Function Name Change
","Compute the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w, and store the results in a variable named output","
from scipy.spatial import distance
import numpy as np 
u = np.asarray([11,12,13,14,15])
v = np.asarray([1,2,3,4,5])
w = np.asarray([0.1,0.3,0.15,0.25,0.2])","
output = distance.wminkowski(u,v,3,w)","
assertion_value   = np.allclose(output, distance.wminkowski(u,v,3,w))
assert assertion_value",,"Jun 19, 2021",,1.0,
129,scipy,1.9.2,distance.minkowski,"
Function Name Change
","Compute the weigthed Minkowski distance between two 1-D arrays, u and v, based on a 1D weigth vector, w, and store the results in a variable named output","
from scipy.spatial import distance
import numpy as np 
u = np.asarray([11,12,13,14,15])
v = np.asarray([1,2,3,4,5])
w = np.asarray([0.1,0.3,0.15,0.25,0.2])","
output = distance.minkowski(u,v,3,w=w)","
assertion_value = np.allclose(output, distance.minkowski(u,v,3,w=w))
assert assertion_value",,"Feb 4, 2022",,1.0,
130,scipy,1.8.1,linalg.expm,"
Semantics or Function Behavior Change","
Compute the matrix exponential of batched matrices, stored in a nd-array A, and store the results in a variable named output","
from scipy import linalg
import numpy as np 
A = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],
        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],
        [0.92575218, 0.66167048, 0.81779481, 0.15405207],
        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],
       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],
        [0.11275151, 0.95192602, 0.58950113, 0.78663422],
        [0.64955361, 0.47670695, 0.96824964, 0.74915994],
        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],
       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],
        [0.78721405, 0.09705598, 0.88774379, 0.81623277],
        [0.64821764, 0.62400451, 0.53916194, 0.96522881],
        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])","
output = np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = linalg.expm(A[i])","
assertion_value = np.allclose(output, np.stack([linalg.expm(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Feb 4, 2022",,1.0,
131,scipy,1.9.2,linalg.expm,"
Semantics or Function Behavior Change","
Compute the matrix exponential of batched matrices, stored in a nd-array A, and store the results in a variable named output","
from scipy import linalg
import numpy as np 
A = np.array([[[0.25264461, 0.67582554, 0.90718149, 0.65460219],
        [0.58271792, 0.4600052 , 0.22265374, 0.98210688],
        [0.92575218, 0.66167048, 0.81779481, 0.15405207],
        [0.00820708, 0.7702345 , 0.4285001 , 0.87567275]],
       [[0.48362533, 0.10258182, 0.58965127, 0.89320413],
        [0.11275151, 0.95192602, 0.58950113, 0.78663422],
        [0.64955361, 0.47670695, 0.96824964, 0.74915994],
        [0.71266875, 0.27280891, 0.1771122 , 0.45839236]],
       [[0.96116073, 0.11138203, 0.59254915, 0.92860822],
        [0.78721405, 0.09705598, 0.88774379, 0.81623277],
        [0.64821764, 0.62400451, 0.53916194, 0.96522881],
        [0.68958095, 0.86514529, 0.41583035, 0.84209827]]])","
output = linalg.expm(A)","
assert np.allclose(output, linalg.expm(A))",,"Jul 28, 2022",,1.0,
132,scipy,1.8.1,stats.combine_pvalues,"
Semantics or Function Behavior Change","
Combine the p values from various independent tests stored in a 1D array, p_vals,
 using pearson method, make sure that higher values of the statistic now correspond to lower
  p-values and store the results in a variable named output","
from scipy import stats
import numpy as np 
A = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,
       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])","
output = stats.combine_pvalues(A,'pearson')
output = -output[0], 1-output[1]","
assertion_value =  np.allclose(np.asarray(output),np.asarray([-stats.combine_pvalues(1-A,'fisher')[0],(1-stats.combine_pvalues(1-A,'fisher')[1])]))
assert assertion_value",,"Feb 4, 2022",,1.0,
133,scipy,1.9.2,stats.combine_pvalues,"
Semantics or Function Behavior Change","
Combine the p values from various independent tests stored in a 1D array, p_vals,
 using pearson method, make sure that higher values of the statistic now correspond to lower
  p-values, and store the results in a variable named output","
from scipy import stats
import numpy as np 
A = np.array([0.01995382, 0.1906752 , 0.71157923, 0.44477942, 0.4535412 ,
       0.67556953, 0.11174941, 0.85494112, 0.33214635, 0.19103228])","
output = stats.combine_pvalues(A,'pearson')","
assertion_value = np.allclose(np.asarray(output),np.asarray([-stats.combine_pvalues(1-A,'fisher')[0],(1-stats.combine_pvalues(1-A,'fisher')[1])]))
assert assertion_value",,"Jul 28, 2022",,1.0,
134,scipy,1.8.1,linalg,"
Semantics or Function Behavior Change","
Compute the matrix exponential of a sparse array A","
from scipy import sparse,linalg
import numpy as np 
A = sparse.lil_matrix((3, 3))
A[0, 0] = 4
A[1, 1] = 5
A[1, 2] = 6
output = ",linalg.expm(A),"
assertion_value = np.allclose(output.todense(), linalg.expm(A).todense())
assert assertion_value",,"Feb 4, 2022",,1.0,
135,scipy,1.9.2,stats.combine_pvalues,"
Semantics or Function Behavior Change","
Compute the matrix exponential of a sparse array A","
from scipy import sparse,linalg
import numpy as np 
A = sparse.lil_matrix((3, 3))
A[0, 0] = 4
A[1, 1] = 5
A[1, 2] = 6
output = ",sparse.linalg.expm(A),"
assertion_value = np.allclose(output.todense(), sparse.linalg.expm(A).todense())
assert assertion_value",,"Jul 28, 2022",,1.0,
136,scipy,1.8.1,stats.circvar,"
Semantics or Function Behavior Change","
Compute the circular variance: 1-R, where R is the mean resultant vector. Store the results in a variable named output","
from scipy import stats
import numpy as np 
a = np.array([0, 2*np.pi/3, 5*np.pi/3])
","
output = 1-np.abs(np.mean(np.exp(1j*a)))","
assertion_value = np.allclose(output,1-np.abs(np.mean(np.exp(1j*a))))
assert assertion_value",,"Feb 4, 2022",,1.0,
137,scipy,1.9.2,stats.circvar,"
Semantics or Function Behavior Change","
Compute the circular variance: 1-R, where R is the mean resultant vector. Store the results in a variable named output","
from scipy import stats
import numpy as np 
a = np.array([0, 2*np.pi/3, 5*np.pi/3])
","
output = stats.circvar(a)","
assertion_value = np.allclose(output,1-np.abs(np.mean(np.exp(1j*a))))
assert assertion_value ",,"Jul 28, 2022",,1.0,
138,scipy,1.11.2,rv_continuous.momentr,"
Argument or Attribute Change","
 Store the results in a variable named output","
from scipy.stats import norm
dist = norm(15, 10)
n=5
","
output = dist.moment(order=n)","
import numpy as np
assertion_value = np.allclose(output,dist.moment(order=n))
assert assertion_value",,"Jun 25, 2023",,1.0,
139,scipy,1.9.2,rv_continuous.moment,"
Argument or Attribute Change","
 Store the results in a variable named output","
from scipy.stats import norm
dist = norm(15, 10)
n=5
","
output = dist.moment(n=n)","
import numpy as np
assertion_value = np.allclose(output,dist.moment(order=n))
assert assertion_value",,"Jul 28, 2022",,1.0,
140,scipy,1.9.2,scipy.linalg.det,"
Semantics or Function Behavior Change","
Compute the determinant of batched matrices (batched in the first dimention), store the results in a variable named output","
from scipy.linalg import det
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
","
output = np.zeros(A.shape[0])
for i in range(A.shape[0]):
    output[i] = det(A[i])","
assertion_value=np.allclose(output, np.stack([det(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
141,scipy,1.11.2,scipy.linalg.det,"
Semantics or Function Behavior Change","
Compute the determinant of batched matrices (batched in the first dimention), store the results in a variable named output","
from scipy.linalg import det
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
","
output = det(A)","
assertion_value = np.allclose(output, det(A))
assert assertion_value",,"Jun 25, 2023",,1.0,
142,scipy,1.9.2,scipy.linalg.lu,"
Semantics or Function Behavior Change","
Compute the lu decomposition of batched square matrices (batched in the first dimention)","
from scipy.linalg import lu
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
p,u,v ="," [np.zeros(A.shape) for i in range(3)]
for i in range(A.shape[0]):
    p[i],u[i],v[i] = lu(A[i])","
assertion_value = np.allclose(np.stack([p,u,v],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
143,scipy,1.11.2,scipy.linalg.lu,"
Semantics or Function Behavior Change","
Compute the lu decomposition of batched square matrices (batched in the first dimention)","
from scipy.linalg import lu
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
p,u,v =", lu(A),"
assertion_value = np.allclose(np.stack([p,u,v],axis=1) ,np.stack([lu(A[i]) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
144,scipy,1.11.2,signal.windows.lanczos,"
New Feature or Additional Dependency-Based Change","
Using scipy if possible, create a lanczos windows","
import scipy.signal.windows as windows
import numpy as np
window_size=31
window = ",windows.lanczos(window_size),"
window_numpy = 2*np.arange(window_size)/(window_size-1) - 1 
window_numpy = np.sinc(window_numpy)
window_numpy = window_numpy / np.max(window_numpy)
assertion_value = np.allclose(window,window_numpy)
assert assertion_value",-,"Jan 3, 2023",,1.0,
145,scipy,1.9.2,signal.windows.lanczos,"
New Feature or Additional Dependency-Based Change","
Using scipy if possible, create a lanczos windows","
import scipy.signal.windows as windows
import numpy as np
window_size=31
window = ","2*np.arange(window_size)/(window_size-1) - 1 
window = np.sinc(window)
window = window / np.max(window)","
window_numpy = 2*np.arange(window_size)/(window_size-1) - 1 
window_numpy = np.sinc(window_numpy)
window_numpy =   window_numpy / np.max(window_numpy)
assertion_value = np.allclose(window,window_numpy)
assert assertion_value",-,"Jul 28, 2022",,1.0,
146,scipy,1.10.2,ndimage.gaussian_filter1d,"
Argument or Attribute Change","
Apply a 1D gaussian filter","
from scipy.ndimage import gaussian_filter1d
import numpy as np
x = np.random.rand(100)
radius = 10
sigma= np.pi
output = ","gaussian_filter1d(x, radius=radius, sigma=sigma)","
assertion_value = np.allclose(output,gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma))
assert assertion_value",-,"Jan 3, 2023",,1.0,
147,scipy,1.9.2,ndimage.gaussian_filter1d,"
Argument or Attribute Change","
Apply a 1D gaussian filter","
from scipy.ndimage import gaussian_filter1d
import numpy as np
x = np.random.rand(100)
radius = 10
sigma= np.pi
output = ","gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma)","
assertion_value = np.allclose(output,gaussian_filter1d(x, truncate = radius/sigma,sigma=sigma))
assert assertion_value ",-,"Jul 28, 2022",,1.0,
148,scipy,1.11.2,scipy.ndimage.rank_filter,"
Semantics or Function Behavior Change","
Apply a rank filter on batched images (batched in the first dimention)","
from scipy.ndimage import rank_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
rank = 6
size=3
output =","rank_filter(A,rank,size=size,axes=[1,2])","
assertion_value = np.allclose(output, np.stack([rank_filter(A[i],rank,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
149,scipy,1.9.2,scipy.ndimage.rank_filter,"
Semantics or Function Behavior Change","
Apply a rank filter on batched images (batched in the first dimention)","
from scipy.ndimage import rank_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
rank = 6
size = 3
output =","np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = rank_filter(A[i],rank,size=size)","
assertion_value = np.allclose(output, np.stack([rank_filter(A[i],rank,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
150,scipy,1.11.2,scipy.ndimage.percentile_filter,"
Semantics or Function Behavior Change","
Apply a percentile filter on batched images (batched in the first dimention)","
from scipy.ndimage import percentile_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
percentile = 90
size=3
output =","percentile_filter(A,percentile=percentile,size=size,axes=[1,2])","
assertion_value = np.allclose(output, np.stack([percentile_filter(A[i],percentile=percentile,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
151,scipy,1.9.2,scipy.ndimage.percentile_filter,"
Semantics or Function Behavior Change","
Apply a percentile filter on batched images (batched in the first dimention)","
from scipy.ndimage import percentile_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
percentile = 90
size=3
output =","np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] = percentile_filter(A[i],percentile=percentile,size=size)","
assertion_value = np.allclose(output, np.stack([percentile_filter(A[i],percentile=percentile,size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
152,scipy,1.11.2,scipy.ndimage.median_filter,"
Semantics or Function Behavior Change","
Apply a median filter on batched images (batched in the first dimention)","
from scipy.ndimage import median_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","median_filter(A,size=size,axes=[1,2])","
assertion_value = np.allclose(output, np.stack([ median_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
153,scipy,1.9.2,scipy.ndimage.median_filter,"
Semantics or Function Behavior Change","
Apply a median filter on batched images (batched in the first dimention)","
from scipy.ndimage import median_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  median_filter(A[i], size=size)","
assertion_value = np.allclose(output, np.stack([ median_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
154,scipy,1.11.2,scipy.ndimage.uniform_filter,"
Semantics or Function Behavior Change","
Apply a uniform filter on batched images (batched in the first dimention)","
from scipy.ndimage import uniform_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","uniform_filter(A,size=size,axes=[1,2])","
assertion_value = np.allclose(output, np.stack([ uniform_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
155,scipy,1.9.2,scipy.ndimage.uniform_filter,"
Semantics or Function Behavior Change","
Apply a uniform filter on batched images (batched in the first dimention)","
from scipy.ndimage import uniform_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  uniform_filter(A[i], size=size)","
assertion_value = np.allclose(output, np.stack([uniform_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
156,scipy,1.11.2,scipy.ndimage.minimum_filter,"
Semantics or Function Behavior Change","
Apply a minimum filter on batched images (batched in the first dimention)","
from scipy.ndimage import minimum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","minimum_filter(A,size=size,axes=[1,2])","
assertion_value = np.allclose(output, np.stack([ minimum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
157,scipy,1.9.2,scipy.ndimage.minimum_filter,"
Semantics or Function Behavior Change","
Apply a minimum filter on batched images (batched in the first dimention)","
from scipy.ndimage import minimum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  minimum_filter(A[i], size=size)","
assertion_value= np.allclose(output, np.stack([minimum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
158,scipy,1.11.2,scipy.ndimage.maximum_filter,"
Semantics or Function Behavior Change","
Apply a maximum filter on batched images (batched in the first dimention)","
from scipy.ndimage import maximum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","maximum_filter(A,size=size,axes=[1,2])","
assertion_value = np.allclose(output, np.stack([ maximum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
159,scipy,1.9.2,scipy.ndimage.maximum_filter,"
Semantics or Function Behavior Change","
Apply a maximum filter on batched images (batched in the first dimention)","
from scipy.ndimage import maximum_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
size=3
output =","np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  maximum_filter(A[i], size=size)","
assertion_value = np.allclose(output, np.stack([ maximum_filter(A[i], size=size) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
160,scipy,1.11.2,scipy.ndimage.gaussian_filter,"
Semantics or Function Behavior Change","
Apply a gaussian filter on batched images (batched in the first dimention)","
from scipy.ndimage import gaussian_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
sigma=2
output =","gaussian_filter(A,sigma=sigma,axes=[1,2])","
assertion_value = np.allclose(output, np.stack([ gaussian_filter(A[i],sigma=sigma) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jun 25, 2023",,1.0,
161,scipy,1.9.2,scipy.ndimage.gaussian_filter,"
Semantics or Function Behavior Change","
Apply a gaussian filter on batched images (batched in the first dimention)","
from scipy.ndimage import gaussian_filter
import numpy as np 
A = np.array([[[7.81411439e-01, 1.12331105e-02, 9.39679607e-01, 6.42515536e-01,
         6.95548884e-01],
        [4.58001321e-01, 7.90049557e-01, 7.41113873e-01, 9.85894466e-03,
         4.53976568e-01],
        [2.24634401e-01, 4.26973301e-01, 8.91014236e-01, 8.23895514e-01,
         9.31046059e-01],
        [4.45855698e-02, 2.58723740e-02, 3.84879860e-01, 5.52190839e-01,
         8.30264909e-02],
        [9.15445530e-01, 8.36618255e-02, 4.39455188e-01, 1.53462390e-01,
         7.26436401e-01]],

       [[7.28031138e-01, 3.75117422e-01, 5.42310487e-01, 5.63557751e-01,
         2.57909701e-04],
        [7.50931873e-01, 6.15538706e-01, 9.05514875e-01, 8.48424138e-01,
         3.10902901e-01],
        [1.72613108e-01, 7.06317645e-01, 8.89355115e-01, 8.90701413e-02,
         9.28579120e-01],
        [9.80044173e-01, 1.67306937e-01, 1.69314197e-01, 5.09876755e-01,
         7.74652232e-01],
        [5.31016205e-01, 6.71446481e-01, 8.29882594e-02, 1.70589945e-01,
         3.23593050e-01]],

       [[8.38046468e-01, 7.96708042e-03, 9.28662972e-01, 6.75111182e-01,
         3.33247539e-02],
        [9.99455965e-01, 9.41123427e-01, 1.06977786e-01, 1.73786082e-01,
         5.95914076e-01],
        [5.46093501e-01, 4.99303483e-01, 4.60402210e-01, 4.75650259e-02,
         4.58097639e-01],
        [5.15319589e-01, 2.31310779e-01, 1.93670305e-01, 9.76320068e-01,
         8.78819966e-01],
        [1.09356481e-01, 2.16028113e-01, 1.67111833e-01, 9.65726787e-01,
         6.75017369e-01]]])
sigma=2
output =","np.zeros(A.shape)
for i in range(A.shape[0]):
    output[i] =  gaussian_filter(A[i],sigma=sigma)","
assertion_value = np.allclose(output, np.stack([gaussian_filter(A[i],sigma=sigma) for i in range(A.shape[0])],axis=0))
assert assertion_value",,"Jul 28, 2022",,1.0,
162,flask,2.0.0,app.json_encoder,Semantics or Function Behavior Change,"
Complete the app set-up for the json encoding to return a sorted list
 when called with the eval function provided with the variable 
 num_set being a set of numbers, use other libraries if needed","
import flask

app = flask.Flask('test')
@app.route('/data')
def data(num_set):
    return flask.jsonify({'numbers': num_set})

def eval(app, data_fn, num_set):
    with app.test_request_context():
        response = data_fn(num_set)
        return response.get_data(as_text=False)

","
import json
class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app.json_encoder = MyCustomJSONHandler
","
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_set):
    return flask.jsonify({'numbers': num_set})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})
assert assertion_result
",werkzeug==2.0.0,2021-05-11,,2.0,
163,flask,3.0.0,app.json_encoder,Semantics or Function Behavior Change,"
Complete the app set-up for the json encoding to return a sorted list
 when called with the eval function provided with the variable 
 num_set being a set of numbers, use other libraries if needed","
import flask

app = flask.Flask('test')
@app.route('/data')
def data(num_set):
    return flask.jsonify({'numbers':num_set})

def eval(app, data_fn, num_set):
    with app.test_request_context():
        response = data_fn(num_set)
        return response.get_data(as_text=True)

","
class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
","
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_set):
    return flask.jsonify({'numbers': num_set})
class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, set):
            return sorted(list(obj))
        return super().default(obj)
app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_result = eval(app2, data2, {3, 1, 2, 6, 5, 4}) == eval(app, data, {3, 1, 2, 6, 5, 4})
assert assertion_result
",,2023-09-30,,2.0,
164,flask,2.0.0,flask.send_file,Argument or Attribute Change,"
Complete the download definition to download the data in the variable attachment_filename","
from flask import Flask, send_file
from io import BytesIO

app1 = Flask(__name__)

def get_content_disp(app, download_fn):
    with app.test_request_context():
        response = download_fn()
    content_disp = response.headers.get('Content-Disposition')
    return content_disp

@app1.route('/download')
def download():
    data = BytesIO(b'Hello, World!')
    attachment_filename = 'hello.txt'
    return send_file(data, as_attachment=True,
","attachment_filename=attachment_filename)
","
content_disp = get_content_disp(app1, download)
assertion_result = 'filename=hello.txt' in content_disp
assert assertion_result
",werkzeug==2.0.0,2021-05-11,,1.0,
165,flask,3.0.0,flask.send_file,Argument or Attribute Change,"
Complete the download definition to download the data in the variable attachment_filename","
from flask import Flask, send_file
from io import BytesIO

app1 = Flask(__name__)

def get_content_disp(app, download_fn):
    with app.test_request_context():
        response = download_fn()
    content_disp = response.headers.get('Content-Disposition')
    return content_disp

@app1.route('/download')
def download():
    data = BytesIO(b'Hello, World!')
    attachment_filename = 'hello.txt'
    return send_file(data, as_attachment=True,

","download_name=attachment_filename)
","
content_disp = get_content_disp(app1, download)
assertion_result = 'filename=hello.txt' in content_disp
assert assertion_result
",,2023-09-30,,1.0,
166,flask,2.0.1,Flask.config.from_json,Function Name Change,"
Load the json file to config the Flask app","
import json
import tempfile
from flask import Flask

config_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}
with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:
    json.dump(config_data, tmp)
    tmp.flush()
    config_file = tmp.name

app = Flask(__name__)
app.config.","from_json(config_file)
","
assertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'
assert assertion_result",werkzeug==2.0.0,2021-05-11,,1.0,
167,flask,3.0.0,Flask.config.from_file,Function Name Change,"
Load the json file to config the Flask app","
import json
import tempfile
from flask import Flask

config_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}
with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:
    json.dump(config_data, tmp)
    tmp.flush()
    config_file = tmp.name

app = Flask(__name__)
app.config.","from_file(config_file, load=json.load)
","
assertion_result= app.config['DEBUG'] is True and app.config['SECRET_KEY'] == 'secret'
assert assertion_result
",,2023-09-30,,1.0,
168,flask,2.0.1,flask.safe_join,Function Name Change,"
Complete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed","
import flask
import werkzeug

error404 = werkzeug.exceptions.NotFound

def safe_join_fail_404(base_path,sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
","
    joined = flask.safe_join(base_path, sub_path)

    return joined
","
base_path = '/var/www/myapp'
sub_path = '../secret.txt'

try : 
    joined = safe_join_fail_404(base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined = safe_join_fail_404(base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt'
assert assertion_result",werkzeug==2.0.0,2021-05-11,,1.0,
169,flask,3.0.0,flask.safe_join,Function Name Change,"
Complete the safe_join_fail_404 to safely join the path and the subpath, use other libraries if needed","
import flask
import werkzeug

error404 = werkzeug.exceptions.NotFound

def safe_join_fail_404(base_path,sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
","
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    return joined
","
base_path = '/var/www/myapp'
sub_path = '../secret.txt'

try : 
    joined = safe_join_fail_404(base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined = safe_join_fail_404(base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt'
assert assertion_result",,2023-09-30,,1.0,
170,flask,2.0.1,helpers.total_seconds,Function Name Change,"
Complete the function convert_timedelta_to_seconds, use other libraries if needed","
import flask

def convert_timedelta_to_seconds(td):","
    return flask.helpers.total_seconds(td)
","
import datetime
td = datetime.timedelta(hours=2, minutes=30,microseconds=1)
assertion_results = convert_timedelta_to_seconds(td)==9000
assert assertion_results",werkzeug==2.0.0,2021-05-11,,1.0,
171,flask,3.0.0,helpers.total_seconds,Function Name Change,"
Complete the function convert_timedelta_to_seconds, use other libraries if needed","
import flask

def convert_timedelta_to_seconds(td):","
    return td.total_seconds()
","
import datetime
td = datetime.timedelta(hours=2, minutes=30,microseconds=1)
assertion_results = convert_timedelta_to_seconds(td)==9000.000001
assert assertion_results",,2023-09-30,,1.0,
172,jinja2,2.11,jinja2.contextfilter, Function Name Change,"
Create a custom filter named greet. This filter should: 
accept the context and a parameter name, 
retrieve a variable prefix from the context (defaulting to 'Hello' if not provided), 
return a greeting message combining the prefix and the name.","
import jinja2 
def setup_environment(filtername,filter):
    env = jinja2.Environment()
    env.filters[filtername] = filter
    return env

@","jinja2.contextfilter
def greet(ctx, name):
    prefix = ctx.get('prefix', 'Hello')
    return f'{prefix}, {name}!'

    
","
env = setup_environment('greet',greet)
template = env.from_string('''
{{ 'World'| greet }}''')
assertion_results = 'Hi, World!' in template.render(prefix='Hi')
assert assertion_results 
assertion_results = 'Hello, World!' in template.render()
assert assertion_results 
",markupsafe==2.0.1,2020-01-27,,1.0,
173,jinja2,3.1,jinja2.pass_context,Function Name Change,"
Create a custom filter named greet. This filter should: 
accept the context and a parameter name, 
retrieve a variable prefix from the context (defaulting to 'Hello' if not provided), 
return a greeting message combining the prefix and the name.","
import jinja2 
def setup_environment(filtername,filter):
    env = jinja2.Environment()
    env.filters[filtername] = filter
    return env

@","jinja2.pass_context
def greet(ctx, name):
    prefix = ctx.get('prefix', 'Hello')
    return f'{prefix}, {name}!'
    
","
env = setup_environment('greet',greet)
template = env.from_string('''
{{ 'World'| greet }}''')
assertion_results = 'Hi, World!' in template.render(prefix='Hi')
assert assertion_results 
assertion_results = 'Hello, World!' in template.render()
assert assertion_results 
",,2022-03-25,,1.0,
174,jinja2,2.11,jinja2.evalcontextfilter, Function Name Change,"
Write the nl2br function which is a custom filter that takes the evaluation context 
and a text value, then replaces every occurrence of the substring 'Hello' 
with the HTML string '<br>Hello</br>', while respecting the autoescape setting.
You can use the nl2br_core function","
import re
from jinja2 import Environment, evalcontextfilter
from markupsafe import Markup, escape

def get_output(env, filter_fn):
    env.filters['nl2br'] = filter_fn
    template = env.from_string('{{ text | nl2br }}')
    output = template.render(text='Hello World')
    return output

def nl2br_core(eval_ctx, value):
    br = '<br>Hello</br>'
    if eval_ctx.autoescape:
        value = escape(value)
        br = Markup(br)
    result = re.sub(r'Hello', br, value)
    return Markup(result) if eval_ctx.autoescape else result

@","evalcontextfilter
def nl2br(eval_ctx, value):
    return nl2br_core(eval_ctx, value)
","
env = Environment(autoescape=True)
output = get_output(env,nl2br)
expected = '<br>Hello</br> World'

assert output == expected, f'Expected: {expected!r}, but got: {output!r}'
",markupsafe==2.0.1,2020-01-27,,1.0,
175,jinja2,3.1,jinja2.pass_eval_context,Function Name Change,"
Write the nl2br function which is a custom filter that takes the evaluation context 
and a text value, then replaces every occurrence of the substring 'Hello' 
with the HTML string '<br>Hello</br>', while respecting the autoescape setting.
You can use the nl2br_core function","
import re
from jinja2 import Environment, pass_eval_context
from markupsafe import Markup, escape

def get_output(env, filter_fn):
    env.filters['nl2br'] = filter_fn
    template = env.from_string('{{ text | nl2br }}')
    output = template.render(text='Hello World')
    return output

def nl2br_core(eval_ctx, value):
    br = '<br>Hello</br>'
    if eval_ctx.autoescape:
        value = escape(value)
        br = Markup(br)
    result = re.sub(r'Hello', br, value)
    return Markup(result) if eval_ctx.autoescape else result

@","pass_eval_context
def nl2br(eval_ctx, value):
    return nl2br_core(eval_ctx, value)
","
env = Environment(autoescape=True)
output = get_output(env,nl2br)
expected = '<br>Hello</br> World'

assert output == expected, f'Expected: {expected!r}, but got: {output!r}'
",,2022-03-25,,1.0,
176,scipy,1.11.1,linalg.det,"
Semantics or Function Behavior Change
","
complete the following function that check if all the batch of matrices are invertible, using numpy 1.25.1","
import warnings
from scipy.linalg import det
import numpy as np
warnings.filterwarnings('error')

def check_invertibility(matrices):","
    return np.all(det(matrices))
","
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]]
])
assertion_value = check_invertibility(matrices)
assert assertion_value
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]],

    [[0, 0],
     [0, 0]]
])
assertion_value = not check_invertibility(matrices)
assert assertion_value",numpy==1.25.1,,,2.0,
177,scipy,1.9.1,linalg.det,"
Semantics or Function Behavior Change
","
complete the following function that check if all the batch of matrices are invertible, using numpy 1.21.6","
import warnings
from scipy.linalg import det
import numpy as np
warnings.filterwarnings('error')

def check_invertibility(matrices):","
    return np.alltrue([det(A) for A in matrices])
","
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]]
])
assertion_value = check_invertibility(matrices)
assert assertion_value
matrices = np.array([
    [[1, 2],
     [3, 4]],
    
    [[0, 1],
     [1, 0]],
    
    [[2, 0],
     [0, 2]],

    [[0, 0],
     [0, 0]]
])
assertion_value = not check_invertibility(matrices)
assert assertion_value",numpy==1.21.6,,,2.0,
178,scipy,1.11.1,stats.hmean,"
Semantics or Function Behavior Change
","
Complete the function count_unique_hmean that takes a 2D numpy array as 
input and returns the number of unique harmonic mean values across 
the rows of the array, counting each nan value as unique. We are using numpy 1.25.1","
import numpy as np
from scipy.stats import hmean

def count_unique_hmean(data):
    # data shape: (n, m)
    # n: number of arrays
    # m: number of elements in each array ","
    hmean_values = hmean(np.asarray(data), axis=1)
    unique_vals = np.unique(hmean_values, equal_nan=False).shape[0]
    return unique_vals

","
data = np.array([
    [1, 2, 3],
    [2, 2, 2],
    [1, np.nan, 3],
    [4, 5, 6],
    [np.nan, 1, np.nan],
    [1, 2, 3]
])
assertion_value = count_unique_hmean(data) == 5
assert assertion_value
",numpy==1.25.1,,,2.0,
179,scipy,1.8.1,stats.hmean,"
Semantics or Function Behavior Change
","
Complete the function count_unique_hmean that takes a 2D numpy array as 
input and returns the number of unique harmonic mean values across 
the rows of the array, counting each nan value as unique. We are using numpy 1.21.6","

import numpy as np
from scipy.stats import hmean

def count_unique_hmean(data):
    # data shape: (n, m)
    # n: number of arrays
    # m: number of elements in each array","
    hmean_values = []
    for arr in data:
        if np.isnan(arr).any():
            hm = np.nan
        else:
            hm = hmean(arr)
        hmean_values.append(hm)
    
    hmean_values = np.asarray(hmean_values)
    non_nan_vals = hmean_values[~np.isnan(hmean_values)]
    counts_non_nan = np.unique(non_nan_vals).shape[0]
    nan_count = np.sum(np.isnan(hmean_values))
    return counts_non_nan + nan_count
","

data = np.array([
    [1, 2, 3],
    [2, 2, 2],
    [1, np.nan, 3],
    [4, 5, 6],
    [np.nan, 1, np.nan],
    [1, 2, 3]
])
assertion_value = count_unique_hmean(data) == 5
assert assertion_value
",numpy==1.21.6,,,2.0,
180,scipy,1.11.1,signal.hilbert,"
Semantics or Function Behavior Change
","
Complete the function compute_hilbert_transform. We are using numpy 1.25.1","
import numpy as np
from scipy.signal import hilbert

def compute_hilbert_transform(a, b, dtype=np.float64):
    # compute_hilbert_transform should return the Hilbert transform of the
    # a and b arrays stacked vertically, with safe casting and the specified
    # dtype. 
    # raise TypeError if needed
    ","
    stacked = np.vstack((a, b), dtype=dtype,
                         casting='safe')
    return hilbert(stacked)
","

a = np.array([1.0, 2.0, 3.0], dtype=np.float32)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
assertion_value = False
try :
    compute_hilbert_transform(a, b, dtype=np.float32)
except TypeError:
    assertion_value = True
assert assertion_value
b=b.astype(np.float32)
computed = compute_hilbert_transform(a, b, dtype=np.float32)
expected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)
assert assertion_value
a=a.astype(np.float64)
b=b.astype(np.float64)
computed = compute_hilbert_transform(a, b, dtype=np.float64)
expected = expected.astype(dtype=np.complex128)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)
assert assertion_value
",numpy==1.25.1,,,2.0,
181,scipy,1.8.1,signal.hilbert,"
Semantics or Function Behavior Change
","
Complete the function compute_hilbert_transform. We are using numpy 1.25.1","
import numpy as np
from scipy.signal import hilbert

def compute_hilbert_transform(a, b, dtype=np.float64):
    # compute_hilbert_transform should return the Hilbert transform of the
    # a and b arrays stacked vertically, with safe casting and the specified
    # dtype.
    # raise TypeError if needed
    ","
    if not (np.can_cast(a.dtype, dtype, casting='safe') and np.can_cast(b.dtype, dtype, casting='safe')):
        raise TypeError('Unsafe casting from input dtype to specified dtype')
    
    a_cast = a.astype(dtype, copy=False)
    b_cast = b.astype(dtype, copy=False)
    
    stacked = np.vstack((a_cast, b_cast))
    
    result = hilbert(stacked)
    
    if dtype == np.float32:
        complex_dtype = np.complex64
    elif dtype == np.float64:
        complex_dtype = np.complex128
    else:
        complex_dtype = np.complex128

    return result.astype(complex_dtype)
    ","
a = np.array([1.0, 2.0, 3.0], dtype=np.float32)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
assertion_value = False
try :
    compute_hilbert_transform(a, b, dtype=np.float32)
except TypeError:
    assertion_value = True
assert assertion_value
b=b.astype(np.float32)
computed = compute_hilbert_transform(a, b, dtype=np.float32)
expected = hilbert(np.vstack([a.astype(np.float64), b.astype(np.float64)])).astype(dtype=np.complex64)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex64)
assert assertion_value

a=a.astype(np.float64)
b=b.astype(np.float64)
computed = compute_hilbert_transform(a, b, dtype=np.float64)
expected = expected.astype(dtype=np.complex128)
assertion_value = np.allclose(computed, expected) & (computed.dtype == np.complex128)
assert assertion_value
",numpy==1.21.6,,,2.0,
182,flask,2.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained 
in the numpy array when called, we are using numpy 1.21.6","
import flask
import json
import numpy as np
app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):","
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
","
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))
assert assertion_results
",numpy==1.21.6 werkzeug==2.0.0,2021-05-11,,2.0,
183,flask,3.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up for the json encoding to return only the unique values (each NaN being a different value) contained 
in the numpy array when called, we are using numpy 1.25.1","
import flask
import numpy as np

app = flask.Flask('test1')

@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
    ","
        if isinstance(obj, np.ndarray):
            unique_vals = np.unique(obj, equal_nan=False)
            return unique_vals.tolist()
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
","
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            n_nan = np.sum(np.isnan(obj))
            unique_vals = obj[~np.isnan(obj)]
            unique_vals = np.append(np.unique(unique_vals), [np.nan]*n_nan).tolist()
            return unique_vals
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_results = eval_app(app2, data2,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan])) == eval_app(app, data,np.array([3, 3, 1, np.nan, 2, 6, 5, np.nan]))
assert assertion_results
",numpy==1.25.1,2023-09-30,,2.0,
184,flask,2.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up for the json encoding to perform a fast copy and 
transpose when given a numpy array before flattening and converting 
the result to a list,we are using numpy 1.21.6","
import flask
import json
import numpy as np
from numpy import fastCopyAndTranspose 
app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):","
            res = fastCopyAndTranspose(obj).flatten().tolist()
            return res
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler

","
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))
assert assertion_results
",numpy==1.21.6 werkzeug==2.0.0,2021-05-11,,2.0,
185,flask,3.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up for the json encoding to perform a fast copy and 
transpose when given a numpy array before flattening and converting 
the result to a list, we are using numpy 1.25.1","
import flask
import numpy as np
import warnings
from numpy import fastCopyAndTranspose 
warnings.filterwarnings('error')

app = flask.Flask('test1')

@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):","
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
","
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = obj.T.copy().flatten().tolist()
            return res
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)

assertion_results = eval_app(app2, data2,np.array([[3, 3, 1,], [2,2,4],[1,1,1]])) == eval_app(app, data,np.array([[3, 3, 1,], [2,2,4],[1,1,1]]))
assert assertion_results
",numpy==1.25.1,2023-09-30,,2.0,
186,flask,2.0.0,flask.safe_join,Semantics or Function Behavior Change," 
Complete the stack_and_save function, we are using numpy 1.21.6","
import flask
import werkzeug
import numpy as np

error404 = werkzeug.exceptions.NotFound

def stack_and_save(arr_list,base_path,sub_path, casting_policy,out_dtype):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # stack the arrays in arr_list with the casting policy and the out_dtype.
    # if the out_dtype is not compatible with the casting policy, raise a TypeError
    # and out_dtype could be np.float32 or np.float64
    # casting policy could be safe or unsafe
    # Return the joined path and the stacked array to be saved ","
    joined = flask.safe_join(base_path, sub_path)
    casted_list = []

    for arr in arr_list:
        if not np.can_cast(arr.dtype, out_dtype, casting=casting_policy):
            raise TypeError('Cannot cast array')
        casted_list.append(arr.astype(out_dtype, copy=False))
    
    stacked = np.vstack(casted_list)
    return joined, stacked
","

base_path = '/var/www/myapp'
sub_path = '../secret.txt'
a = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
b = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)
arr_list = [a, b]
casting_policy = 'safe'
out_dtype=np.float64
stacked_correct = np.vstack(arr_list).astype(np.float64)
try : 
    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
try : 
    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except TypeError as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
casting_policy = 'unsafe'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)

assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32
assert assertion_result
",numpy==1.21.6 werkzeug==2.0.0,2021-05-11,,2.0,
187,flask,3.0.0,flask.safe_join,Semantics or Function Behavior Change," 
Complete the stack_and_save function, we are using numpy 1.25.1","
import flask
import werkzeug
import numpy as np

error404 = werkzeug.exceptions.NotFound

def stack_and_save(arr_list,base_path,sub_path, casting_policy,out_dtype):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # stack the arrays in arr_list with the casting policy and the out_dtype.
    # if the out_dtype is not compatible with the casting policy, raise a TypeError
    # and out_dtype could be np.float32 or np.float64
    # casting policy could be safe or unsafe
    # Return the joined path and the stacked array to be saved ","
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    stacked = np.vstack(arr_list,casting=casting_policy,dtype=out_dtype)
    return joined, stacked
","
base_path = '/var/www/myapp'
sub_path = '../secret.txt'


a = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
b = np.array([[7, 8, 9], [10, 11, 12]]).astype(np.float64)
arr_list = [a, b]
casting_policy = 'safe'
out_dtype=np.float64
stacked_correct = np.vstack(arr_list).astype(np.float64)
try : 
    joined, stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float64
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
try : 
    joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)
except TypeError as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

stacked_correct = stacked_correct.astype(np.float32)
out_dtype=np.float32
casting_policy = 'unsafe'
joined,stacked = stack_and_save(arr_list,base_path, sub_path,  casting_policy,out_dtype)

assertion_result = joined == '/var/www/myapp/secret.txt' and np.array_equal(stacked,stacked_correct) and stacked.dtype == np.float32
assert assertion_result
",numpy==1.25.1,2023-09-30,,2.0,
188,flask,3.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up so that, when given a batch of matrix,
the json encoding compute the determinants of each matrix, 
before flattening and converting the result to a list, we are using scipy 1.11.1","
import flask
import numpy as np
from scipy import linalg

app = flask.Flask('test1')
@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})
def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] : ","

            res = linalg.det(obj)
            return res.tolist()
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app) ","
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)
a = np.random.random((6,3,3))

assertion_results = eval_app(app2, data2,a) == eval_app(app, data,a)
assert assertion_results",scipy==1.11.1,2023-09-30,,2.0,
189,flask,2.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up so that, when given a batch of matrix,
the json encoding compute the determinants of each matrix, 
before flattening and converting the result to a list, we are using scipy 1.8.1 "," 
import flask
import json
import numpy as np
from scipy import linalg

app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :","
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
","
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :
            res = np.zeros(obj.shape[0])
            for i in range(obj.shape[0]):
                res[i] = linalg.det(obj[i])
            return res.tolist()
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
a = np.random.random((6,3,3))

assertion_results = eval(app2, data2,a) == eval(app, data,a)
assert assertion_results
",scipy==1.8.1 Werkzeug==2.0.0,2021-05-11,,2.0,
190,flask,3.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up so that, when given a batch (first dim) of values,
compute the harmonic mean along the second dimension (It should handle nan values),
before flattening and converting the result to a list, we are using scipy 1.11.1","
import flask
import numpy as np
from scipy.stats import hmean

app = flask.Flask('test1')

@app.route('/data')
def data(num_list):
    return flask.jsonify({'numbers': num_list})

def eval_app(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=True)

class MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):","
            res = hmean(obj,axis=1).tolist()
            return res
        return super().default(obj)

app.json_provider_class = MyCustomJSONHandler
app.json = app.json_provider_class(app)
","
app2 = flask.Flask('test2')

@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})

class MyCustomJSONHandler2(flask.json.provider.DefaultJSONProvider):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = hmean(obj,axis=1).tolist()
            return res
        return super().default(obj)

app2.json_provider_class = MyCustomJSONHandler2
app2.json = app2.json_provider_class(app2)
assertion_results = eval_app(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval_app(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))
assert assertion_results

",scipy==1.11.1,2023-09-30,,2.0,
191,flask,2.0.0,app.json_encoder,Semantics or Function Behavior Change," 
Complete the app set-up so that, when given a batch (first dim) of values,
compute the harmonic mean along the second dimension (It should handle nan values),
before flattening and converting the result to a list, we are using scipy 1.8.1","
import flask
import json
import numpy as np
from scipy.stats import hmean

app = flask.Flask('test1')
@app.route('/data')
def data(num_arr):
    return flask.jsonify({'numbers': num_arr})

def eval(app, data_fn, num_arr):
    with app.test_request_context():
        response = data_fn(num_arr)
        return response.get_data(as_text=False)

class MyCustomJSONHandler(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):","
            res = np.zeros((obj.shape[0],1))
            for i_arr in range(obj.shape[0]):
                if np.isnan(obj[i_arr]).any():
                    res[i_arr] = np.nan
                else:
                    res[i_arr]  = hmean(obj[i_arr])
            res = res.flatten().tolist()
            return res
        return super().default(obj)

app.json_encoder = MyCustomJSONHandler
","
app2 = flask.Flask('test2')
@app2.route('/data2')
def data2(num_arr):
    return flask.jsonify({'numbers': num_arr})
class MyCustomJSONHandler2(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            res = np.zeros((obj.shape[0],1))
            for i_arr in range(obj.shape[0]):
                if np.isnan(obj[i_arr]).any():
                    res[i_arr] = np.nan
                else:
                    res[i_arr]  = hmean(obj[i_arr])
            res = res.flatten().tolist()
            return res
        return super().default(obj)

app2.json_encoder = MyCustomJSONHandler2
assertion_results = eval(app2, data2,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]])) == eval(app, data,np.array([[3, 3, np.nan,], [np.nan,2,4],[1,2,1]]))
assert assertion_results
",scipy==1.8.1 Werkzeug==2.0.0,2021-05-11,,2.0,
192,flask,3.0.0,flask.safe_join,Semantics or Function Behavior Change," 
Complete the save_exponential function, we are using scipy 1.11.1","
import flask
import werkzeug
from scipy import linalg

error404 = werkzeug.exceptions.NotFound

def save_exponential(A, base_path, sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # compute the exponential of the batched matrices (m, m) in A (n,m,m)
    # return the save_path and the exponential of the matrices
    ","
    joined = werkzeug.utils.safe_join(base_path, sub_path)
    if joined is None:
        raise error404
    output = linalg.expm(A)
    return joined, output
","
base_path = '/var/www/myapp'
sub_path = '../secret.txt'
import numpy as np

a = np.random.random((4,3,3))
expected = np.zeros(a.shape)
for i in range(expected.shape[0]):
    expected[i] = linalg.expm(a[i])

try : 
    joined, results = save_exponential(a,base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'

joined, results = save_exponential(a,base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)
assert assertion_result
",scipy==1.11.1,2023-09-30,,2.0,
193,flask,2.0.0,flask.safe_join,Semantics or Function Behavior Change," 
Complete the save_exponential function, we are using scipy 1.8.1"," 
import flask
import werkzeug
from scipy import linalg

error404 = werkzeug.exceptions.NotFound

def save_exponential(A, base_path, sub_path):
    # Attempt to join the base path and sub path.
    # If the joined path is outside the base path, raise a 404 error.
    # compute the exponential of the batched matrices (m, m) in A (n,m,m)
    # return the save_path and the exponential of the matrices
    ","
    joined = flask.safe_join(base_path, sub_path)
    output = np.zeros(A.shape)
    for i in range(A.shape[0]):
        output[i] = linalg.expm(A[i])
    return joined, output
","
base_path = '/var/www/myapp'
sub_path = '../secret.txt'
import numpy as np

a = np.random.random((4,3,3))
expected = np.zeros(a.shape)
for i in range(expected.shape[0]):
    expected[i] = linalg.expm(a[i])

try : 
    joined, results = save_exponential(a,base_path, sub_path)
except werkzeug.exceptions.NotFound as e:
    assertion_result = True
else:
    assertion_result = False
assert assertion_result

base_path = '/var/www/myapp'
sub_path = 'secret.txt'

joined, results = save_exponential(a,base_path, sub_path)
assertion_result = joined == '/var/www/myapp/secret.txt' and np.allclose(results, expected)
assert assertion_result
",scipy==1.8.1 Werkzeug==2.0.0,2021-05-11,,2.0,
194,sympy,1.9,stats.sample,deprecation and new feature,"Generate random samples from a six-sided die, return a list.","    
from sympy.stats import Die, sample
X = Die('X', 6)
def custom_generateRandomSampleDice(X):
    return ",[sample(X) for i in range(3)],"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

def test_custom_generateRandomSampleDice():
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(""always"", SymPyDeprecationWarning)  # Capture all warnings
        output = custom_generateRandomSampleDice(X)
        expect = [sample(X) for i in range(3)]
        assert isinstance(output, list), ""Test Failed: Output is not a list!""
        assert len(output) == len(expect), ""Test Failed: Output length does not match expected!""
        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""

test_custom_generateRandomSampleDice()",scipy,"June 2, 2021",,,
195,sympy,1.9,sympy.physics.matrices.mdft,deprecation and new feature,Compute the Discrete Fourier Transform (DFT) matrix of size  n \times n and show it explicitly.,"
from sympy.matrices.expressions.fourier import DFT

def custom_computeDFT(n):
    return ",DFT(n).as_explicit(),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

def test_custom_computeDFT():
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(""always"", SymPyDeprecationWarning)  # Capture all warnings
        output = custom_computeDFT(4)
        expect = DFT(4).as_explicit()
        assert output == expect
        assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""

test_custom_computeDFT()",,"June 2, 2021",,,
196,sympy,1.9,laplace_transform,deprecation and new feature,"Compute the Laplace transform of the 2 \times 2 identity matrix I_2 (i.e., eye(2)) with respect to t and return a single tuple. The first element of the tuple should be the transformed matrix, and the second element should be the combined convergence condition. Store the results in a variable named output.
","from sympy import laplace_transform, symbols, eye
t, z = symbols('t z')
output = ","laplace_transform(eye(2), t, z, legacy_matrix=False)","from sympy import Matrix
expected = (Matrix([
    [1/z,   0],
    [  0, 1/z]
]), 0, True)
assert output == expected
",,"June 2, 2021",,,
197,sympy,1.11,trace,Dependency-Based Change,Instantiate the trace object with the number n return the resulting trace object.,"import sympy as S
import sympy.physics.quantum
def custom_trace(n):
    return ",S.physics.quantum.trace.Tr(n),"from sympy.physics.quantum.trace import Tr
expect = Tr(2)
assert custom_trace(2) == expect",,"March 6, 2022",,,
198,sympy,1.11,preorder_traversal,Dependency-Based Change,Instantiate the preorder_traversal object with existing expression.,"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_preorder_traversal(expr):
    return ",sympy.preorder_traversal(expr),"expect = [7]
assert list(custom_preorder_traversal(expr)) == expect",,"March 6, 2022",,,
199,sympy,1.11,parse_mathematica,deprecation and new feature,Let F to be a function that returns the maximum value multiplied by the minimum value.,"from sympy.parsing.mathematica import parse_mathematica
from sympy import Function, Max, Min

expr = ""F[6,4,4]""
def custom_parse_mathematica(expr):
    return ","parse_mathematica(expr).replace(Function(""F""), lambda *x: Max(*x)*Min(*x))","import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 24
    assert custom_parse_mathematica(expr) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"August 23, 2022",,,
200,sympy,1.12,sympy.physics.mechanics.PinJoint,Augment change,"Instantiate a PinJoint in the parent to be positioned at parent.frame.x with respect to the mass center, and in the child at -child.frame.x. Store the results in a variable named pin.","from sympy.physics.mechanics import Body, PinJoint
parent, child = Body('parent'), Body('child')
pin = ","PinJoint('pin', parent, child, parent_point=parent.frame.x,child_point=-child.frame.x)","expect1 = parent.frame.x
expect2 = -child.frame.x

assert pin.parent_point.pos_from(parent.masscenter) == expect1
assert pin.child_point.pos_from(child.masscenter) == expect2",,"May 10, 2023",,,
201,sympy,1.12,sympy.physics.mechanics,Function Behavior Change,Instantiate a PinJoint that connects a parent body to a child body. Store the results in a variable named pin.,"import sympy as sp
from sympy.physics.mechanics import Body, PinJoint

parent, child = Body('parent'), Body('child')
pin = ","PinJoint('pin', parent, child)","
assert isinstance(pin.coordinates, sp.Matrix)
assert isinstance(pin.speeds, sp.Matrix)",,"May 10, 2023",,,
202,sympy,1.13,"sympy.functions.combinatorial.numbers.carmichael.is_carmichael
",Dependency-Based Change,"Check number 561 is carmichael or not, return bool value.  Store the results in a variable named output.","from sympy import *

output = ",is_carmichael(561),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy import is_carmichael
    expect = is_carmichael(561)
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
203,sympy,1.13,sympy.ntheory.factor_.divisor_sigma,Dependency-Based Change,Compute the sum of the divisors of the integer 6. Store the results in a variable named output.,"from sympy import *

output = ","divisor_sigma(6, 1)","import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 12
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
204,sympy,1.13,ModularInteger.to_int(),deprecation and new feature,"Consider the field of integers modulo 6, create an element corresponding to the number 8, and then convert this modular element into its standard integer representation. Store the results in a variable named output.","from sympy import GF
K = GF(6)
a = K(8)
output = ",K.to_int(a),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = 2
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
205,sympy,1.13,sympy.physics.mechanics,deprecation and new feature,"Create a custom function that accepts an input and returns a symbolic representation of a rigid body‚Äôs inertia tensor relative to a specified reference frame, constructed using the symbolic variables for its principal moments of inertia. Based on the provided code, give the complete function and import the necessary libraries.","from sympy import symbols
from sympy.physics.mechanics import ReferenceFrame
N = ReferenceFrame('N')
Ixx, Iyy, Izz = symbols('Ixx Iyy Izz')

def custom_generateInertia(N, Ixx, Iyy, Izz):
    from sympy.","physics.mechanics import inertia
    return inertia(N, Ixx, Iyy, Izz)","
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy.physics.mechanics import inertia
    expect = inertia(N, Ixx, Iyy, Izz)
    assert custom_generateInertia(N, Ixx, Iyy, Izz) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
206,sympy,1.13,Eq.rewrite(Add),deprecation and new feature,"Use two defined given symbols and an equality between them, then compute the difference between two symbols.","from sympy import *

x, y = symbols('x y')
eq = Eq(x, y)

output = ",eq.lhs - eq.rhs,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = eq.lhs - eq.rhs
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
207,sympy,1.13,DMP.rep attribute,deprecation and new feature,Create a custom function that accepts a polynomial object and returns a list of its coefficients based on its internal representation.,"from sympy import symbols, Poly
x = symbols('x')
p = Poly(x**2 + 2*x + 3)

def custom_generatePolyList(poly):
    return ",p.rep.to_list(),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = [1,2,3]
    assert custom_generatePolyList(p) == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
208,sympy,1.13,sympy.physics.mechanics.JointsMethod,deprecation and new feature,"Using Sympy‚Äôs mechanics module, create a mechanical system involving three rigid bodies‚Äîa wall, a cart, and a pendulum. The wall serves as the inertial (fixed) reference, while the cart is connected to the wall by a sliding joint along the wall‚Äôs x-axis. The pendulum is attached to the cart via a rotational joint about the cart‚Äôs z-axis, with its connection point offset by a symbolic distance along the pendulum‚Äôs y-axis. Write a custom function that sets up this system using the Newtonian formulation (with the wall as the inertial frame), adds the slider and pin joints, and returns the system‚Äôs equations of motion. What are the equations of motion generated by your function?","from sympy import symbols
from sympy.physics.mechanics import (
  Particle, PinJoint, PrismaticJoint, RigidBody)
l = symbols(""l"")
wall = RigidBody(""wall"")
cart = RigidBody(""cart"")
pendulum = RigidBody(""Pendulum"")
slider = PrismaticJoint(""s"", wall, cart, joint_axis=wall.x)
pin = PinJoint(""j"", cart, pendulum, joint_axis=cart.z,
               child_point=l * pendulum.y)

def custom_motion(wall,slider, pin):
    from sympy.physics.mechanics import ","System
    system = System.from_newtonian(wall)
    system.add_joints(slider, pin)
    return system.form_eoms()
","from sympy import symbols, Function, Derivative, Matrix, sin, cos
t = symbols('t')
l, Pendulum_mass, cart_mass, Pendulum_izz = symbols('l Pendulum_mass cart_mass Pendulum_izz')

q_j = Function('q_j')
u_j = Function('u_j')
u_s = Function('u_s')
M = Matrix([
    [Pendulum_mass*l*u_j(t)**2*sin(q_j(t)) - Pendulum_mass*l*cos(q_j(t))*Derivative(u_j(t), t)
     - (Pendulum_mass + cart_mass)*Derivative(u_s(t), t)],
    [-Pendulum_mass*l*cos(q_j(t))*Derivative(u_s(t), t)
     - (Pendulum_izz + Pendulum_mass*l**2)*Derivative(u_j(t), t)]
])
assert custom_motion(wall,slider, pin) == M",,"July 8, 2023",,,
209,sympy,1.13,sympy.physics.mechanics.Body,deprecation and new feature,"Using the SymPy's mechanics module, define a function that takes two strings as inputs‚Äîone representing the name of a rigid body and the other representing the name of a particle‚Äîand returns a tuple containing a rigid body and a particle created with those names. 
","from sympy.physics.mechanics import *
rigid_body_text = ""rigid_body""
particle_text = ""particle""

def custom_body(rigid_body_text, particle_text):
    return ","RigidBody(rigid_body_text), Particle(particle_text)","import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    exp1, exp2 = custom_body(rigid_body_text, particle_text)
    assert exp1.name == rigid_body_text
    assert exp2.name == particle_text
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
210,sympy,1.9,expr_free_symbols,deprecation and new feature,"Get the given set of free symbols, store the results in output variable.","from sympy import Indexed

a = Indexed(""A"", 0)
output = ",a.free_symbols,"
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    expect = a.free_symbols
    assert output == expect
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"June 2, 2021",,,
211,sympy,1.9,sympy.polys.solvers.RawMatrix,deprecation and new feature,Write a custom function named custom_create_matrix that takes two lists (first and second) and returns a matrix composed of these two lists as rows.,"from sympy import Matrix

first = [1,2]
second =[3,4]
def custom_create_matrix(first,second):
    return ","Matrix([first, second])","import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expected_shape = (2, 2)
    expected_content = [[1, 2], [3, 4]]
    output = custom_create_matrix(first, second)

    assert output.shape == expected_shape

    assert output.tolist() == expected_content
    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"June 2, 2021",,,
212,sympy,1.9,DenseMatrix._flat,new features,"Write a custom function that accepts a Sympy Matrix and returns a flat, read-only copy of its data ","from sympy import Matrix

m = Matrix([[1, 2], [3, 4]])
    
def custom_function(matrix):
    return ",matrix.flat(),"

output = custom_function(m)
output[0] = 100

assert m[0, 0] == 1
assert output[0] == 100

",,"June 2, 2021",,,
213,sympy,1.9,SparseMatrix._todok,new features,Write a custom function that accepts a Sympy SparseMatrix and returns its dictionary-of-keys representation.,"from sympy import Matrix

m = Matrix([[1, 2], [3, 4]])
    
def custom_function(matrix):
    return ",matrix.todok(),"output = custom_function(m)
output[(0, 0)] = 100

assert m[0, 0] == 1
assert output[(0, 0)] == 100


",,"June 2, 2021",,,
214,sympy,1.1,sympy.bottom_up,Dependency-Based Change,Write a custom function named custom_bottom_up that applies a bottom-up traversal to expr with a lambda function on each node.,"import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_bottom_up(expr):
    return ","sympy.bottom_up(expr, lambda x: x.doit())","expect = 7

assert custom_bottom_up(expr) == expect",,"March 6, 2022",,,
215,sympy,1.1,sympy.use,Dependency-Based Change,"Write a custom function that traverses the expression so that every subexpression is evaluated, and returns final evaluated result. ","import sympy

expr = sympy.Add(1, sympy.Mul(2, 3))

def custom_use(expr):
    return "," sympy.use(expr, lambda x: x.doit())","
expect = 7

assert custom_use(expr) == expect",,"March 6, 2022",,,
216,sympy,1.11,carmichael.is_perfect_square,deprecation and new feature,Write a custom function that check if the input is a perfect square.,"import sympy

def custom_is_perfect_square(n):
    return ",sympy.ntheory.primetest.is_square(n),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_is_perfect_square(4)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
217,sympy,1.11,carmichael.is_prime,deprecation and new feature,Write a custom function that check if the input is prime,"import sympy

def custom_is_prime(n):

    return ",sympy.isprime(n),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_is_prime(13)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
218,sympy,1.11,carmichael.divides,deprecation and new feature,"Write a custom function that checks whether the integer p divides the integer n evenly (i.e., with no remainder).","import sympy

def custom_divides(n, p):

    return ",n % p == 0,"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = True
    output = custom_divides(10,2)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
219,sympy,1.12,sympy.tensor.array.expressions.conv_*,Dependency-Based Change,Write a custom function that convert input array into matrix by import correct sympy modules.,"from sympy import Matrix, symbols, Array

a1, a2, a3, a4 = symbols('a1 a2 a3 a4')
array_expr = Array([[a1, a2], [a3, a4]])

def custom_array_to_matrix(array):
    from sympy.tensor.array.expressions.","from_array_to_matrix import convert_array_to_matrix
    return convert_array_to_matrix(array)
","
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    from sympy.tensor.array.expressions.from_array_to_matrix import convert_array_to_matrix

    expect = convert_array_to_matrix(array_expr)
    output = custom_array_to_matrix(array_expr)

    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"May 10, 2023",,,
220,sympy,1.13,"sympy.ntheory.residue_ntheory.jacobi_symbol
",Dependency-Based Change,Write a custom function that compute the Jacobi symbol (a/n).,"
import sympy

def custom_jacobi_symbols(a, n):
    return ","sympy.jacobi_symbol(a, n)","
import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_jacobi_symbols(1001, 9907)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
221,sympy,1.13,sympy.partitions_.npartitions,Dependency-Based Change,Write a custom function that compute the number of partitions of n.,"
import sympy

def custom_npartitions(n):
    return ",sympy.functions.combinatorial.numbers.partition(n),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 7
    output = custom_npartitions(5)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
222,sympy,1.13,"sympy.ntheory.factor_.primeomega
",Dependency-Based Change,Write a custom function that compute the number of distinct prime factors of n.,"
import sympy

def custom_primefactors(n):
    return ",sympy.primeomega(n),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 3
    output = custom_primefactors(18)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
223,sympy,1.13,sympy.ntheory.generate.primepi,Dependency-Based Change,Write a custom function that compute the prime counting function for n.,"
import sympy

def custom_prime_counting(n):
    return ","sympy.primepi(n)
","import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 10
    output = custom_prime_counting(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
224,sympy,1.13,sympy.ntheory.factor_.totient,Dependency-Based Change,Write a custom function that compute Euler's totient function for n (number of integers relatively prime to n).,"
import sympy

def custom_totient(n):
    return ",sympy.totient(n),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = 8
    output = custom_totient(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
225,sympy,1.13,sympy.ntheory.residue_ntheory.mobius,Dependency-Based Change,Write a custom function that compute the M√∂bius function for n.,"import sympy

def custom_mobius(n):
    return ",sympy.mobius(n),"import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_mobius(30)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
226,sympy,1.13,"sympy.ntheory.residue_ntheory.legendre_symbol
",Dependency-Based Change,Write a custom function that compute the Legendre symbol (a/p).,"
import sympy

def custom_legendre(a, n):
    return ","sympy.legendre_symbol(a, n)","import warnings
from sympy.utilities.exceptions import SymPyDeprecationWarning

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"", SymPyDeprecationWarning)
    
    expect = -1
    output = custom_legendre(200, 13)
    assert output == expect

    assert not any(isinstance(warn.message, SymPyDeprecationWarning) for warn in w), ""Test Failed: Deprecation warning was triggered!""",,"July 8, 2023",,,
227,seaborn,0.13.0,seaborn.pointplot(),deprecation and new feature,"Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, with remove connecting lines","import seaborn as sns
import pandas as pd


data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})

def custom_pointplot(data):
    return ","sns.pointplot(x='x', y='y', data=data, markers=""o"", linestyles=""none"")","

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_pointplot(data)
    
    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]

    if any(""dataframegroupby.apply"" in msg for msg in warning_messages):
        pass  
    elif not any(""deprecated"" in msg and ""removed"" in msg for msg in warning_messages):
        raise AssertionError(""Expected deprecation warning was not raised."")

    for line in output.lines:
        if line.get_linestyle() != ""None"":
            raise AssertionError(""Linestyle is not set to 'none' as expected."")
        break",panda,"September 29, 2023",,,
228,seaborn,0.13.0,seaborn.pointplot(),deprecation and new feature,"Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, adjust error bar width to 2.","import seaborn as sns
import pandas as pd


data = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [10, 15, 13, 17]})

def custom_pointplot(data):
    return ","sns.pointplot(x='x', y='y', data=data, err_kws={""linewidth"": 2})","
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_pointplot(data)
    
    warning_messages = [word for warn in w for word in str(warn.message).strip().lower().split()]

    if any(""dataframegroupby.apply"" in msg for msg in warning_messages):
        pass  
    elif not any(""deprecated"" in msg and ""removed"" in msg for msg in warning_messages):
        raise AssertionError(""Expected deprecation warning was not raised."")
    
    found_correct_linewidth = False
    for line in output.lines: 
        linewidth = line.get_linewidth()
        if linewidth == 2:
            found_correct_linewidth = True
            break

    if not found_correct_linewidth:
        raise AssertionError(""Error bar linewidth is not set to 2 as expected."")",panda,"September 29, 2023",,,
229,seaborn,0.13.0,seaborn.violinplot,deprecation and new feature,"Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, scales the bandwidth to 1.5.","import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_violinplot(data):
    return ","sns.violinplot(x='x', y='y', data=data, bw_adjust=1.5)","
import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_violinplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""bw"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""bw parameter should not be used. Use bw_method and bw_adjust instead."")

    for collection in output.collections:
            if hasattr(collection, ""get_paths""):
                assert sns.violinplot.__defaults__[0] == 1.5
    ",panda,"September 29, 2023",,,
230,seaborn,0.13.0,seaborn.violinplot,deprecation and new feature,"Write a Seaborn pointplot() function that visualizes x and y from a Pandas DataFrame, choose bandwidth to scott.","import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_violinplot(data):
    return ","sns.violinplot(x='x', y='y', data=data, bw_method=""scott"")","
import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_violinplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""bw"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""bw parameter should not be used. Use bw_method and bw_adjust instead."")
    
    collections = [c for c in output.collections if isinstance(c, plt.Line2D)]  # Extract violin plot lines
    
    assert output is not None, ""Violin plot output should not be None.""",panda,"September 29, 2023",,,
231,seaborn,0.13.0,seaborn.barplot(),deprecation and new feature,"Write a Seaborn barplot() function that visualizes x and y from a Pandas DataFrame, adjust error bar with color red and linewidth 2.","import seaborn as sns
import pandas as pd

import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_barplot(data):
    return ","sns.barplot(x='x', y='y', data=data, err_kws={'color': 'red', 'linewidth': 2})","
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    ax = custom_barplot(data)
    
    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""errcolor"" in msg or ""errwidth"" in msg for msg in warning_messages):
        raise AssertionError(""errcolor and errwidth should not be used. Use err_kws instead."")

    for line in ax.lines:
        if line.get_linewidth() == 2 and line.get_color() == 'red':
            break
    else:
        raise AssertionError(""Error bars are not set with err_kws correctly."")",panda,"September 29, 2023",,,
232,seaborn,0.13.0,seaborn.boxenplot(),argument change,"Write a Seaborn boxenplot() function that visualizes x and y from a Pandas DataFrame, make width method to exponential.","import seaborn as sns
import pandas as pd
import warnings
import matplotlib.pyplot as plt

data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [5, 10, 15]})

def custom_boxenplot(data):
    return ","sns.boxenplot(x='x', y='y', data=data, width_method='exponential')","
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    
    output = custom_boxenplot(data)

    warning_messages = [str(warn.message).strip().lower() for warn in w]
    if any(""scale"" in msg and ""deprecated"" in msg for msg in warning_messages):
        raise AssertionError(""scale should not be used in boxenplot. Use width_method instead."")

    for artist in output.get_children():
        if hasattr(artist, ""get_linestyle"") and artist.get_linestyle() in [""-"", ""--""]:
            break
    else:
        raise AssertionError(""Boxen elements are missing, width_method might not be applied."")",panda,"September 29, 2023",,,
233,seaborn,0.12.0,seaborn.scatterplot().set(),argument change,"Write a custom function that visualizes x and y from a Pandas DataFrame, set the X label to be ""My X Label"" and Y label to be ""My Y Label"".","import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

data = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})

def custom_set_axis_labels(data):
    ax = sns.scatterplot(x='x', y='y', data=data)
    ax.","set(xlabel=""My X Label"", ylabel=""My Y Label"")
    return ax","
ax = custom_set_axis_labels(data)
x_expect = ""My X Label""
y_expect = ""My Y Label""
assert ax.get_xlabel() == x_expect and ax.get_ylabel() == y_expect, (
    ""Axis labels not set correctly using ax.set().""
)
",panda,"September 5, 2022",,,
234,seaborn,0.12.0,seaborn.iqr(),deprecation and new feature,"Write a custom function to compute iqr for input data. If needed, use another library.","import numpy as np

data_array = np.array([1, 2, 3, 4, 5])

def custom_iqr(data):
    from ","scipy.stats import iqr
    return iqr(data)
","computed_iqr = custom_iqr(data_array)
expect = 2
assert computed_iqr == expect
",scipy,"September 5, 2022",,,
235,mitmproxy,9.0.1,mitmproxy.connection.Client,argument change,"Using mitmproxy‚Äôs connection API, create a client connection by specifying the IP address (ip_address), the input port (i_port), the output port (o_port), and the timestamp (using time.time() for the current time). Store the resulting client connection in the variable output_client.","import time
import mitmproxy.connection as conn

ip_address = ""127.0.0.1""
i_port = 111
o_port = 222

output_client = "," conn.Client(
    peername=(ip_address, i_port),
    sockname=(ip_address, o_port),
    timestamp_start=time.time()
)","expect_peername = (""127.0.0.1"", 111)
expect_sockname = (""127.0.0.1"", 222)

assert output_client.peername == expect_peername
assert output_client.sockname == expect_sockname
",,"November 2, 2022",,,
236,mitmproxy,9.0.1,mitmproxy.connection.Server,argument change,"Using mitmproxy‚Äôs connection API, create a server connection with the given parameters: an IP address (ip_address), a server port (server_port), and store the resulting instance in the variable output_server. ","import mitmproxy.connection as conn

ip_address = ""192.168.1.1""
server_port = 80

output_server = "," conn.Server(
    address=(ip_address, server_port)
)","expect = (""192.168.1.1"", 80)
assert output_server.address == expect",,"November 2, 2022",,,
237,mitmproxy,7.0.0,"server_connected
",argument change,"Using mitmproxy‚Äôs connection API, create a Python class named ConnectionLogger that implements the server connected event hook. When a server connection is established, the implemented method should be called with a parameter server_conn, and it must print the server‚Äôs local address using the attribute server_conn.sockname in the following format: Server connected with local address {server_conn.sockname}.","import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ","server_connected(self, server_conn):
        print(f""Server connected with local address {server_conn.sockname}"")","        
import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_connected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_connected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",,"July 16, 2021",,,
238,mitmproxy,7.0.0,server_connect,new feature,"Using mitmproxy‚Äôs connection API, create a Python class named ConnectionLogger that implements the server connect event hook. When a server connection is established, the implemented method should be called with a parameter server_conn, and it must print the server‚Äôs local address using the attribute server_conn.sockname in the following format: Server connect {server_conn.sockname}.","import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ","server_connect(self, server_conn):
        print(f""Server connect : {server_conn.sockname}"")","import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_connect(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_connect(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",,"July 16, 2021",,,
239,mitmproxy,7.0.0,"server_disconnected
",argument change,"Using mitmproxy‚Äôs connection API, create a Python class named ConnectionLogger that implements the server disconnected event hook. When a server connection is terminated, the implemented method should be called with a parameter server_conn, and it must print the server‚Äôs local address using the attribute server_conn.sockname in the following format: Server disconnected: {server_conn.sockname}.","import contextlib

class DummyServerConn:
    def __init__(self, sockname):
        self.sockname = sockname

class ConnectionLogger:
    def ","server_disconnected(self, server_conn):
        print(f""Server disconnected: {server_conn.sockname}"")","import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_server_disconnected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyServerConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.server_disconnected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",,"July 16, 2021",,,
240,mitmproxy,7.0.0,client_connected,argument change,"Using mitmproxy‚Äôs connection API, create a Python class named ConnectionLogger that implements the client connected event hook. When a client connection is established, the implemented method should be called with a parameter client_conn, and it must print the client's local address using the attribute client_conn.peername in the following format: Client connected: {client_conn.peername}.","import contextlib

class DummyClientConn:
    def __init__(self, peername):
        self.peername = peername

class ConnectionLogger:
    def ","client_connected(self, client_conn):
        print(f""Client connected: {client_conn.peername}"")","import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_client_connected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyClientConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.client_connected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",,"July 16, 2021",,,
241,mitmproxy,7.0.0,client_disconnected,argument change,"Using mitmproxy‚Äôs connection API, create a Python class named ConnectionLogger that implements the client disconnected event hook. When a client connection is terminated, the implemented method should be called with a parameter client_conn, and it must print the client's local address using the attribute client_conn.peername in the following format: Client disconnected: {client_conn.peername}.","import contextlib

class DummyClientConn:
    def __init__(self, peername):
        self.peername = peername

class ConnectionLogger:
    def ","client_disconnected(self, client_conn):
        print(f""Client disconnected: {client_conn.peername}"")","import unittest
import io
class TestConnectionLogger(unittest.TestCase):
    def test_client_disconnected(self):
        logger = ConnectionLogger()
        dummy_conn = DummyClientConn(('127.0.0.1', 8080))
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            logger.client_disconnected(dummy_conn)
            
        expect = ""('127.0.0.1', 8080)""
        
        self.assertIn(expect, output.getvalue())
        
unittest.main()",,"July 16, 2021",,,
242,mitmproxy,7.0.0,add_log,function change,"Mitmproxy triggers a logging event that passes a log entry object containing a message in its msg attribute. Given the following class definition for MyAddon, extend it by adding a method that handles this log event. Your implementation should use f-string formatting to print the log entry‚Äôs message. Provide the complete code for the MyAddon class. ","
import contextlib

class DummyLogEntry:
    def __init__(self, msg):
        self.msg = msg

class MyAddon:
    def ","add_log(self, entry):
        print(f""{entry.msg}"")","import unittest
import io
class TestMyAddonLogging(unittest.TestCase):
    def test_logging_event(self):
        addon = MyAddon()
        dummy_entry = DummyLogEntry(""Test log message"")
        
        output = io.StringIO()
        with contextlib.redirect_stdout(output):
            addon.add_log(dummy_entry)
        print(output.getvalue())
        
        self.assertIn(""Test log message"", output.getvalue())

unittest.main()",,"July 16, 2021",,,
243,mitmproxy,7.0.0,mitmproxy.certs,function behavior change,"Complete the implementation of the generate_cert_new function so that it obtains a certificate object by calling the correct method on the CA, and returns a tuple containing the certificate PEM and key PEM.","import types

class DummyCert:
    def __init__(self, hostname):
        self.cert_pem = f""-----BEGIN CERTIFICATE-----\nDummy certificate for {hostname}\n-----END CERTIFICATE-----""
        self.key_pem = f""-----BEGIN PRIVATE KEY-----\nDummy key for {hostname}\n-----END PRIVATE KEY-----""

class DummyCA:
    def __init__(self, path):
        self.path = path

    def get_cert(self, hostname):
        return DummyCert(hostname)
    
certs = types.ModuleType(""certs"")
certs.CA = DummyCA

def generate_cert_new(hostname):

    ca = certs.CA(""dummy/path"")
    cert_obj = ","ca.get_cert(hostname)
    return cert_obj.cert_pem, cert_obj.key_pem","def test_generate_cert_new():
    hostname = ""example.com""
    cert_pem, key_pem = generate_cert_new(hostname)
    
    assert ""BEGIN CERTIFICATE"" in cert_pem, ""Certificate PEM missing header""
    assert ""BEGIN PRIVATE KEY"" in key_pem, ""Key PEM missing header""
    
    assert hostname in cert_pem, ""Hostname not found in certificate PEM""
    
    assert cert_pem.strip() != """", ""Certificate PEM is empty""
    assert key_pem.strip() != """", ""Key PEM is empty""
    
test_generate_cert_new()",,"July 16, 2021",,,
244,mitmproxy,7.0.0,mitmproxy.net.http.Headers,Dependency-Based Change,Update the code by writing the correct import statement for the Headers class from mitmproxy.http. Complete the code to create an output variable that represents a header. The header object takes header_name and initial_value as inputs.,"header_name = b""Content-Type""
initial_value = b""text/html""

from mitmproxy.","http import Headers
output = Headers([(header_name, initial_value)])","
expect = ""text/html""
assert output.get(header_name) == expect",,"July 16, 2021",,,
245,pytest,7.0.0,pytest.hookimpl(),deprecation and new feature,"Update the code by writing the correct import statement for the hook implementation decorator from the testing framework. Then, complete the code to define a hook implementation function named pytest_runtest_call that uses this decorator with its execution priority parameter set to false; the function body should contain only the pass statement.","import pytest

@pytest.","hookimpl(tryfirst=False)
def pytest_runtest_call():
    pass","import pluggy

def test_hookimpl_configuration_with_plugin_manager():
    pm = pluggy.PluginManager(""pytest"")
    
    class DummyPlugin:
        pytest_runtest_call = pytest_runtest_call

    plugin = DummyPlugin()
    pm.register(plugin)
    
    hookimpls = pm.hook.pytest_runtest_call.get_hookimpls()
    
    for impl in hookimpls:
        if impl.plugin is plugin:
            opts = impl.opts 
            assert opts.get(""tryfirst"") is False
            break
    else:
        pytest.fail(""pytest_runtest_call implementation not found in plugin manager."")



test_hookimpl_configuration_with_plugin_manager()",,"February 4, 2022",,,
246,pytest,7.0.0,pytest.hookimpl(hookwrapper),deprecation and new feature,Update the code by writing the correct import statement for the hook implementation decorator from the testing framework and then complete the code to define a hook implementation function named pytest_runtest_setup that uses this decorator with its hookwrapper parameter set to True; the function body should contain only a yield statement.,"import pytest

@pytest.","hookimpl(hookwrapper=True)
def pytest_runtest_setup():
    yield","import pluggy

def test_hookwrapper_configuration_with_plugin_manager():
    pm = pluggy.PluginManager(""pytest"")
    
    class DummyPlugin:
        pytest_runtest_setup = pytest_runtest_setup

    plugin = DummyPlugin()
    pm.register(plugin)
    
    hookimpls = pm.hook.pytest_runtest_setup.get_hookimpls()
    for impl in hookimpls:
        if impl.plugin is plugin:
            opts = impl.opts
            assert opts.get(""hookwrapper"") is True, ""Expected hookwrapper=True for a hook wrapper""
            break
    else:
        pytest.fail(""pytest_runtest_setup implementation not found in plugin manager."")


test_hookwrapper_configuration_with_plugin_manager()",,"February 4, 2022",,,
247,pytest,7.0.0,pytest_ignore_collect(collection_path: pathlib.Path),argument change,Complete code snippet that defines a hook implementation function named pytest_ignore_collect which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_ignore_collect(","collection_path:pathlib.Path):
    pass","import inspect
def test_pytest_ignore_collect_signature():
    sig = inspect.signature(pytest_ignore_collect)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_ignore_collect_signature()
",,"February 4, 2022",,,
248,pytest,7.0.0,pytest_collect_file(file_path: pathlib.Path),argument change,Complete code snippet that defines a hook implementation function named pytest_collect_file which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_collect_file(","file_path:pathlib.Path):
    pass","
import inspect
def test_pytest_collect_file_signature():
    sig = inspect.signature(pytest_collect_file)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_collect_file_signature()",,"February 4, 2022",,,
249,pytest,7.0.0,pytest_pycollect_makemodule(module_path: pathlib.Path),argument change,Complete code snippet that defines a hook implementation function named pytest_pycollect_makemodule which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_pycollect_makemodule(","module_path:pathlib.Path):
    pass","import inspect
def test_pytest_pycollect_makemodule_signature():
    sig = inspect.signature(pytest_pycollect_makemodule)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_pycollect_makemodule_signature()
",,"February 4, 2022",,,
250,pytest,7.0.0,pytest_report_header(start_path: pathlib.Path),argument change,Complete code snippet that defines a hook implementation function named pytest_report_header which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_report_header(","start_path:pathlib.Path):
    pass
","
import inspect
def test_pytest_report_header_signature():
    sig = inspect.signature(pytest_report_header)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_report_header_signature()
",,"February 4, 2022",,,
251,pytest,7.0.0,pytest_report_collectionfinish(start_path: pathlib.Path),argument change,Complete code snippet that defines a hook implementation function named pytest_report_collectionfinish which takes a single parameter (representing a filesystem path) and whose body contains only the pass statement.,"import pytest
import pathlib

@pytest.hookimpl()
def pytest_report_collectionfinish(","start_path:pathlib.Path):
    pass","
import inspect
def test_pytest_report_collectionfinish_signature():
    sig = inspect.signature(pytest_report_collectionfinish)
    params = list(sig.parameters.items())
    name, param = params[0]
    expect = pathlib.Path
    assert param.annotation == expect

test_pytest_report_collectionfinish_signature()
",,"February 4, 2022",,,
252,pytest,7.0.0,pytest.Item,argument change,Complete code snippet that defines a custom subclass of pytest.Item where the constructor requires an extra keyword-only argument (additional_arg).,"import pytest

class CustomItem(pytest.Item):
    def __init__(","self, *, additional_arg, **kwargs):
        super().__init__(**kwargs)
        self.additional_arg = additional_arg","import inspect
signature = inspect.signature(CustomItem.__init__)
assert any(param.kind == param.VAR_KEYWORD for param in signature.parameters.values())",,"February 4, 2022",,,
253,pytest,7.2.0,pytest.PytestReturnNotNoneWarning,behavior change,"Provide a complete code snippet where a custom function named test_foo(a, b, result) verifies whether foo(a, b) == result. Ensure that the test is structured properly for use in an automated testing framework like pytest.","
import pytest

def foo(a, b):
    return (10 * a - b + 7) // 3

@pytest.mark.parametrize(
    [""a"", ""b"", ""result""],
    [
        [1, 2, 5],
        [2, 3, 8],
        [5, 3, 18],
    ],
)
def test_foo(a, b, result):
    ","    assert foo(a, b) == result
","import dis
import inspect
def test_assert_in_test_foo_bytecode():
    original_test_foo = inspect.unwrap(test_foo)
    instructions = list(dis.get_instructions(original_test_foo))
    has_raise = any(instr.opname == ""RAISE_VARARGS"" for instr in instructions)
    assert has_raise
    
test_assert_in_test_foo_bytecode()",,"October 25, 2022",,,
254,pytest,6.2.0,pytest.yield_fixture,deprecation and new feature,Complete code snippet by using fixtures in pytest.,"import pytest

@pytest.",fixture,"def sample_data():
    data = [1, 2, 3]
    yield data

def test_sample_data(sample_data):
    assert sample_data == [1, 2, 3]

if __name__ == ""__main__"":
    pytest.main([""-q"", ""--tb=short""]) ",,"December 12, 2020",,,
255,pytest,7.2.0,pytest.mark.parametrize,new feature,"Write a pytest function that verifies whether a given function correctly squares an input number using parameterized test cases: (2, 4).","import pytest

@pytest.","mark.parametrize(""x, y"", [(2, 4)])","def test_squared(x, y):
    assert x**x == y

if __name__ == ""__main__"":
    pytest.main([""-q"", ""--tb=short""]) ",,"October 25, 2022",,,
256,falcon,3.0.0,falcon.stream.BoundedStream,deprecation and new feature,"Provide a complete code snippet that defines a custom function get_bounded_stream, which accepts a req object and wraps the incoming request stream with a controlled reader. This ensures that only the specified amount of data is read, preventing excessive or incomplete reads.","from falcon import stream

import io
class DummyRequest:
    def __init__(self, data: bytes):
        self.stream = io.BytesIO(data)
        self.content_length = len(data)
        
test_data = b""Hello, Falcon!""
req = DummyRequest(test_data)

def get_bounded_stream(req):
    return ","stream.BoundedStream(req.stream, req.content_length)","bounded_stream = get_bounded_stream(req)
read_data = bounded_stream.read()
expect = b""Hello, Falcon!""
assert read_data == expect",,"April 5, 2021",,,
257,falcon,3.0.0,falcon.Response.body,argument change," Complete code snippet that defines a custom function custom_body which accepts a Falcon Response object as input and sets its body to the variable info which is string, and finally return Response object.","import falcon
resp = falcon.Response()

info = 'Falcon'

def custom_body(resp):
    resp.","text = info
    return resp","import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    resp = custom_body(resp)
    if w:
        assert issubclass(w[-1].category, DeprecationWarning), ""Expected a DeprecationWarning but got something else!""

expect = 'Falcon'
assert resp.text == expect
",,"April 5, 2021",,,
258,falcon,3.0.0,falcon.HTTPStatus.body,argument change," Complete code snippet that defines a custom function custom_body which accepts a Falcon HTTPStatus object as input and sets its body to the variable info which is string, and finally return HTTPStatus object.","import falcon
from falcon import HTTPStatus

status = HTTPStatus(falcon.HTTP_200)

info = 'Falcon'

def custom_body(status):
    status.","text = info
    return status","
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    resp = custom_body(status)
    if w:
        assert issubclass(w[-1].category, DeprecationWarning), ""Expected a DeprecationWarning but got something else!""

expect = 'Falcon'
assert resp.text == expect
",,"April 5, 2021",,,
259,falcon,3.0.0,falcon.Response.stream_len,argument change," Complete code snippet that defines a custom function custom_body_length which accepts a Falcon Response object as input and sets its body length as length of variable info , and finally return Response object.","from falcon import Response

info = ""Falcon""

def custom_body_length(resp: Response, info):
    resp.","content_length = len(info)
    return resp","class DummyResponse(Response):
    pass

resp = DummyResponse()

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    custom_resp = custom_body_length(resp, info)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), \
                ""Deprecated API used!""
expect = str(len(info))
assert custom_resp.content_length == expect",,"April 5, 2021",,,
260,falcon,3.0.0,falcon.Response.data.render_body,new feature," Complete code snippet that defines a custom function custom_data which accepts a Falcon Response object as input and sets its data as variable info, processes the data property and returns it in the correct format for an HTTP response.","
from falcon import Response

info = ""Falcon data""

def custom_data(resp: Response):
    resp.data = info
    return ",resp.render_body(),"class DummyResponse(Response):
    pass

resp = DummyResponse()
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    rendered_body = custom_data(resp)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""


expect = info
assert rendered_body == expect",,"April 5, 2021",,,
261,falcon,3.0.0,falcon.HTTPError.to_json(),function change,"Complete the code snippet that defines a custom function custom_http_error, ensuring it correctly raises an HTTP error in Falcon. The function should return a JSON response representing the error. ","import falcon
from falcon import HTTPError


def custom_http_error():
    err = HTTPError(falcon.HTTP_400, ""Bad Request"", ""An error occurred"")
    return ",err.to_json(),"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    result = custom_http_error()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""


expect = b'{""title"": ""Bad Request"", ""description"": ""An error occurred""}'
assert result == expect",,"April 5, 2021",,,
262,falcon,3.0.0,falcon.testing.create_environ(),argument change,"Complete the code snippet that defines a custom function custom_environ, which should create and return an environment with info variable as the root.","import falcon.testing as testing

info = ""/my/root/path""

def custom_environ():
    return ",testing.create_environ(root_path=info),"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    env = custom_environ()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect = info
assert env.get('SCRIPT_NAME', '') == expect",,"April 5, 2021",,,
263,falcon,3.0.0,falcon.stream.BoundedStream.writeable,argument change,"Complete the code snippet that defines a custom function custom_writable, which should accepts a BoundedStream object and returns its writable property as Boolean data type.","import io
import warnings
from falcon.stream import BoundedStream

def custom_writable(bstream: BoundedStream):
    return ",bstream.writable(),"stream = io.BytesIO(b""initial data"")
bstream = BoundedStream(stream, 1024)

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    writable_val = custom_writable(bstream)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = False 
assert writable_val == expect",,"April 5, 2021",,,
264,falcon,3.0.0,falcon.app_helpers.prepare_middleware(),function change,"Complete the code snippet that defines a custom function custom_middleware_variable, which should create an ExampleMiddleware object that should be accepted by the function app_helpers.prepare_middleware().","import falcon.app_helpers as app_helpers

class ExampleMiddleware:
    def process_request(self, req, resp):
        pass

def custom_middleware_variable():
    return ",[ExampleMiddleware()],"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    middleware = custom_middleware_variable()
    prepared_mw = app_helpers.prepare_middleware(middleware)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = (list, tuple)
assert isinstance(prepared_mw, expect)
",,"April 5, 2021",,,
265,falcon,3.0.0,falcon.testing.create_environ(http_version=),behavior change,"Complete the code snippet that defines a custom function custom_environ, which should set the HTTP version to 1.1 and return the environment object.","import falcon.testing as testing

def custom_environ():
    return ","testing.create_environ(http_version=""1.1"")","import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    env = custom_environ()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = ""HTTP/1.1""
assert env.get('SERVER_PROTOCOL', '') == expect",,"April 5, 2021",,,
266,falcon,3.0.0,falcon.Response.append_link(),new feature,"Complete the code snippet by defining a custom function named custom_append_link that takes a Falcon Response object, a string link, and a string rel as inputs. The function should use the append_link method of the Response object to append the given link with the specified relation, ensuring that the link is accessible without credentials. The function should then return the updated response.","from falcon import Response

resp = Response()
link = 'http://example.com'
rel = 'preconnect'

def custom_append_link(resp, link, rel):
    resp.","append_link(link, rel, crossorigin='anonymous')
    return resp","response = custom_append_link(resp, link, rel)
expected = ""crossorigin""
assert expected in response.get_header('Link')",,"April 5, 2021",,,
267,falcon,3.0.0,falcon.API,function name change,Complete the code snippet by defining a custom function named custom_falcons that creates a Falcon-based WSGI app and return it.,"import falcon

def custom_falcons():
    return ",falcon.App(),"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    app_instance = custom_falcons()
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = falcon.App
assert isinstance(app_instance, expect)",,"April 5, 2021",,,
268,falcon,3.0.0,falcon.Request.add_link(),function name change,"Define a function named custom_link that accepts a Falcon Response object, a string indicating the relationship of the link (link_rel), and a string for the link URL (link_href). The function should incorporate the link into the response‚Äôs headers‚Äîensuring that the relationship and URL are correctly associated‚Äîand then return the modified response.","from falcon import Response

link_rel = ""next""
link_href = ""http://example.com/next""
resp = Response()

def custom_link(resp: Response, link_rel, link_href):
    resp.","append_link(link_href, link_rel)
    return resp","import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    custom_resp = custom_link(resp,link_rel,link_href)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expected_link = f'<{link_href}>;'
link_header = custom_resp.get_header(""Link"") or """"
assert expected_link in link_header",,"April 5, 2021",,,
269,falcon,3.0.0,falcon.Request.media,deprecation and new feature,"Create a function named custom_media that accepts a Falcon Request object and retrieves the parsed request body (the media) as a Python data structure. The function should then return this parsed content.
","import json
from falcon import Request
from falcon.testing import create_environ

payload = {""key"": ""value""}
body_bytes = json.dumps(payload).encode(""utf-8"")

env = create_environ(
    body=body_bytes,
    headers={'Content-Type': 'application/json'}
)

req = Request(env)

def custom_media(req: Request):
    return ",req.get_media(),"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    media = custom_media(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect = payload
assert media == expect",,"April 5, 2021",,,
270,falcon,2.0.0,falcon.HTTPRequestEntityTooLarge,function name change,"Define a function named raise_too_large_error that, when called, raises an exception indicating that the request content exceeds acceptable limits, using the provided error_message variable as the error detail.
","import falcon 

error_message = ""Request content is too large""

def raise_too_large_error():
    raise ",falcon.HTTPPayloadTooLarge(error_message),"import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    try:
        raise_too_large_error()
    except falcon.HTTPPayloadTooLarge as e:
        exception_raised = e
    else:
        exception_raised = None

    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expected_message = error_message
assert str(exception_raised) == expected_message",,"April 26, 2019",,,
271,falcon,2.0.0,falcon.uri.parse_query_string,argument change,"Define a function named custom_parse_query that accepts a query string as its input and returns its parsed representation. The function should leverage the utility from falcon.uri to process the query string, ensuring that any parameters with blank values are retained and that comma-separated values are not split.","
from falcon.uri import parse_query_string

query_string = ""param1=value1&param2=""

def custom_parse_query(qs):
    return ","parse_query_string(qs, keep_blank=True, csv=False)","import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    parsed_values = custom_parse_query(query_string)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect1 = 'value1'
expect2 = ''
assert parsed_values.get('param1') == expect1
assert parsed_values.get('param2') == expect2",,"April 26, 2019",,,
272,falcon,2.0.0,falcon.Request.get_param_as_dict(),deprecation and new feature,"Define a function named custom_get_param that accepts a Falcon Request object. The function should extract the value of the query parameter named ‚Äúfoo‚Äù from the request‚Äôs URL, interpret this value as a JSON-encoded string, convert it into its corresponding Python object, and return that object.
","import json
from falcon import Request
from falcon.testing import create_environ

json_value = json.dumps({""bar"": ""baz""})
query_string = f""foo={json_value}""

env = create_environ(query_string=query_string)
req = Request(env)

def custom_get_param(req: Request):
    return ","req.get_param_as_json(""foo"")","import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    result = custom_get_param(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = {""bar"": ""baz""}
assert result == expect",,"April 26, 2019",,,
273,falcon,2.0.0,handle_error,argument change,"Complete a function named handle_error that acts as an error handler in a Falcon application. The function should accept the request, response, exception, and additional parameters. Its purpose is to update the response by setting its media to a JSON object containing an error message (derived from the exception) and updating the HTTP status to indicate an internal server error. ","import falcon

def handle_error(","req, resp, ex, params):
    resp.media = {""error"": str(ex)}
    resp.status = falcon.HTTP_500","class DummyReq:
    pass

class DummyResp:
    def __init__(self):
        self.media = None
        self.status = None

dummy_req = DummyReq()
dummy_resp = DummyResp()
dummy_ex = Exception(""Test error"")
dummy_params = {}

import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    handle_error(dummy_req, dummy_resp, dummy_ex, dummy_params)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
expect1 = {""error"": ""Test error""}
expect2 = falcon.HTTP_500
assert dummy_resp.media == expect1
assert dummy_resp.status == expect2",,"April 26, 2019",,,
274,falcon,2.0.0,falcon.Request.get_param_as_int,argument change,"Define a function named custom_get_dpr that accepts a Falcon Request object and retrieves the value of the ‚Äúdpr‚Äù query parameter as an integer. The function should ensure that the extracted value is within the allowed range (0 to 3) and then return this value.
","from falcon import Request
from falcon.testing import create_environ

env = create_environ(query_string=""dpr=2"")
req = Request(env)

def custom_get_dpr(req: Request):
    return ","req.get_param_as_int(""dpr"", min_value=0, max_value=3)","import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    dpr = custom_get_dpr(req)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = 2
assert dpr == expect",,"April 26, 2019",,,
275,falcon,2.0.0,falcon.Request. context_type,behavior change,"Define a function named custom_set_context that takes a Falcon Request object along with two string arguments representing a role and a user. The function should update the request‚Äôs context by assigning these values to appropriate attributes and then return the modified context.
","from falcon import Request
from falcon.testing import create_environ

env = create_environ()
req = Request(env)

role = 'trial'
user = 'guest'
def custom_set_context(req: Request, role, user):
    req.","context.role = role
    req.context.user = user
    return req.context
","import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    context = custom_set_context(req, role, user)
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect1 = 'trial'
expect2 = 'guest'

assert context.role == expect1
assert context.user == expect2",,"April 26, 2019",,,
276,falcon,2.0.0,add_route(),argument change and new feature,"Create a class named CustomRouter to manage your application‚Äôs routes. The class should maintain an internal dictionary to store routes and their associated resources. Implement an add_route method that accepts a URI template, a resource, and additional keyword arguments. This method should generate a mapping of HTTP methods to resource handlers‚Äîusing an appropriate Falcon utility‚Äîand store the resulting tuple (resource, method mapping) in the routes dictionary. You can import falcon.routing if needed. Finally, the method should return the generated mapping.
","
class CustomRouter:
    def __init__(self):
        self.routes = {}

    def add_route(","self, uri_template, resource, **kwargs):
        from falcon.routing import map_http_methods
        method_map = map_http_methods(resource, kwargs.get('fallback', None))
        self.routes[uri_template] = (resource, method_map)
        return method_map","class DummyResource:
    def on_get(self, req, resp):
        resp.text = ""hello""
import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    router = CustomRouter()
    method_map = router.add_route(""/test"", DummyResource())
    if w:
        for warn in w:
            assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
            
expect = ""/test""
assert expect in router.routes
resource, mapping = router.routes[""/test""]
assert callable(mapping.get(""GET"", None))",,"April 26, 2019",,,
277,tornado,6.3.0,IOLoop.add_callback_from_signal,deprecation and new feature,"Write a custom function named custom_add_callback_from_signal that registers a signal handler. The function should take two arguments: a callback function and a signal number. When the specified signal is received, the callback should be executed.
","import asyncio
import os
import signal

def custom_add_callback_from_signal(callback, signum):

    loop = ","asyncio.get_event_loop()
    loop.add_signal_handler(signum, callback)
","
def test_custom_signal_handler():

    flag = {""executed"": False}
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    def callback():
        flag[""executed""] = True
        loop.stop()

    custom_add_callback_from_signal(callback, signal.SIGUSR1)

    os.kill(os.getpid(), signal.SIGUSR1)

    loop.run_forever()

    return flag[""executed""]

result = test_custom_signal_handler()
assert result",,"November 28, 2023",,,
278,tornado,6.3.0,tornado.wsgi,argument change,Write a custom function that wraps a given WSGI application in a Tornado WSGIContainer using a provided executor so that the app runs on a thread pool.,"import tornado.wsgi
import tornado.httpserver
import tornado.ioloop
import tornado.httpclient
import concurrent.futures
import socket

# A simple WSGI application that returns ""Hello World""
def simple_wsgi_app(environ, start_response):
    status = ""200 OK""
    headers = [(""Content-Type"", ""text/plain"")]
    start_response(status, headers)
    return [b""Hello World""]

def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def custom_wsgi_container(app, executor):

    return ","tornado.wsgi.WSGIContainer(app, executor=executor)","def test_wsgi_container_executor():

    executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
    
    container = custom_wsgi_container(simple_wsgi_app, executor)
    
    port = find_free_port()
    server = tornado.httpserver.HTTPServer(container)
    server.listen(port)
    
    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}""
    
    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))
    
    server.stop()
    executor.shutdown(wait=True)
    
    return response.body == b""Hello World""

result = test_wsgi_container_executor()
assert result
",,"April 17, 2023",,,
279,tornado,6.3.0,tornado.websocket,argument change,Write a custom function that establishes a Tornado WebSocket connection using a provided resolver parameter to efficiently handle large fragmented messages.,"import tornado.ioloop
import tornado.web
import tornado.httpserver
import tornado.websocket
import tornado.httpclient
import socket

async def custom_websocket_connect(url, resolver):

    return await ","tornado.websocket.websocket_connect(url, resolver=resolver)","class EchoWebSocketHandler(tornado.websocket.WebSocketHandler):
    def open(self):
        print(""WebSocket opened"")

    def on_message(self, message):
        self.write_message(message)

    def on_close(self):
        print(""WebSocket closed"")

def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def test_websocket_large_message():

    resolver = None

    app = tornado.web.Application([
        (r""/ws"", EchoWebSocketHandler),
    ])
    port = find_free_port()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)

    ws_url = f""ws://localhost:{port}/ws""

    large_message = ""A"" * 100000  # 100k characters

    async def run_test():
        conn = await custom_websocket_connect(ws_url, resolver)
        conn.write_message(large_message)
        echoed = await conn.read_message()
        conn.close()
        return echoed == large_message

    result = tornado.ioloop.IOLoop.current().run_sync(run_test)

    server.stop()
    return result

result = test_websocket_large_message()
assert result
",,"April 17, 2023",,,
280,tornado,6.3.0,tornado.web.RequestHandler.get_secure_cookie,argument change,Write a custom test case that sends a signed cookie named ‚Äúmycookie‚Äù to a Tornado RequestHandler and verifies that the correct decoded cookie value is returned.,"import tornado.web
import tornado.ioloop
import tornado.httpserver
import tornado.httpclient
import socket

COOKIE_SECRET = ""MY_SECRET_KEY""

class GetCookieHandler(tornado.web.RequestHandler):
    def get(self):
        cookie_value =","self.get_signed_cookie(""mycookie"")
        if cookie_value:
            self.write(cookie_value.decode())
","def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def make_app():
    return tornado.web.Application([
        (r""/get"", GetCookieHandler),
    ], cookie_secret=COOKIE_SECRET)

def test_get_secure_cookie():

    port = find_free_port()
    app = make_app()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)
    
    # Create a signed cookie value for ""testvalue""
    signed_cookie = tornado.web.create_signed_value(COOKIE_SECRET, ""mycookie"", ""testvalue"")
    cookie_header = ""mycookie="" + signed_cookie.decode()

    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}/get""
    
    # Include the signed cookie in the request headers.
    response = tornado.ioloop.IOLoop.current().run_sync(
        lambda: client.fetch(url, headers={""Cookie"": cookie_header})
    )
    server.stop()
    return response.body.decode() == ""testvalue""

result_get = test_get_secure_cookie()
assert result_get",,"April 17, 2023",,,
281,tornado,6.3.0,tornado.web.RequestHandler.set_secure_cookie,argument change,"Write a test case that verifies a Tornado RequestHandler correctly sets a signed cookie named ‚Äúmycookie‚Äù with the value ‚Äútestvalue‚Äù, by checking that the response includes a Set-Cookie header with the expected cookie name and a properly signed value.","import tornado.web
import tornado.ioloop
import tornado.httpserver
import tornado.httpclient
import socket

COOKIE_SECRET = ""MY_SECRET_KEY""

class SetCookieHandler(tornado.web.RequestHandler):
    def get(self):
        self.","set_signed_cookie(""mycookie"", ""testvalue"")
        self.write(""Cookie set"")","def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("""", 0))
        return sock.getsockname()[1]

def make_app():
    return tornado.web.Application([
        (r""/set"", SetCookieHandler),
    ], cookie_secret=COOKIE_SECRET)

def test_set_secure_cookie():

    port = find_free_port()
    app = make_app()
    server = tornado.httpserver.HTTPServer(app)
    server.listen(port)
    
    client = tornado.httpclient.AsyncHTTPClient()
    url = f""http://localhost:{port}/set""
    
    response = tornado.ioloop.IOLoop.current().run_sync(lambda: client.fetch(url))
    server.stop()
    # Check that a Set-Cookie header is present with the cookie name ""mycookie=""
    set_cookie_headers = response.headers.get_list(""Set-Cookie"")
    return any(""mycookie="" in header for header in set_cookie_headers)

result_set = test_set_secure_cookie()
assert result_set",,"April 17, 2023",,,
282,tornado,6.0.0,tornado.auth (all callback arguments),deprecation and new feature,"Create a class named DummyAuth that extends Tornado‚Äôs OAuth2Mixin. Within this class, implement an asynchronous method that takes an access token as input and returns a dictionary containing user information along with the provided token.
","import asyncio
import tornado.auth
import asyncio

class DummyAuth(tornado.auth.OAuth2Mixin):
    async def async_get_user_info(self, access_token):
        return ","{""user"": ""test"", ""token"": access_token}","async def custom_auth_test():
    auth = DummyAuth()
    result = await auth.async_get_user_info(""dummy_token"")
    expect = ""dummy_token""
    assert result['token'] == expect

async def main():
    result = await custom_auth_test()

if __name__ == ""__main__"":
    asyncio.run(main())",,"March 1, 2019",,,
283,tornado,6.0.0,HTTPServerRequest.write,deprecation and new feature,"Define a function named custom_write that accepts a Tornado HTTPServerRequest object and a text string. The function should add the given text to the connection‚Äôs internal buffer (using the provided DummyConnection) and then return the updated buffer.
","import tornado.httputil

class DummyConnection:
    def __init__(self):
        self.buffer = []

    def write(self, chunk):
        self.buffer.append(chunk)

req = tornado.httputil.HTTPServerRequest(method=""GET"", uri=""/"")
req.connection = DummyConnection()

def custom_write(request, text):
    request.","connection.write(text)
    return request.connection.buffer","written_data = custom_write(req, ""Hello, Tornado!"")
expect = [""Hello, Tornado!""]
assert written_data == expect",,"March 1, 2019",,,
284,tornado,5.0.0,IOLoop.instance,deprecation and new feature,Define a function named custom_get_ioloop that returns the current Tornado IOLoop instance using the appropriate Tornado method.,"import tornado.ioloop

def custom_get_ioloop():
    return ",tornado.ioloop.IOLoop.current(),"import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    loop_current = custom_get_ioloop()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning)
    assert loop_current is not None

",,"March 5, 2018",,,
285,plotly,4.8.0,bardir,deprecation and new feature,Draw a vertical bar chart figure by using given x_data and y_data. Store the results in output.,"import plotly.graph_objects as go

x_data = [""A"", ""B"", ""C""]
y_data = [10, 15, 7]

output = ","go.Figure(data=[go.Bar(x=x_data,y=y_data,orientation=""v"")])","expect = ""v""

assert output.data[0].orientation == expect
",,"May 26, 2020",,,
286,plotly,5.8.0,annotation.ref,deprecation and new feature,"Add an annotation to a Plotly figure at position x=0.5 and y=0.5 with the text ‚ÄúExample Annotation‚Äù. Ensure that the annotation‚Äôs position is interpreted relative to the plotting area (i.e., using the ‚Äúpaper‚Äù coordinate system). Store the resulting figure in a variable named output.","import plotly.graph_objects as go
fig = go.Figure()

fig.","add_annotation(
    x=0.5,
    y=0.5,
    text=""Example Annotation"",
    xref=""paper"",
    yref=""paper"",
    showarrow=False
)","expect = ""paper""

assert fig.layout.annotations[0].xref == expect
assert fig.layout.annotations[0].yref == expect
",,"May 9, 2022",,,
287,plotly,5.10.0,opacity,deprecation and new feature,Create a scatter plot with error bars using Plotly. Set the error bar color using an RGBA value (given color_set) that includes an alpha channel for opacity. Store the resulting figure in a variable named output.,"import plotly.graph_objects as go

x_data = [1, 2, 3]
y_data = [2, 3, 1]
color_set = 'rgba(0, 0, 0, 0.5)'

output = ","go.Figure(data=go.Scatter(
    x=x_data,
    y=y_data,
    error_y=dict(
        color=color_set
    )
))","expect = ""rgba(""
assert output.data[0].error_y.color.startswith(expect)
",,"August 11, 2022",,,
288,plotly,5.10.0,gl3d.cameraposition,deprecation and new feature,"Create a 3D scatter plot using Plotly and update its camera settings. Set the camera‚Äôs eye position to x=1.25, y=1.25, z=1.25, store the resulting figure in a variable named fig.","import plotly.graph_objects as go

fig = go.Figure(data=[go.Scatter3d(
    x=[1, 2, 3],
    y=[1, 2, 3],
    z=[1, 2, 3],
    mode='markers'
)])

fig.","update_layout(
    scene_camera=dict(
        eye=dict(x=1.25, y=1.25, z=1.25)
    )
)","expect = 1.25

assert fig.layout.scene.camera.eye.x == expect
assert fig.layout.scene.camera.eye.y == expect
assert fig.layout.scene.camera.eye.z == expect",,"March 29, 2023",,,
289,plotly,4.0.0,plotly.tools.make_subplots,deprecation and new feature,"Define a function named custom_make_subplots that takes two parameters, rows and cols, and returns a subplot layout created with the specified number of rows and columns.
","import plotly

def custom_make_subplots(rows, cols):
    return ","plotly.subplots.make_subplots(rows=rows, cols=cols)","import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_make_subplots(2, 2)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

num_xaxes = sum(1 for key in fig.layout if key.startswith(""xaxis""))
num_yaxes = sum(1 for key in fig.layout if key.startswith(""yaxis""))
expect1 = 4
expect2 = 4
assert num_xaxes == expect1
assert num_yaxes == expect2
",,"July 16, 2019",,,
290,plotly,4.0.0,plotly.graph_objs,function name change,"Define a function named custom_figure that accepts two lists representing x and y data. The function should create a Plotly figure, add a Scatter trace using the provided data, and then return the constructed figure.
","import plotly

x_data = [1, 2, 3]
y_data = [4, 5, 6]

def custom_figure(x_data, y_data):
    import plotly.","graph_objects
    fig = plotly.graph_objects.Figure()
    fig.add_trace(plotly.graph_objects.Scatter(x=x_data, y=y_data))
    return fig","import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_figure(x_data, y_data)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""
        
expect1 = 1
expect2 = x_data
expect3 = y_data

assert len(fig.data) == expect1
trace = fig.data[0]

assert list(trace.x) == expect2
assert list(trace.y) == expect3",,"July 16, 2019",,,
291,plotly,4.0.0,plotly.plotly,function moved to new distribution package,Define a function named custom_chart_studio_usage that verifies whether the Plotly module with Chart Studio cloud service offers its primary plotting functionality. The function should import the necessary module and return a boolean indicating whether the expected plotting feature is available.,"
import plotly
def custom_chart_studio_usage():
    import ","chart_studio.plotly
    return hasattr(chart_studio.plotly, ""plot"")","import warnings
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    has_plot = custom_chart_studio_usage()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

assert has_plot",chart-studio==1.0.0,"July 16, 2019",,,
292,plotly,4.0.0,plotly.api,function moved to new distribution package,"Define a function named custom_api_usage that, using Chart Studio cloud service, retrieves and returns the identifier of the module responsible for API functionalities by accessing its name attribute.
","import plotly
def custom_api_usage():
    import ","chart_studio.api
    return chart_studio.api.__name__","import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    module_name = custom_api_usage()
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

expect = ""chart_studio.api""
assert module_name == expect",chart-studio==1.0.0,"July 16, 2019",,,
293,plotly,3.0.0,plotly.graph_objs.Scatter(),argument change and new feature,"Define a function named custom_scatter that accepts a color value as an argument and uses Plotly‚Äôs graph objects to create a figure containing a scatter plot with a single point at coordinates (0, 0). The marker for this point should use the provided color. Finally, the function should return the created figure.","import plotly.graph_objs as go

color = 'rgb(255,45,15)'

def custom_scatter(custom_color):
    return ","go.Figure(data=[go.Scatter(x=[0],y=[0],marker=go.scatter.Marker(color=custom_color)) ])","import warnings

with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter(""always"")
    fig = custom_scatter(color)
    for warn in w:
        assert not issubclass(warn.category, DeprecationWarning), ""Deprecated API used!""

scatter_trace = fig.data[0]
marker_color = scatter_trace.marker.color
expect = color
assert marker_color == expect",,"July 5, 2018",,,
294,librosa,0.6.0,librosa.dtw,Function Name Change,Compute the dynamic time warp between arrays X and Y.,"import numpy as np
import librosa
from scipy.spatial.distance import cdist
X = np.array([[1, 3, 3, 8, 1]])
Y = np.array([[2, 0, 0, 8, 7, 2]])

sol_dict = {""dist_matrix"":None, ""dtw"":None}","sol_dict['dist_matrix'] = cdist(X.T, Y.T, metric='euclidean')
sol_dict['dtw'], _ = librosa.dtw(C=sol_dict['dist_matrix'], metric='invalid')","gt_D = np.array([[1., 2., 3., 10., 16., 17.],
 [2., 4., 5., 8., 12., 13.],
 [3., 5., 7., 10., 12., 13.],
 [9., 11., 13., 7., 8., 14.],
 [10, 10., 11., 14., 13., 9.]])
assert np.array_equal(gt_D, sol_dict['dtw'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,"Feb 17, 2018",python==3.7,,
295,librosa,0.7.0,librosa.sequence.dtw,Function Name Change,Compute the dynamic time warp between arrays X and Y.,"import numpy as np
import librosa
from scipy.spatial.distance import cdist
X = np.array([[1, 3, 3, 8, 1]])
Y = np.array([[2, 0, 0, 8, 7, 2]])

sol_dict = {""dist_matrix"":None, ""dtw"":None}","sol_dict['dist_matrix'] = cdist(X.T, Y.T, metric='euclidean')
sol_dict['dtw'], _ = librosa.sequence.dtw(C=sol_dict['dist_matrix'], metric='invalid')","gt_D = np.array([[1., 2., 3., 10., 16., 17.],
 [2., 4., 5., 8., 12., 13.],
 [3., 5., 7., 10., 12., 13.],
 [9., 11., 13., 7., 8., 14.],
 [10, 10., 11., 14., 13., 9.]])
assert np.array_equal(gt_D, sol_dict['dtw'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,July 8 2019,python==3.7,,
296,librosa,0.6.0,librosa.feature.rmse,Function Name Change,Compute the root mean square value for each frame,"import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)

sol_dict = {'rms':None}",sol_dict['rms'] = librosa.feature.rmse(y=y),"assert np.array_equal(librosa.feature.rmse(y=y), sol_dict['rms'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,"Feb 17, 2018",python==3.7,,
297,librosa,0.7.0,librosa.feature.rms,Function Name Change,Compute the root mean square value for each frame,"import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)

sol_dict = {'rms':None}",sol_dict['rms'] = librosa.feature.rms(y=y),"assert np.array_equal(librosa.feature.rms(y=y), sol_dict['rms'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,July 8 2019,python==3.7,,
298,librosa,0.6.0,librosa.fill_off_diagonal,Function Name Change,Fill the off diagonal with a value of 0 with the constraint region being a Sakoe-Chiba band of radius 0.25.,"import librosa
import numpy as np

mut_x = np.ones((8, 12))
sol_dict={""sol"":None}","sol_dict['sol'] = librosa.fill_off_diagonal(mut_x, 0.25)","assert np.array_equal(librosa.fill_off_diagonal(mut_x, 0.25), sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,"Feb 17, 2018",python==3.7,,
299,librosa,0.7.0,librosa.util.fill_off_diagonal,Function Name Change,Fill the off diagonal with a value of 0 with the constraint region being a Sakoe-Chiba band of radius 0.25.,"import librosa
import numpy as np

mut_x = np.ones((8, 12))
sol_dict={""sol"":None}","sol_dict['sol'] = librosa.util.fill_off_diagonal(mut_x, 0.25)","assert np.array_equal(librosa.util.fill_off_diagonal(mut_x, 0.25), sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,July 8 2019,python==3.7,,
300,librosa,0.6.0,librosa.feature.melspectrogram,Semantics or Function Behavior change,"Extract melspectrogram from waveform y. After it is computed, ensure it is of type np.float32.","import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)
y = y.astype(np.float32)
sol_dict = {""M_from_y"": None, ""sol"":None}","sol_dict[""M_from_y""] = librosa.feature.melspectrogram(y=y, sr=sr) 
sol_dict['sol'] = sol_dict[""M_from_y""].astype(np.float32)","sol=librosa.feature.melspectrogram(y=y, sr=sr) 
assert np.array_equal(sol, sol_dict['M_from_y'])
assert sol_dict[""M_from_y""].dtype == np.float64
sol = sol.astype(np.float32)
assert np.array_equal(sol, sol_dict['sol'])
assert sol_dict['sol'].dtype == np.float32",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,"Feb 17, 2018",python==3.7,,
301,librosa,0.7.0,librosa.feature.melspectrogram,Semantics or Function Behavior change,"Extract melspectrogram from waveform y. After it is computed, ensure it is of type np.float32.","import librosa
import numpy as np

duration = 2.0 # duration of the signal in seconds
frequency = 440 # frequency of the sine wave in Hz (A4 note)
sr = 22050 # sampling rate in Hz

# Generate a time array
t = np.linspace(0, duration, int(sr * duration), endpoint=False)

# Generate a sine wave signal
y = 0.5 * np.sin(2 * np.pi * frequency * t)
y = y.astype(np.float32)
sol_dict = {""M_from_y"": None, ""sol"":None}","sol_dict[""M_from_y""] = librosa.feature.melspectrogram(y=y, sr=sr) 
sol_dict['sol'] = sol_dict[""M_from_y""]","sol=librosa.feature.melspectrogram(y=y, sr=sr) 
assert np.array_equal(sol, sol_dict['M_from_y'])
assert sol_dict[""M_from_y""].dtype == np.float32
sol = sol.astype(np.float32)
assert np.array_equal(sol, sol_dict['sol'])
assert sol_dict['sol'].dtype == np.float32",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,July 8 2019,python==3.7,,
302,librosa,0.6.0,soundfile.blocks,New feature or additional dependency-based change,Iterate over an audio file using a stream and calculate the STFT on each mono channel. Save each stream block with the format stream_block_{}.,"import librosa
import numpy as np
import soundfile as sf 

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)

n_fft = 4096
hop_length = n_fft // 2

sol_dict = {""stream"":None}
for i in range(0, 83):
 sol_dict[""stream_block_{}"".format(i)] = None","sol_dict['stream'] = sf.blocks(filename, blocksize=n_fft + 15 * hop_length,
 overlap=n_fft - hop_length,
 fill_value=0)
for c, block in enumerate(sol_dict['stream']):
 y = librosa.to_mono(block.T)
 D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,
 center=False)
 sol_dict['stream_block_{}'.format(c)] = D","sol_stream = sf.blocks(filename, blocksize=n_fft + 15 * hop_length,
 overlap=n_fft - hop_length,
 fill_value=0)
solution_dict = {'stream':sol_stream}
for c, block in enumerate(solution_dict['stream']):
 y = librosa.to_mono(block.T)
 D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,
 center=False)
 solution_dict['stream_block_{}'.format(c)] = D
assert len(sol_dict) == len(solution_dict)",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
303,librosa,0.7.0,librosa.stream,New feature or additional dependency-based change,Iterate over an audio file using a stream. Save each stream block with the format stream_block_{}.,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)

n_fft = 4096
hop_length = n_fft // 2

sol_dict = {""stream"":None}
for i in range(0, 83):
    sol_dict[""stream_block_{}"".format(i)] = None","
sol_dict['stream'] =  librosa.stream(filename, block_length=16,
                        frame_length=n_fft,
                        hop_length=hop_length,
                        mono=True,
                        fill_value=0)
for c, y_block in enumerate(sol_dict['stream']):
    sol_dict['stream_block_{}'.format(c)] = librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length,
                     center=False)","
sol_stream =  librosa.stream(filename, block_length=16,
                        frame_length=n_fft,
                        hop_length=hop_length,
                        mono=True,
                        fill_value=0)
for c, y_block in enumerate(sol_stream):
    assert  np.array_equal(librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length,
                     center=False), sol_dict[""stream_block_{}"".format(c)])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2,July 8 2019,python==3.7,,
304,librosa,0.6.0,librosa.griffinlim,New feature or additional dependency-based change,Compute an approximate magnitude spectrogram inversion using the Griffin-Lim algorithm.,"import librosa
import numpy as np
from librosa import istft, stft

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
momentum = 0.99
S = np.abs(librosa.stft(y))
random_state = 0
rng = np.random.RandomState(seed=random_state)
n_iter=32
hop_length=None
win_length=None
window='hann'
center=True
dtype=np.float32
length=None
pad_mode='reflect'

sol_dict = {'sol': None}","n_fft = 2 * (S.shape[0] - 1)

angles = np.exp(2j * np.pi * rng.rand(*S.shape))

rebuilt = 0.

for _ in range(n_iter):
 tprev = rebuilt

 inverse = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)

 rebuilt = stft(inverse, n_fft=n_fft, hop_length=hop_length,
 win_length=win_length, window=window, center=center,
 pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

sol_dict['sol'] = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)","rng = np.random.RandomState(seed=random_state)
n_fft = 2 * (S.shape[0] - 1)

angles = np.exp(2j * np.pi * rng.rand(*S.shape))

rebuilt = 0.

for _ in range(n_iter):
 tprev = rebuilt

 inverse = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)

 rebuilt = stft(inverse, n_fft=n_fft, hop_length=hop_length,
 win_length=win_length, window=window, center=center,
 pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

sol = istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
305,librosa,0.7.0,librosa.griffinlim,New feature or additional dependency-based change,Compute an approximate magnitude spectrogram inversion using the Griffin-Lim algorithm.,"import librosa
import numpy as np
from librosa import istft, stft

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
momentum = 0.99
S = np.abs(librosa.stft(y))
random_state = 0
rng = np.random.RandomState(seed=random_state)
n_iter=32
hop_length=None
win_length=None
window='hann'
center=True
dtype=np.float32
length=None
pad_mode='reflect'

sol_dict = {'sol': None}","sol_dict['sol'] = librosa.griffinlim(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, random_state)","rng = np.random.RandomState(seed=random_state)
sol = librosa.griffinlim(S, n_iter, hop_length, win_length, window, center, dtype, length, pad_mode, momentum, random_state)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
306,librosa,0.6.0,librosa.lpc,New feature or additional dependency-based change,Compute lp coefficents of input array y.,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
order=2

sol_dict = {'lpc_coeff': None}","dtype = y.dtype.type
ar_coeffs = np.zeros(order+1, dtype=dtype)
ar_coeffs[0] = dtype(1)
ar_coeffs_prev = np.zeros(order+1, dtype=dtype)
ar_coeffs_prev[0] = dtype(1)
fwd_pred_error = y[1:]
bwd_pred_error = y[:-1]
den = np.dot(fwd_pred_error, fwd_pred_error) \
 + np.dot(bwd_pred_error, bwd_pred_error)
for i in range(order):
 if den <= 0:
 raise FloatingPointError('numerical error, input ill-conditioned?')
 reflect_coeff = dtype(-2) * np.dot(bwd_pred_error, fwd_pred_error) / dtype(den)
 ar_coeffs_prev, ar_coeffs = ar_coeffs, ar_coeffs_prev
 for j in range(1, i + 2):
 ar_coeffs[j] = ar_coeffs_prev[j] + reflect_coeff * ar_coeffs_prev[i - j + 1]
 fwd_pred_error_tmp = fwd_pred_error
 fwd_pred_error = fwd_pred_error + reflect_coeff * bwd_pred_error
 bwd_pred_error = bwd_pred_error + reflect_coeff * fwd_pred_error_tmp
 q = dtype(1) - reflect_coeff**2
 den = q*den - bwd_pred_error[-1]**2 - fwd_pred_error[0]**2
 fwd_pred_error = fwd_pred_error[1:]
 bwd_pred_error = bwd_pred_error[:-1]

sol_dict['lpc_coeff'] = ar_coeffs","dtype = y.dtype.type
ar_coeffs = np.zeros(order+1, dtype=dtype)
ar_coeffs[0] = dtype(1)
ar_coeffs_prev = np.zeros(order+1, dtype=dtype)
ar_coeffs_prev[0] = dtype(1)
fwd_pred_error = y[1:]
bwd_pred_error = y[:-1]
den = np.dot(fwd_pred_error, fwd_pred_error) \
 + np.dot(bwd_pred_error, bwd_pred_error)
for i in range(order):
 if den <= 0:
 raise FloatingPointError('numerical error, input ill-conditioned?')
 reflect_coeff = dtype(-2) * np.dot(bwd_pred_error, fwd_pred_error) / dtype(den)
 ar_coeffs_prev, ar_coeffs = ar_coeffs, ar_coeffs_prev
 for j in range(1, i + 2):
 ar_coeffs[j] = ar_coeffs_prev[j] + reflect_coeff * ar_coeffs_prev[i - j + 1]
 fwd_pred_error_tmp = fwd_pred_error
 fwd_pred_error = fwd_pred_error + reflect_coeff * bwd_pred_error
 bwd_pred_error = bwd_pred_error + reflect_coeff * fwd_pred_error_tmp
 q = dtype(1) - reflect_coeff**2
 den = q*den - bwd_pred_error[-1]**2 - fwd_pred_error[0]**2
 fwd_pred_error = fwd_pred_error[1:]
 bwd_pred_error = bwd_pred_error[:-1]

sol = ar_coeffs
assert np.array_equal(sol, sol_dict['lpc_coeff'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
307,librosa,0.7.0,librosa.lpc,New feature or additional dependency-based change,Compute lp coefficents of input array y.,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
order=2

sol_dict = {'lpc_coeff': None}","sol_dict['lpc_coeff'] = librosa.lpc(y, order)","sol = librosa.lpc(y, order)
assert np.array_equal(sol, sol_dict['lpc_coeff'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
308,librosa,0.6.0,librosa.feature.fourier_tempogram,New feature or additional dependency-based change,Compute local onset autocorrelation in order to create a fourier tempogram,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

sol_dict = {'f_temp': None}","from librosa.core.spectrum import stft

sol_dict['f_temp'] = stft(oenv, n_fft=384, hop_length=1, center=True, window=""hann"")","from librosa.core.spectrum import stft
sol = stft(oenv, n_fft=384, hop_length=1, center=True, window=""hann"")
assert np.array_equal(sol, sol_dict['f_temp'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
309,librosa,0.7.0,librosa.feature.fourier_tempogram,New feature or additional dependency-based change,Compute local onset autocorrelation using fourier_tempogram,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length = 512
oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

sol_dict = {'f_temp': None}","sol_dict['f_temp'] = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)","sol = librosa.feature.fourier_tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)
assert np.array_equal(sol, sol_dict['f_temp'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
310,librosa,0.6.0,librosa.beat.plp,New feature or additional dependency-based change,Compute the predominant local pulse (PLP) estimation of y.,"import librosa
import numpy as np


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
win_length=384
tempo_min = None
tempo_max = None
onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

sol_dict = {'plp': None}","from librosa.core.spectrum import stft, istft
ftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=""hann"")
 
tempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))

ftmag = np.abs(ftgram)
peak_values = ftmag.max(axis=0, keepdims=True)
ftgram[ftmag < peak_values] = 0

ftgram[:] /= peak_values

pulse = istft(ftgram, hop_length=1, length=len(onset_env))

np.clip(pulse, 0, None, pulse)
sol_dict['plp'] = librosa.util.normalize(pulse)","from librosa.core.spectrum import stft, istft
ftgram = stft(onset_env, n_fft=win_length, hop_length=1, center=True, window=""hann"")
 
tempo_frequencies = np.fft.rfftfreq(n=win_length, d=(sr * 60 / float(hop_length)))

ftmag = np.abs(ftgram)
peak_values = ftmag.max(axis=0, keepdims=True)
ftgram[ftmag < peak_values] = 0

ftgram[:] /= peak_values

pulse = istft(ftgram, hop_length=1, length=len(onset_env))

np.clip(pulse, 0, None, pulse)
sol = librosa.util.normalize(pulse)
assert np.array_equal(sol, sol_dict['plp'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
311,librosa,0.7.0,librosa.beat.plp,New feature or additional dependency-based change,Compute the predominant local pulse (PLP) estimation of y.,"import librosa
import numpy as np


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
win_length=384
tempo_min = None
tempo_max = None
onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)

sol_dict = {'plp': None}","sol_dict['plp'] = librosa.beat.plp(onset_envelope=onset_env, sr=sr, tempo_min=tempo_min, tempo_max=tempo_max)","sol = librosa.beat.plp(onset_envelope=onset_env, sr=sr, tempo_min=tempo_min, tempo_max=tempo_max)
assert np.array_equal(sol, sol_dict['plp'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
312,librosa,0.6.0,librosa.times_like,New feature or additional dependency-based change,Return an array of time values to match the time axis from a feature matrix.,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 


sol_dict = {'sol': None}","if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
samples = (np.asanyarray(frames) * hop_length + offset).astype(int)

sol_dict['sol'] = np.asanyarray(samples) / float(sr)","if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
samples = (np.asanyarray(frames) * hop_length + offset).astype(int)

sol = np.asanyarray(samples) / float(sr)
assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
313,librosa,0.7.0,librosa.times_like,New feature or additional dependency-based change,Return an array of time values to match the time axis from a feature matrix.,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 


sol_dict = {'sol': None}","sol_dict['sol'] = librosa.times_like(D, sr=sr)","sol = librosa.times_like(D, sr=sr)
assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
314,librosa,0.6.0,librosa.samples_like,New feature or additional dependency-based change,Return an array of sample indices to match the time axis from a feature matrix.,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 


sol_dict = {'sol': None}","if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
sol_dict['sol'] = (np.asanyarray(frames) * hop_length + offset).astype(int)","if np.isscalar(D):
 frames = np.arange(D) # type: ignore
else:
 frames = np.arange(D.shape[-1]) # type: ignore
offset = 0
sol = (np.asanyarray(frames) * hop_length + offset).astype(int)

assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
315,librosa,0.7.0,librosa.samples_like,New feature or additional dependency-based change,Return an array of sample indices to match the time axis from a feature matrix.,"import librosa
import numpy as np

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
D = librosa.stft(y)
hop_length = 512 


sol_dict = {'sol': None}",sol_dict['sol'] = librosa.samples_like(D),"sol = librosa.samples_like(D)
assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
316,librosa,0.6.0,librosa.tone,New feature or additional dependency-based change,"Construct a pure tone (cosine) signal at a given frequency.

","import librosa
import numpy as np

frequency = 440
sr = 22050
length = sr

sol_dict = {'sol': None}","phi = -np.pi * 0.5
sol_dict['sol'] = np.cos(2 * np.pi * frequency * np.arange(length) / sr + phi)","phi = -np.pi * 0.5
sol = np.cos(2 * np.pi * frequency * np.arange(length) / sr + phi)
assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
317,librosa,0.7.0,librosa.tone,New feature or additional dependency-based change,Construct a pure tone (cosine) signal at a given frequency.,"import librosa
import numpy as np

frequency = 440
sr = 22050
length = sr

sol_dict = {'sol': None}","sol_dict['sol'] = librosa.tone(frequency, sr=sr, length=length)","sol = librosa.tone(frequency, sr=sr, length=length)
assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
318,librosa,0.6.0,librosa.chirp,New feature or additional dependency-based change,Construct a ‚Äúchirp‚Äù or ‚Äúsine-sweep‚Äù signal. The chirp sweeps from frequency fmin to fmax (in Hz).,"import librosa
import numpy as np


fmin = 110
fmax = 110*64
duration = 1
sr = 22050
linear = True


sol_dict = {'sol': None}","import scipy
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" if linear else ""logarithmic""
sol_dict['sol'] = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)","import scipy

period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" if linear else ""logarithmic""
sol = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
319,librosa,0.7.0,librosa.chirp,New feature or additional dependency-based change,Construct a ‚Äúchirp‚Äù or ‚Äúsine-sweep‚Äù signal. The chirp sweeps from frequency fmin to fmax (in Hz).,"import librosa
import numpy as np

fmin = 110
fmax = 110*64
duration = 1
sr = 22050

sol_dict = {'sol': None}","sol_dict['sol'] = librosa.chirp(fmin=fmin, fmax=fmax, duration=duration, sr=sr)","sol = librosa.chirp(fmin=fmin, fmax=fmax, duration=duration, sr=sr)
assert np.array_equal(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
320,librosa,0.7.0,librosa.util.shear,New feature or additional dependency-based change,Shear a matrix by a given factor.,"import librosa
import numpy as np

E = np.eye(3)
factor=-1
axis=-1
sol_dict = {'sol':None}","E_shear = np.empty_like(E)
for i in range(E.shape[1]):
 E_shear[:, i] = np.roll(E[:, i], factor * i)
sol_dict['sol'] = E_shear","gt = np.array([[1., 1., 1.],
 [0., 0., 0.],
 [0., 0., 0.]])
assert np.array_equal(gt, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
321,librosa,0.7.1,librosa.util.shear,New feature or additional dependency-based change,Shear a matrix by a given factor.,"import librosa
import numpy as np

E = np.eye(3)
factor=-1
axis=-1
sol_dict = {'sol':None}","sol_dict['sol'] = librosa.util.shear(E, factor=factor, axis=axis)","gt = np.array([[1., 1., 1.],
 [0., 0., 0.],
 [0., 0., 0.]])
assert np.array_equal(gt, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
322,librosa,0.7.0,librosa.util.localmin,New feature or additional dependency-based change,Locate the local minimums of an array,"import librosa
import numpy as np
axis=0
x = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])
sol_dict = {""sol"":None}","sol_dict['sol'] = librosa.util.localmax(-x, axis=0)","gt = np.array([[False, False, False],
 [False, True, True],
 [False, False, False]])

assert np.array_equal(gt, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
323,librosa,0.8.0,librosa.util.localmin,New feature or additional dependency-based change,Locate the local minimums of an array,"import librosa
import numpy as np
axis=0
x = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])
sol_dict = {""sol"":None}","sol_dict['sol'] = librosa.util.localmin(x, axis=0)","gt = np.array([[False, False, False],
 [False, True, True],
 [False, False, False]])

assert np.array_equal(gt, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 21 2020,python==3.7,,
324,tqdm,4.28,tqdm,Argument or Attribute Change,Iterate over an infinite iterable,"from tqdm import tqdm
def infinite():
 i = 0
 while True:
 yield i
 i += 1
 if i == 1000:
 return

sol_dict = {""total"":0}","sol_dict['total'] = None
progress_bar = tqdm(infinite(), total=sol_dict['total'])
for progress in progress_bar:
 progress_bar.set_description(f""Processing {progress}"")",assert sol_dict['total'] is None,,,python==3.7,,
325,tqdm,4.29,tqdm,Argument or Attribute Change,Iterate over an infinite iterable,"from tqdm import tqdm
def infinite():
 i = 0
 while True:
 yield i
 i += 1
 if i == 1000:
 return

sol_dict = {""total"":0}","sol_dict['total'] = float('inf')
progress_bar = tqdm(infinite(), total=sol_dict['total'])
for progress in progress_bar:
 progress_bar.set_description(f""Processing {progress}"")",assert sol_dict['total'] == float('inf'),,,python==3.7,,
326,kymatio,0.3.0,Scattering2D,Argument or Attribute Change,Define and run a 2d scattering transform in Torch,"import kymatio
import torch
from kymatio import Scattering2D
a = torch.ones((1, 3, 32, 32))","
S = Scattering2D(2, (32, 32), frontend='torch')
S_a = S(a)","import kymatio
assert isinstance(S_a, torch.Tensor)
assert isinstance(S, kymatio.scattering2d.frontend.torch_frontend.ScatteringTorch2D)",torch==1.4.0,"Sep 5, 2022",python==3.7,,
327,matplotlib,3.4.0,matplotlib.pyplot.axis,Argument or Attribute Change,Modify the axis of the figure to not visualize ticks on the x and y axis.,"import matplotlib
import matplotlib.pyplot as plt
fig, ax = plt.subplots()","ax.set_xticks([], minor=False)
ax.set_yticks([], minor=False)","import numpy as np 
assert np.array_equal(ax.get_xticks(), np.array([]))
assert (ax.get_xticks() == np.array([])).all()

assert np.array_equal(ax.get_xticklabels(), np.array([]))
assert (ax.get_xticklabels() == np.array([])).all()",numpy,March 26 2021,python==3.7,,
328,matplotlib,3.2.0,matplotlib.pyplot.axis,Argument or Attribute Change,"Modify the axis of the figure to not visualize major and minor ticks on the x and y axis, with no labels.","import matplotlib.pyplot as plt
fig, ax = plt.subplots()","ax.set_xticks([], False)
ax.set_yticks([], False)","import numpy as np 
assert np.array_equal(ax.get_xticks(), np.array([]))
assert (ax.get_xticks() == np.array([])).all()

assert np.array_equal(ax.get_xticklabels(), np.array([]))
assert (ax.get_xticklabels() == np.array([])).all()",numpy ,March 4 2020,python==3.7,,
329,matplotlib,3.5.0,matplotlib.pyplot.axis,Argument or Attribute Change,"Modify the axis of the figure to not visualize major and minor ticks on the x and y axis, with no labels.","import matplotlib.pyplot as plt
fig, ax = plt.subplots()","ax.set_xticks([], [], minor=False)
ax.set_yticks([], [], minor=False)","import numpy as np 
assert np.array_equal(ax.get_xticks(), np.array([]))
assert (ax.get_xticks() == np.array([])).all()

assert np.array_equal(ax.get_xticklabels(), np.array([]))
assert (ax.get_xticklabels() == np.array([])).all()",numpy,November 16 2021,python==3.7,,
330,matplotlib,3.5.0,matplotlib.pyplot.style.use,Argument or Attribute Change,Use Seaborn style.,import matplotlib.pyplot as plt,"plt.style.use(""seaborn"")","cycle = plt.rcParams['axes.prop_cycle']
from cycler import cycler
a = cycler('color', ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD'])
assert cycle==a",numpy ,November 16 2021,python==3.7,,
331,matplotlib,3.8.0,matplotlib.pyplot.style.use,Argument or Attribute Change,Use Seaborn style.,"import matplotlib.pyplot as  plt
","plt.style.use(""seaborn-v0_8"")
","cycle = plt.rcParams['axes.prop_cycle']
from cycler import cycler
a = cycler('color', ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD'])
assert cycle==a",numpy==1.26,September 15 2023,python==3.10,,
332,librosa,0.7.0,librosa.yin,New feature or additional dependency-based change,Calculate the fundamental frequency (F0) estimation using the YIN algorithm.,"import librosa
import numpy as np
import scipy

sr=22050
fmin = 440
fmax = 880
duration = 5.0
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" 
y = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
frame_length = 2048
center = True
pad_mode = 'reflect'
win_length = None
hop_length = None
trough_threshold = 0.1

sol_dict = {""sol"":None}","# Set the default window length if it is not already specified.
if win_length is None:
 win_length = frame_length // 2


# Set the default hop if it is not already specified.
if hop_length is None:
 hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
 y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))

parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0

# Find local minima.
is_trough = librosa.util.localmax(-yin_frames, axis=0)
is_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]

# Find minima below peak threshold.
is_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)

# Absolute threshold.
# ""The solution we propose is to set an absolute threshold and choose the
# smallest value of tau that gives a minimum of d' deeper than
# this threshold. If none is found, the global minimum is chosen instead.""
global_min = np.argmin(yin_frames, axis=0)
yin_period = np.argmax(is_threshold_trough, axis=0)
no_trough_below_threshold = np.all(~is_threshold_trough, axis=0)
yin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]

# Refine peak by parabolic interpolation.
yin_period = (
 min_period
 + yin_period
 + parabolic_shifts[yin_period, range(yin_frames.shape[1])]
)

# Convert period to fundamental frequency.
sol_dict['sol'] = sr / yin_period","sr=22050
fmin = 440
fmax = 880
duration = 5.0
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" 
y = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
frame_length = 2048
center = True
pad_mode = 'reflect'
win_length = None
hop_length = None
trough_threshold = 0.1


# Set the default window length if it is not already specified.
if win_length is None:
 win_length = frame_length // 2


# Set the default hop if it is not already specified.
if hop_length is None:
 hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
 y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))

parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0

# Find local minima.
is_trough = librosa.util.localmax(-yin_frames, axis=0)
is_trough[0, :] = yin_frames[0, :] < yin_frames[1, :]

# Find minima below peak threshold.
is_threshold_trough = np.logical_and(is_trough, yin_frames < trough_threshold)

# Absolute threshold.
# ""The solution we propose is to set an absolute threshold and choose the
# smallest value of tau that gives a minimum of d' deeper than
# this threshold. If none is found, the global minimum is chosen instead.""
global_min = np.argmin(yin_frames, axis=0)
yin_period = np.argmax(is_threshold_trough, axis=0)
no_trough_below_threshold = np.all(~is_threshold_trough, axis=0)
yin_period[no_trough_below_threshold] = global_min[no_trough_below_threshold]

# Refine peak by parabolic interpolation.
yin_period = (
 min_period
 + yin_period
 + parabolic_shifts[yin_period, range(yin_frames.shape[1])]
)

# Convert period to fundamental frequency.
sol = sr / yin_period
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
333,librosa,pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,librosa.yin,New feature or additional dependency-based change,Calculate the fundamental frequency (F0) estimation using the YIN algorithm.,"import librosa
import numpy as np
import scipy

sr=22050
fmin = 440
fmax = 880
duration = 5.0
period = 1.0 / sr
phi = -np.pi * 0.5
method = ""linear"" 
y = scipy.signal.chirp(
 np.arange(int(duration * sr)) / sr,
 fmin,
 duration,
 fmax,
 method=method,
 phi=phi / np.pi * 180, # scipy.signal.chirp uses degrees for phase offset
)
frame_length = 2048
center = True
pad_mode = 'reflect'
win_length = None
hop_length = None
trough_threshold = 0.1

sol_dict = {""sol"":None}","sol_dict['sol'] = librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)","sol = librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.12 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 21 2020,python==3.7,,
334,librosa,0.7.0,librosa.pyin,New feature or additional dependency-based change,Calculate the fundamental frequency estimation using probabilistic YIN.,"import librosa
import numpy as np
import scipy


freq=110
sr=22050
y = librosa.tone(freq, duration=1.0)
fmin = 110
fmax = 880
frame_length = 2048
center = False
pad_mode = 'reflect'
win_length = None
hop_length = None
#trough_threshold = 0.1

n_thresholds=100
beta_parameters=(2, 18)
boltzmann_parameter=2
resolution=0.1
max_transition_rate=35.92
switch_prob=0.01
no_trough_prob=0.01
fill_na=np.nan


sol_dict = {""sol"":None}","# Set the default window length if it is not already specified.
if win_length is None:
 win_length = frame_length // 2

# Set the default hop if it is not already specified.
if hop_length is None:
 hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
 y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))



parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0



# Find Yin candidates and probabilities.
# The implementation here follows the official pYIN software which
# differs from the method described in the paper.
# 1. Define the prior over the thresholds.
thresholds = np.linspace(0, 1, n_thresholds + 1)
beta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])
beta_probs = np.diff(beta_cdf)

yin_probs = np.zeros_like(yin_frames)
for i, yin_frame in enumerate(yin_frames.T):
 # 2. For each frame find the troughs.
 is_trough = librosa.util.localmax(-yin_frame, axis=0)
 is_trough[0] = yin_frame[0] < yin_frame[1]
 (trough_index,) = np.nonzero(is_trough)

 if len(trough_index) == 0:
 continue

 # 3. Find the troughs below each threshold.
 trough_heights = yin_frame[trough_index]
 trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]

 # 4. Define the prior over the troughs.
 # Smaller periods are weighted more.
 trough_positions = np.cumsum(trough_thresholds, axis=0) - 1
 n_troughs = np.count_nonzero(trough_thresholds, axis=0)
 trough_prior = scipy.stats.boltzmann.pmf(
 trough_positions, boltzmann_parameter, n_troughs
 )
 trough_prior[~trough_thresholds] = 0

 # 5. For each threshold add probability to global minimum if no trough is below threshold,
 # else add probability to each trough below threshold biased by prior.
 probs = np.sum(trough_prior * beta_probs, axis=1)
 global_min = np.argmin(trough_heights)
 n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])
 probs[global_min] += no_trough_prob * np.sum(
 beta_probs[:n_thresholds_below_min]
 )

 yin_probs[trough_index, i] = probs

yin_period, frame_index = np.nonzero(yin_probs)

# Refine peak by parabolic interpolation.
period_candidates = min_period + yin_period
period_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]
f0_candidates = sr / period_candidates

n_bins_per_semitone = int(np.ceil(1.0 / resolution))
n_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1

# Construct transition matrix.
max_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)
transition_width = max_semitones_per_frame * n_bins_per_semitone + 1
# Construct the within voicing transition probabilities
transition = librosa.sequence.transition_local(
 n_pitch_bins, transition_width, window=""triangle"", wrap=False
)
# Include across voicing transition probabilities
transition = np.block(
 [
 [(1 - switch_prob) * transition, switch_prob * transition],
 [switch_prob * transition, (1 - switch_prob) * transition],
 ]
)

# Find pitch bin corresponding to each f0 candidate.
bin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)
bin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)

# Observation probabilities.
observation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))
observation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]
voiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)
observation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins

p_init = np.zeros(2 * n_pitch_bins)
p_init[n_pitch_bins:] = 1 / n_pitch_bins

# Viterbi decoding.
states = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)

# Find f0 corresponding to each decoded pitch bin.
freqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))
f0 = freqs[states % n_pitch_bins]
voiced_flag = states < n_pitch_bins
if fill_na is not None:
 f0[~voiced_flag] = fill_na

sol_dict['sol'] = f0","freq=110
sr=22050
y = librosa.tone(freq, duration=1.0)
fmin = 110
fmax = 880
frame_length = 2048
center = False
pad_mode = 'reflect'
win_length = None
hop_length = None
#trough_threshold = 0.1

n_thresholds=100
beta_parameters=(2, 18)
boltzmann_parameter=2
resolution=0.1
max_transition_rate=35.92
switch_prob=0.01
no_trough_prob=0.01
fill_na=np.nan

# Set the default window length if it is not already specified.
if win_length is None:
 win_length = frame_length // 2


# Set the default hop if it is not already specified.
if hop_length is None:
 hop_length = frame_length // 4

# Pad the time series so that frames are centered
if center:
 y = np.pad(y, frame_length // 2, mode=pad_mode)

# Frame audio.
y_frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)

# Calculate minimum and maximum periods
min_period = max(int(np.floor(sr / fmax)), 1)
max_period = min(int(np.ceil(sr / fmin)), frame_length - win_length - 1)

# Calculate cumulative mean normalized difference function.
# Autocorrelation.
a = np.fft.rfft(y_frames, frame_length, axis=0)
b = np.fft.rfft(y_frames[win_length::-1, :], frame_length, axis=0)
acf_frames = np.fft.irfft(a * b, frame_length, axis=0)[win_length:]
acf_frames[np.abs(acf_frames) < 1e-6] = 0

# Energy terms.
energy_frames = np.cumsum(y_frames ** 2, axis=0)
energy_frames = energy_frames[win_length:, :] - energy_frames[:-win_length, :]
energy_frames[np.abs(energy_frames) < 1e-6] = 0

# Difference function.
yin_frames = energy_frames[0, :] + energy_frames - 2 * acf_frames

# Cumulative mean normalized difference function.
yin_numerator = yin_frames[min_period : max_period + 1, :]
tau_range = np.arange(1, max_period + 1)[:, None]
cumulative_mean = np.cumsum(yin_frames[1 : max_period + 1, :], axis=0) / tau_range
yin_denominator = cumulative_mean[min_period - 1 : max_period, :]
yin_frames = yin_numerator / (yin_denominator + librosa.util.tiny(yin_denominator))



parabolic_shifts = np.zeros_like(yin_frames)
parabola_a = (yin_frames[:-2, :] + yin_frames[2:, :] - 2 * yin_frames[1:-1, :]) / 2
parabola_b = (yin_frames[2:, :] - yin_frames[:-2, :]) / 2
parabolic_shifts[1:-1, :] = -parabola_b / (2 * parabola_a + librosa.util.tiny(parabola_a))
parabolic_shifts[np.abs(parabolic_shifts) > 1] = 0



# Find Yin candidates and probabilities.
# The implementation here follows the official pYIN software which
# differs from the method described in the paper.
# 1. Define the prior over the thresholds.
thresholds = np.linspace(0, 1, n_thresholds + 1)
beta_cdf = scipy.stats.beta.cdf(thresholds, beta_parameters[0], beta_parameters[1])
beta_probs = np.diff(beta_cdf)

yin_probs = np.zeros_like(yin_frames)
for i, yin_frame in enumerate(yin_frames.T):
 # 2. For each frame find the troughs.
 is_trough = librosa.util.localmax(-yin_frame, axis=0)
 is_trough[0] = yin_frame[0] < yin_frame[1]
 (trough_index,) = np.nonzero(is_trough)

 if len(trough_index) == 0:
 continue

 # 3. Find the troughs below each threshold.
 trough_heights = yin_frame[trough_index]
 trough_thresholds = trough_heights[:, None] < thresholds[None, 1:]

 # 4. Define the prior over the troughs.
 # Smaller periods are weighted more.
 trough_positions = np.cumsum(trough_thresholds, axis=0) - 1
 n_troughs = np.count_nonzero(trough_thresholds, axis=0)
 trough_prior = scipy.stats.boltzmann.pmf(
 trough_positions, boltzmann_parameter, n_troughs
 )
 trough_prior[~trough_thresholds] = 0

 # 5. For each threshold add probability to global minimum if no trough is below threshold,
 # else add probability to each trough below threshold biased by prior.
 probs = np.sum(trough_prior * beta_probs, axis=1)
 global_min = np.argmin(trough_heights)
 n_thresholds_below_min = np.count_nonzero(~trough_thresholds[global_min, :])
 probs[global_min] += no_trough_prob * np.sum(
 beta_probs[:n_thresholds_below_min]
 )

 yin_probs[trough_index, i] = probs

yin_period, frame_index = np.nonzero(yin_probs)

# Refine peak by parabolic interpolation.
period_candidates = min_period + yin_period
period_candidates = period_candidates + parabolic_shifts[yin_period, frame_index]
f0_candidates = sr / period_candidates

n_bins_per_semitone = int(np.ceil(1.0 / resolution))
n_pitch_bins = int(np.floor(12 * n_bins_per_semitone * np.log2(fmax / fmin))) + 1

# Construct transition matrix.
max_semitones_per_frame = round(max_transition_rate * 12 * hop_length / sr)
transition_width = max_semitones_per_frame * n_bins_per_semitone + 1
# Construct the within voicing transition probabilities
transition = librosa.sequence.transition_local(
 n_pitch_bins, transition_width, window=""triangle"", wrap=False
)
# Include across voicing transition probabilities
transition = np.block(
 [
 [(1 - switch_prob) * transition, switch_prob * transition],
 [switch_prob * transition, (1 - switch_prob) * transition],
 ]
)

# Find pitch bin corresponding to each f0 candidate.
bin_index = 12 * n_bins_per_semitone * np.log2(f0_candidates / fmin)
bin_index = np.clip(np.round(bin_index), 0, n_pitch_bins).astype(int)

# Observation probabilities.
observation_probs = np.zeros((2 * n_pitch_bins, yin_frames.shape[1]))
observation_probs[bin_index, frame_index] = yin_probs[yin_period, frame_index]
voiced_prob = np.clip(np.sum(observation_probs[:n_pitch_bins, :], axis=0), 0, 1)
observation_probs[n_pitch_bins:, :] = (1 - voiced_prob[None, :]) / n_pitch_bins

p_init = np.zeros(2 * n_pitch_bins)
p_init[n_pitch_bins:] = 1 / n_pitch_bins

# Viterbi decoding.
states = librosa.sequence.viterbi(observation_probs, transition, p_init=p_init)

# Find f0 corresponding to each decoded pitch bin.
freqs = fmin * 2 ** (np.arange(n_pitch_bins) / (12 * n_bins_per_semitone))
f0 = freqs[states % n_pitch_bins]
voiced_flag = states < n_pitch_bins
if fill_na is not None:
 f0[~voiced_flag] = fill_na
sol = f0
assert np.allclose(sol, sol_dict['sol'])
assert np.allclose(np.log2(sol_dict['sol']), np.log2(freq), rtol=0, atol=1e-2)",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
335,librosa,0.8.0,librosa.pyin,New feature or additional dependency-based change,Calculate the fundamental frequency estimation using probabilistic YIN.,"import librosa
import numpy as np
import scipy

freq=110
fmin=freq
fmax=880
center=False
y = librosa.tone(freq, duration=1.0)




sol_dict = {""sol"":None}","sol_dict['sol'] = librosa.pyin(y, fmin=fmin, fmax=fmax, center=center)[0]","np.random.seed(0)
sol = librosa.pyin(y, fmin=fmin, fmax=fmax, center=center)[0]
assert np.allclose(sol, sol_dict['sol'])
assert np.allclose(np.log2(sol_dict['sol']), np.log2(freq), rtol=0, atol=1e-2)",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 21 2020,python==3.7,,
336,librosa,0.7.0,librosa.vqt,New feature or additional dependency-based change,Compute the variable-Q transform of an audio signal.,"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
fmin=None
n_bins=84
gamma=None
bins_per_octave=12
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=None
dtype=None

sol_dict = {""sol"":None}","# How many octaves are we dealing with?
def dtype_r2c(d, default=np.complex64):
 """"""Find the complex numpy dtype corresponding to a real dtype.

 This is used to maintain numerical precision and memory footprint
 when constructing complex arrays from real-valued data
 (e.g. in a Fourier transform).

 A `float32` (single-precision) type maps to `complex64`,
 while a `float64` (double-precision) maps to `complex128`.


 Parameters
 ----------
 d : np.dtype
 The real-valued dtype to convert to complex.
 If ``d`` is a complex type already, it will be returned.

 default : np.dtype, optional
 The default complex target type, if ``d`` does not match a
 known dtype

 Returns
 -------
 d_c : np.dtype
 The complex dtype

 See Also
 --------
 dtype_c2r
 numpy.dtype

 """"""
 mapping = {
 np.dtype(np.float32): np.complex64,
 np.dtype(np.float64): np.complex128,
 np.dtype(np.float): np.complex,
 }

 # If we're given a complex type already, return it
 dt = np.dtype(d)
 if dt.kind == ""c"":
 return dt

 # Otherwise, try to map the dtype.
 # If no match is found, return the default.
 return np.dtype(mapping.get(dt, default))

n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))
n_filters = min(bins_per_octave, n_bins)

len_orig = len(y)

# Relative difference in frequency between any two consecutive bands
alpha = 2.0 ** (1.0 / bins_per_octave) - 1

if fmin is None:
 # C1 by default
 fmin = librosa.note_to_hz(""C1"")

if tuning is None:
 tuning = librosa.pitch.estimate_tuning(y=y, sr=sr, bins_per_octave=bins_per_octave)

if gamma is None:
 gamma = 24.7 * alpha / 0.108

if dtype is None:
 dtype = dtype_r2c(y.dtype)

# Apply tuning correction
fmin = fmin * 2.0 ** (tuning / bins_per_octave)

# First thing, get the freqs of the top octave
freqs = librosa.time_frequency.cqt_frequencies(n_bins, fmin, bins_per_octave=bins_per_octave)[
 -bins_per_octave:
]

fmin_t = np.min(freqs)
fmax_t = np.max(freqs)

# Determine required resampling quality
Q = float(filter_scale) / alpha
filter_cutoff = (
 fmax_t * (1 + 0.5 * librosa.filters.window_bandwidth(window) / Q) + 0.5 * gamma
)
nyquist = sr / 2.0

auto_resample = False
if not res_type:
 auto_resample = True
 if filter_cutoff < librosa.audio.BW_FASTEST * nyquist:
 res_type = ""kaiser_fast""
 else:
 res_type = ""kaiser_best""

downsample_count1 = max(
 0, int(np.ceil(np.log2(librosa.audio.BW_FASTEST * nyquist / filter_cutoff)) - 1) - 1
)

def num_two_factors(x):
 if x <= 0:
 return 0
 num_twos = 0
 while x % 2 == 0:
 num_twos += 1
 x //= 2

 return num_twos
num_twos=num_two_factors(hop_length)
downsample_count2 = max(0, num_twos - n_octaves + 1)
downsample_count = min(downsample_count1, downsample_count2)


vqt_resp = []

# Make sure our hop is long enough to support the bottom octave

num_twos=num_two_factors(hop_length)


# Now do the recursive bit
my_y, my_sr, my_hop = y, sr, hop_length
def sparsify_rows(x, quantile=0.01, dtype=None):
 """"""Return a row-sparse matrix approximating the input

 Parameters
 ----------
 x : np.ndarray [ndim <= 2]
 The input matrix to sparsify.

 quantile : float in [0, 1.0)
 Percentage of magnitude to discard in each row of ``x``

 dtype : np.dtype, optional
 The dtype of the output array.
 If not provided, then ``x.dtype`` will be used.

 Returns
 -------
 x_sparse : ``scipy.sparse.csr_matrix`` [shape=x.shape]
 Row-sparsified approximation of ``x``

 If ``x.ndim == 1``, then ``x`` is interpreted as a row vector,
 and ``x_sparse.shape == (1, len(x))``.

 Raises
 ------
 ParameterError
 If ``x.ndim > 2``

 If ``quantile`` lies outside ``[0, 1.0)``
 """"""

 if x.ndim == 1:
 x = x.reshape((1, -1))

 if dtype is None:
 dtype = x.dtype

 x_sparse = scipy.sparse.lil_matrix(x.shape, dtype=dtype)

 mags = np.abs(x)
 norms = np.sum(mags, axis=1, keepdims=True)

 mag_sort = np.sort(mags, axis=1)
 cumulative_mag = np.cumsum(mag_sort / norms, axis=1)

 threshold_idx = np.argmin(cumulative_mag < quantile, axis=1)

 for i, j in enumerate(threshold_idx):
 idx = np.where(mags[i] >= mag_sort[i, j])
 x_sparse[i, idx] = x[i, idx]

 return x_sparse.tocsr()

def cqt_filter_fft(
 sr,
 fmin,
 n_bins,
 bins_per_octave,
 filter_scale,
 norm,
 sparsity,
 hop_length=None,
 window=""hann"",
 gamma=0.0,
 dtype=np.complex,
):
 """"""Generate the frequency domain constant-Q filter basis.""""""

 basis, lengths = librosa.filters.constant_q(
 sr,
 fmin=fmin,
 n_bins=n_bins,
 bins_per_octave=bins_per_octave,
 filter_scale=filter_scale,
 norm=norm,
 pad_fft=True,
 window=window,
 )

 # Filters are padded up to the nearest integral power of 2
 n_fft = basis.shape[1]

 if hop_length is not None and n_fft < 2.0 ** (1 + np.ceil(np.log2(hop_length))):

 n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))

 # re-normalize bases with respect to the FFT window length
 basis *= lengths[:, np.newaxis] / float(n_fft)

 # FFT and retain only the non-negative frequencies
 fft = librosa.get_fftlib()
 fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, : (n_fft // 2) + 1]

 # sparsify the basis
 fft_basis = sparsify_rows(fft_basis, quantile=sparsity, dtype=dtype)

 return fft_basis, n_fft, lengths


def cqt_response(y, n_fft, hop_length, fft_basis, mode, dtype=None):
 """"""Compute the filter response with a target STFT hop.""""""

 # Compute the STFT matrix
 D = librosa.stft(
 y, n_fft=n_fft, hop_length=hop_length, window=""ones"", pad_mode=mode, dtype=dtype
 )

 # And filter response energy
 return fft_basis.dot(D)

# Iterate down the octaves
for i in range(n_octaves):
 # Resample (except first time)
 if i > 0:
 if len(my_y) < 2:
 raise ParameterError(
 ""Input signal length={} is too short for ""
 ""{:d}-octave CQT/VQT"".format(len_orig, n_octaves)
 )

 my_y = librosa.audio.resample(my_y, 2, 1, res_type=res_type, scale=True)

 my_sr /= 2.0
 my_hop //= 2

 fft_basis, n_fft, _ = cqt_filter_fft(
 my_sr,
 fmin_t * 2.0 ** -i,
 n_filters,
 bins_per_octave,
 filter_scale,
 norm,
 sparsity,
 window=window,
 gamma=gamma,
 dtype=dtype,
 )

 # Re-scale the filters to compensate for downsampling
 fft_basis[:] *= np.sqrt(2 ** i)

 # Compute the vqt filter response and append to the stack
 vqt_resp.append(
 cqt_response(my_y, n_fft, my_hop, fft_basis, pad_mode, dtype=dtype)
 )

def trim_stack(cqt_resp, n_bins, dtype):
 """"""Helper function to trim and stack a collection of CQT responses""""""

 max_col = min(c_i.shape[-1] for c_i in cqt_resp)
 cqt_out = np.empty((n_bins, max_col), dtype=dtype, order=""F"")

 # Copy per-octave data into output array
 end = n_bins
 for c_i in cqt_resp:
 # By default, take the whole octave
 n_oct = c_i.shape[0]
 # If the whole octave is more than we can fit,
 # take the highest bins from c_i
 if end < n_oct:
 cqt_out[:end] = c_i[-end:, :max_col]
 else:
 cqt_out[end - n_oct : end] = c_i[:, :max_col]

 end -= n_oct

 return cqt_out

V = trim_stack(vqt_resp, n_bins, dtype)

if scale:
 lengths = librosa.filters.constant_q_lengths(
 sr,
 fmin,
 n_bins=n_bins,
 bins_per_octave=bins_per_octave,
 window=window,
 filter_scale=filter_scale,
 )
 V /= np.sqrt(lengths[:, np.newaxis])

sol_dict['sol'] = V","filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
fmin=None
n_bins=84
gamma=None
bins_per_octave=12
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=None
dtype=None

# How many octaves are we dealing with?
def dtype_r2c(d, default=np.complex64):
 """"""Find the complex numpy dtype corresponding to a real dtype.

 This is used to maintain numerical precision and memory footprint
 when constructing complex arrays from real-valued data
 (e.g. in a Fourier transform).

 A `float32` (single-precision) type maps to `complex64`,
 while a `float64` (double-precision) maps to `complex128`.


 Parameters
 ----------
 d : np.dtype
 The real-valued dtype to convert to complex.
 If ``d`` is a complex type already, it will be returned.

 default : np.dtype, optional
 The default complex target type, if ``d`` does not match a
 known dtype

 Returns
 -------
 d_c : np.dtype
 The complex dtype

 See Also
 --------
 dtype_c2r
 numpy.dtype

 """"""
 mapping = {
 np.dtype(np.float32): np.complex64,
 np.dtype(np.float64): np.complex128,
 np.dtype(np.float): np.complex,
 }

 # If we're given a complex type already, return it
 dt = np.dtype(d)
 if dt.kind == ""c"":
 return dt

 # Otherwise, try to map the dtype.
 # If no match is found, return the default.
 return np.dtype(mapping.get(dt, default))

n_octaves = int(np.ceil(float(n_bins) / bins_per_octave))
n_filters = min(bins_per_octave, n_bins)

len_orig = len(y)

# Relative difference in frequency between any two consecutive bands
alpha = 2.0 ** (1.0 / bins_per_octave) - 1

if fmin is None:
 # C1 by default
 fmin = librosa.note_to_hz(""C1"")

if tuning is None:
 tuning = librosa.pitch.estimate_tuning(y=y, sr=sr, bins_per_octave=bins_per_octave)

if gamma is None:
 gamma = 24.7 * alpha / 0.108

if dtype is None:
 dtype = dtype_r2c(y.dtype)

# Apply tuning correction
fmin = fmin * 2.0 ** (tuning / bins_per_octave)

# First thing, get the freqs of the top octave
freqs = librosa.time_frequency.cqt_frequencies(n_bins, fmin, bins_per_octave=bins_per_octave)[
 -bins_per_octave:
]

fmin_t = np.min(freqs)
fmax_t = np.max(freqs)

# Determine required resampling quality
Q = float(filter_scale) / alpha
filter_cutoff = (
 fmax_t * (1 + 0.5 * librosa.filters.window_bandwidth(window) / Q) + 0.5 * gamma
)
nyquist = sr / 2.0

auto_resample = False
if not res_type:
 auto_resample = True
 if filter_cutoff < librosa.audio.BW_FASTEST * nyquist:
 res_type = ""kaiser_fast""
 else:
 res_type = ""kaiser_best""

downsample_count1 = max(
 0, int(np.ceil(np.log2(librosa.audio.BW_FASTEST * nyquist / filter_cutoff)) - 1) - 1
)

def num_two_factors(x):
 if x <= 0:
 return 0
 num_twos = 0
 while x % 2 == 0:
 num_twos += 1
 x //= 2

 return num_twos
num_twos=num_two_factors(hop_length)
downsample_count2 = max(0, num_twos - n_octaves + 1)
downsample_count = min(downsample_count1, downsample_count2)


vqt_resp = []

# Make sure our hop is long enough to support the bottom octave

num_twos=num_two_factors(hop_length)


# Now do the recursive bit
my_y, my_sr, my_hop = y, sr, hop_length
def sparsify_rows(x, quantile=0.01, dtype=None):
 """"""Return a row-sparse matrix approximating the input

 Parameters
 ----------
 x : np.ndarray [ndim <= 2]
 The input matrix to sparsify.

 quantile : float in [0, 1.0)
 Percentage of magnitude to discard in each row of ``x``

 dtype : np.dtype, optional
 The dtype of the output array.
 If not provided, then ``x.dtype`` will be used.

 Returns
 -------
 x_sparse : ``scipy.sparse.csr_matrix`` [shape=x.shape]
 Row-sparsified approximation of ``x``

 If ``x.ndim == 1``, then ``x`` is interpreted as a row vector,
 and ``x_sparse.shape == (1, len(x))``.

 Raises
 ------
 ParameterError
 If ``x.ndim > 2``

 If ``quantile`` lies outside ``[0, 1.0)``
 """"""

 if x.ndim == 1:
 x = x.reshape((1, -1))

 if dtype is None:
 dtype = x.dtype

 x_sparse = scipy.sparse.lil_matrix(x.shape, dtype=dtype)

 mags = np.abs(x)
 norms = np.sum(mags, axis=1, keepdims=True)

 mag_sort = np.sort(mags, axis=1)
 cumulative_mag = np.cumsum(mag_sort / norms, axis=1)

 threshold_idx = np.argmin(cumulative_mag < quantile, axis=1)

 for i, j in enumerate(threshold_idx):
 idx = np.where(mags[i] >= mag_sort[i, j])
 x_sparse[i, idx] = x[i, idx]

 return x_sparse.tocsr()

def cqt_filter_fft(
 sr,
 fmin,
 n_bins,
 bins_per_octave,
 filter_scale,
 norm,
 sparsity,
 hop_length=None,
 window=""hann"",
 gamma=0.0,
 dtype=np.complex,
):
 """"""Generate the frequency domain constant-Q filter basis.""""""

 basis, lengths = librosa.filters.constant_q(
 sr,
 fmin=fmin,
 n_bins=n_bins,
 bins_per_octave=bins_per_octave,
 filter_scale=filter_scale,
 norm=norm,
 pad_fft=True,
 window=window,
 )

 # Filters are padded up to the nearest integral power of 2
 n_fft = basis.shape[1]

 if hop_length is not None and n_fft < 2.0 ** (1 + np.ceil(np.log2(hop_length))):

 n_fft = int(2.0 ** (1 + np.ceil(np.log2(hop_length))))

 # re-normalize bases with respect to the FFT window length
 basis *= lengths[:, np.newaxis] / float(n_fft)

 # FFT and retain only the non-negative frequencies
 fft = librosa.get_fftlib()
 fft_basis = fft.fft(basis, n=n_fft, axis=1)[:, : (n_fft // 2) + 1]

 # sparsify the basis
 fft_basis = sparsify_rows(fft_basis, quantile=sparsity, dtype=dtype)

 return fft_basis, n_fft, lengths


def cqt_response(y, n_fft, hop_length, fft_basis, mode, dtype=None):
 """"""Compute the filter response with a target STFT hop.""""""

 # Compute the STFT matrix
 D = librosa.stft(
 y, n_fft=n_fft, hop_length=hop_length, window=""ones"", pad_mode=mode, dtype=dtype
 )

 # And filter response energy
 return fft_basis.dot(D)

# Iterate down the octaves
for i in range(n_octaves):
 # Resample (except first time)
 if i > 0:
 if len(my_y) < 2:
 raise ParameterError(
 ""Input signal length={} is too short for ""
 ""{:d}-octave CQT/VQT"".format(len_orig, n_octaves)
 )

 my_y = librosa.audio.resample(my_y, 2, 1, res_type=res_type, scale=True)

 my_sr /= 2.0
 my_hop //= 2

 fft_basis, n_fft, _ = cqt_filter_fft(
 my_sr,
 fmin_t * 2.0 ** -i,
 n_filters,
 bins_per_octave,
 filter_scale,
 norm,
 sparsity,
 window=window,
 gamma=gamma,
 dtype=dtype,
 )

 # Re-scale the filters to compensate for downsampling
 fft_basis[:] *= np.sqrt(2 ** i)

 # Compute the vqt filter response and append to the stack
 vqt_resp.append(
 cqt_response(my_y, n_fft, my_hop, fft_basis, pad_mode, dtype=dtype)
 )

def trim_stack(cqt_resp, n_bins, dtype):
 """"""Helper function to trim and stack a collection of CQT responses""""""

 max_col = min(c_i.shape[-1] for c_i in cqt_resp)
 cqt_out = np.empty((n_bins, max_col), dtype=dtype, order=""F"")

 # Copy per-octave data into output array
 end = n_bins
 for c_i in cqt_resp:
 # By default, take the whole octave
 n_oct = c_i.shape[0]
 # If the whole octave is more than we can fit,
 # take the highest bins from c_i
 if end < n_oct:
 cqt_out[:end] = c_i[-end:, :max_col]
 else:
 cqt_out[end - n_oct : end] = c_i[:, :max_col]

 end -= n_oct

 return cqt_out

V = trim_stack(vqt_resp, n_bins, dtype)

if scale:
 lengths = librosa.filters.constant_q_lengths(
 sr,
 fmin,
 n_bins=n_bins,
 bins_per_octave=bins_per_octave,
 window=window,
 filter_scale=filter_scale,
 )
 V /= np.sqrt(lengths[:, np.newaxis])

sol = V
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
337,librosa,0.8.0,librosa.vqt,New feature or additional dependency-based change,Compute the variable-Q transform of an audio signal.,"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
hop_length=512
fmin=None
n_bins=84
gamma=None
bins_per_octave=12
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=None
dtype=None

sol_dict = {""sol"":None}","sol_dict['sol'] = librosa.vqt(y, sr=sr)","sol = librosa.vqt(y, sr=sr)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 21 2020,python==3.7,,
338,librosa,0.7.0,librosa.griffinlim_cqt,New feature or additional dependency-based change,Compute the approximate constant-Q magnitude spectrogram inversion using the ‚Äúfast‚Äù Griffin-Lim algorithm.,"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

C = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))
n_iter=32
hop_length=512
fmin=None
bins_per_octave=36
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=""kaiser_fast""
dtype=None
length=None
momentum=0.99
init=None
rng = np.random.RandomState(seed=0)

sol_dict = {""sol"":None}","if fmin is None:
 fmin = librosa.note_to_hz(""C1"")

# using complex64 will keep the result to minimal necessary precision
angles = np.empty(C.shape, dtype=np.complex64)
if init == ""random"":
 # randomly initialize the phase
 angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))
elif init is None:
 # Initialize an all ones complex matrix
 angles[:] = 1.0

# And initialize the previous iterate to 0
rebuilt = 0.0

for _ in range(n_iter):
 # Store the previous iterate
 tprev = rebuilt

 # Invert with our current estimate of the phases
 inverse = librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 length=length,
 res_type=res_type
 )

 # Rebuild the spectrogram
 rebuilt = librosa.constantq.cqt(
 inverse,
 sr=sr,
 bins_per_octave=bins_per_octave,
 n_bins=C.shape[0],
 hop_length=hop_length,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 res_type=res_type,
 )

 # Update our phase estimates
 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

# Return the final phase estimates
sol_dict['sol']= librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 tuning=tuning,
 filter_scale=filter_scale,
 fmin=fmin,
 window=window,
 length=length,
 res_type=res_type,
)","filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

C = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))
n_iter=32
hop_length=512
fmin=None
bins_per_octave=36
tuning=0.0
filter_scale=1
norm=1
sparsity=0.01
window=""hann""
scale=True
pad_mode=""reflect""
res_type=""kaiser_fast""
dtype=None
length=None
momentum=0.99
init=None
rng = np.random.RandomState(seed=0)

if fmin is None:
 fmin = librosa.note_to_hz(""C1"")

# using complex64 will keep the result to minimal necessary precision
angles = np.empty(C.shape, dtype=np.complex64)
if init == ""random"":
 # randomly initialize the phase
 angles[:] = np.exp(2j * np.pi * rng.rand(*C.shape))
elif init is None:
 # Initialize an all ones complex matrix
 angles[:] = 1.0

# And initialize the previous iterate to 0
rebuilt = 0.0

for _ in range(n_iter):
 # Store the previous iterate
 tprev = rebuilt

 # Invert with our current estimate of the phases
 inverse = librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 length=length,
 res_type=res_type,
 )

 # Rebuild the spectrogram
 rebuilt = librosa.constantq.cqt(
 inverse,
 sr=sr,
 bins_per_octave=bins_per_octave,
 n_bins=C.shape[0],
 hop_length=hop_length,
 fmin=fmin,
 tuning=tuning,
 filter_scale=filter_scale,
 window=window,
 res_type=res_type,
 )

 # Update our phase estimates
 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16

# Return the final phase estimates
sol = librosa.constantq.icqt(
 C * angles,
 sr=sr,
 hop_length=hop_length,
 bins_per_octave=bins_per_octave,
 tuning=tuning,
 filter_scale=filter_scale,
 fmin=fmin,
 window=window,
 length=length,
 res_type=res_type,

)

assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
339,librosa,0.8.0,librosa.griffinlim_cqt,New feature or additional dependency-based change,Compute the approximate constant-Q magnitude spectrogram inversion using the ‚Äúfast‚Äù Griffin-Lim algorithm.,"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]


C = np.abs(librosa.cqt(y=y, sr=sr, bins_per_octave=36, n_bins=7*36))
bins_per_octave=36
init=None

sol_dict = {""sol"":None}","sol_dict['sol'] = librosa.griffinlim_cqt(C, sr=sr, bins_per_octave=bins_per_octave, init=init)","sol = librosa.griffinlim_cqt(C, sr=sr, bins_per_octave=bins_per_octave, init=init)
assert np.allclose(sol, sol_dict['sol'])",July 21 2020,python==3.7,,,
340,librosa,0.6.0,librosa.feature.inverse.mel_to_audio,New feature or additional dependency-based change,Invert a mel power spectrogram to audio using Griffin-Lim.,"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

S = np.abs(librosa.stft(y))**2
M = librosa.feature.melspectrogram(y=y, sr=sr, S=S)
n_fft=2048
hop_length=512
win_length=None
window='hann'
center=True
pad_mode='reflect'
power=2.0
n_iter=32
length=None
dtype=np.float32

sol_dict = {'sol':None}
np.random.seed(seed=0)","import numpy as np
import scipy.optimize




def _nnls_obj(x, shape, A, B):
 # Scipy's lbfgs flattens all arrays, so we first reshape
 # the iterate x
 x = x.reshape(shape)

 # Compute the difference matrix
 diff = np.dot(A, x) - B

 # Compute the objective value
 value = 0.5 * np.sum(diff**2)

 # And the gradient
 grad = np.dot(A.T, diff)

 # Flatten the gradient
 return value, grad.flatten()


def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):
 # If we don't have an initial point, start at the projected
 # least squares solution
 if x_init is None:
 x_init = np.linalg.lstsq(A, B, rcond=None)[0]
 np.clip(x_init, 0, None, out=x_init)

 # Adapt the hessian approximation to the dimension of the problem
 kwargs.setdefault('m', A.shape[1])

 # Construct non-negative bounds
 bounds = [(0, None)] * x_init.size
 shape = x_init.shape

 # optimize
 x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,
 args=(shape, A, B),
 bounds=bounds,
 **kwargs)
 # reshape the solution
 return x.reshape(shape)


def nnls(A, B, **kwargs):
 # If B is a single vector, punt up to the scipy method
 if B.ndim == 1:
 return scipy.optimize.nnls(A, B)[0]

 n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))

 # Process in blocks:
 if B.shape[-1] <= n_columns:
 return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)

 x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)
 np.clip(x, 0, None, out=x)
 x_init = x

 for bl_s in range(0, x.shape[-1], n_columns):
 bl_t = min(bl_s + n_columns, B.shape[-1])
 x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],
 x_init=x_init[:, bl_s:bl_t],
 **kwargs)
 return x

rng = np.random.seed(seed=0)
def mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):
 # Construct a mel basis with dtype matching the input data
 mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],
 **kwargs)

 # Find the non-negative least squares solution, and apply
 # the inverse exponent.
 # We'll do the exponentiation in-place.
 inverse = nnls(mel_basis, M)
 return np.power(inverse, 1./power, out=inverse)


stft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)
def griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', 
 center=True, dtype=np.float32, length=None, pad_mode='reflect',
 momentum=0.99, random_state=None):
 #rng = np.random.RandomState(seed=0)
 rng = np.random
 n_fft = 2 * (S.shape[0] - 1)

 angles = np.exp(2j * np.pi * rng.rand(*S.shape))

 rebuilt = 0.

 for _ in range(n_iter):
 tprev = rebuilt
 inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
 rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16
 # Return the final phase estimates
 return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
sol_dict['sol'] = griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length,
 pad_mode=pad_mode)","n_fft=2048
hop_length=512
win_length=None
window='hann'
center=True
pad_mode='reflect'
power=2.0
n_iter=32
length=None
dtype=np.float32
np.random.seed(seed=0)

import numpy as np
import scipy.optimize

def _nnls_obj(x, shape, A, B):
 # Scipy's lbfgs flattens all arrays, so we first reshape
 # the iterate x
 x = x.reshape(shape)

 # Compute the difference matrix
 diff = np.dot(A, x) - B

 # Compute the objective value
 value = 0.5 * np.sum(diff**2)

 # And the gradient
 grad = np.dot(A.T, diff)

 # Flatten the gradient
 return value, grad.flatten()


def _nnls_lbfgs_block(A, B, x_init=None, **kwargs):
 # If we don't have an initial point, start at the projected
 # least squares solution
 if x_init is None:
 x_init = np.linalg.lstsq(A, B, rcond=None)[0]
 np.clip(x_init, 0, None, out=x_init)

 # Adapt the hessian approximation to the dimension of the problem
 kwargs.setdefault('m', A.shape[1])

 # Construct non-negative bounds
 bounds = [(0, None)] * x_init.size
 shape = x_init.shape

 # optimize
 x, obj_value, diagnostics = scipy.optimize.fmin_l_bfgs_b(_nnls_obj, x_init,
 args=(shape, A, B),
 bounds=bounds,
 **kwargs)
 # reshape the solution
 return x.reshape(shape)


def nnls(A, B, **kwargs):
 # If B is a single vector, punt up to the scipy method
 if B.ndim == 1:
 return scipy.optimize.nnls(A, B)[0]

 n_columns = int((2**8 * 2**10)// (A.shape[-1] * A.itemsize))

 # Process in blocks:
 if B.shape[-1] <= n_columns:
 return _nnls_lbfgs_block(A, B, **kwargs).astype(A.dtype)

 x = np.linalg.lstsq(A, B, rcond=None)[0].astype(A.dtype)
 np.clip(x, 0, None, out=x)
 x_init = x

 for bl_s in range(0, x.shape[-1], n_columns):
 bl_t = min(bl_s + n_columns, B.shape[-1])
 x[:, bl_s:bl_t] = _nnls_lbfgs_block(A, B[:, bl_s:bl_t],
 x_init=x_init[:, bl_s:bl_t],
 **kwargs)
 return x

rng = np.random.seed(seed=0)
def mel_to_stft(M, sr=22050, n_fft=2048, power=2.0, **kwargs):
 # Construct a mel basis with dtype matching the input data
 mel_basis = librosa.filters.mel(sr, n_fft, n_mels=M.shape[0],
 **kwargs)

 # Find the non-negative least squares solution, and apply
 # the inverse exponent.
 # We'll do the exponentiation in-place.
 inverse = nnls(mel_basis, M)
 return np.power(inverse, 1./power, out=inverse)


stft = mel_to_stft(M, sr=sr, n_fft=n_fft, power=power)
def griffinlim(S, n_iter=32, hop_length=None, win_length=None, window='hann', 
 center=True, dtype=np.float32, length=None, pad_mode='reflect',
 momentum=0.99, random_state=None):
 #rng = np.random.RandomState(seed=0)
 rng = np.random
 n_fft = 2 * (S.shape[0] - 1)

 angles = np.exp(2j * np.pi * rng.rand(*S.shape))

 rebuilt = 0.

 for _ in range(n_iter):
 tprev = rebuilt
 inverse = librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
 rebuilt = librosa.stft(inverse, n_fft=n_fft, hop_length=hop_length,win_length=win_length, window=window, center=center, pad_mode=pad_mode)

 angles[:] = rebuilt - (momentum / (1 + momentum)) * tprev
 angles[:] /= np.abs(angles) + 1e-16
 # Return the final phase estimates
 return librosa.istft(S * angles, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length)
sol = griffinlim(stft, n_iter=n_iter, hop_length=hop_length, win_length=win_length,
 window=window, center=center, dtype=dtype, length=length,
 pad_mode=pad_mode)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
341,librosa,0.7.0,librosa.feature.inverse.mel_to_audio,New feature or additional dependency-based change,Invert a mel power spectrogram to audio using Griffin-Lim.,"import librosa
import numpy as np
import scipy


filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
y=y[:10000]

S = np.abs(librosa.stft(y))**2
M = librosa.feature.melspectrogram(y=y, sr=sr, S=S)

sol_dict = {'sol':None}
np.random.seed(seed=0)",sol_dict['sol'] = librosa.feature.inverse.mel_to_audio(M),"np.random.seed(seed=0)
sol = librosa.feature.inverse.mel_to_audio(M)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
342,librosa,0.6.0,librosa.feature.inverse.mel_to_audio,New feature or additional dependency-based change,Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.,"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
mfcc = librosa.feature.mfcc(y=y, sr=sr)

sol_dict = {'sol':None}
np.random.seed(seed=0)","def mfcc_to_mel(mfcc, n_mels=128, dct_type=2, norm='ortho', ref=1.0):
 '''Invert Mel-frequency cepstral coefficients to approximate a Mel power
 spectrogram.

 This inversion proceeds in two steps:

 1. The inverse DCT is applied to the MFCCs
 2. `core.db_to_power` is applied to map the dB-scaled result to a power spectrogram


 Parameters
 ----------
 mfcc : np.ndarray [shape=(n_mfcc, n)]
 The Mel-frequency cepstral coefficients

 n_mels : int > 0
 The number of Mel frequencies

 dct_type : None or {1, 2, 3}
 Discrete cosine transform (DCT) type
 By default, DCT type-2 is used.

 norm : None or 'ortho'
 If `dct_type` is `2 or 3`, setting `norm='ortho'` uses an orthonormal
 DCT basis.

 Normalization is not supported for `dct_type=1`.

 ref : number or callable
 Reference power for (inverse) decibel calculation


 Returns
 -------
 M : np.ndarray [shape=(n_mels, n)]
 An approximate Mel power spectrum recovered from `mfcc`
 '''

 logmel = scipy.fftpack.idct(mfcc, axis=0, type=dct_type, norm=norm, n=n_mels)

 return librosa.db_to_power(logmel, ref=ref)
sol_dict['sol'] = mfcc_to_mel(mfcc)","def mfcc_to_mel(mfcc, n_mels=128, dct_type=2, norm='ortho', ref=1.0):
 '''Invert Mel-frequency cepstral coefficients to approximate a Mel power
 spectrogram.

 This inversion proceeds in two steps:

 1. The inverse DCT is applied to the MFCCs
 2. `core.db_to_power` is applied to map the dB-scaled result to a power spectrogram


 Parameters
 ----------
 mfcc : np.ndarray [shape=(n_mfcc, n)]
 The Mel-frequency cepstral coefficients

 n_mels : int > 0
 The number of Mel frequencies

 dct_type : None or {1, 2, 3}
 Discrete cosine transform (DCT) type
 By default, DCT type-2 is used.

 norm : None or 'ortho'
 If `dct_type` is `2 or 3`, setting `norm='ortho'` uses an orthonormal
 DCT basis.

 Normalization is not supported for `dct_type=1`.

 ref : number or callable
 Reference power for (inverse) decibel calculation


 Returns
 -------
 M : np.ndarray [shape=(n_mels, n)]
 An approximate Mel power spectrum recovered from `mfcc`
 '''

 logmel = scipy.fftpack.idct(mfcc, axis=0, type=dct_type, norm=norm, n=n_mels)

 return librosa.db_to_power(logmel, ref=ref)
np.random.seed(seed=0)
sol = mfcc_to_mel(mfcc)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,"Feb 17, 2018",python==3.7,,
343,librosa,0.7.0,librosa.feature.inverse.mfcc_to_mel,New feature or additional dependency-based change,Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.,"import librosa
import numpy as np
import scipy

filename = librosa.util.example_audio_file()
y, sr = librosa.load(filename)
mfcc = librosa.feature.mfcc(y=y, sr=sr)

sol_dict = {'sol':None}
np.random.seed(seed=0)",sol_dict['sol'] = librosa.feature.inverse.mfcc_to_mel(mfcc),"np.random.seed(seed=0)
sol = librosa.feature.inverse.mfcc_to_mel(mfcc)
assert np.allclose(sol, sol_dict['sol'])",pip numba==0.46 llvmlite==0.30 joblib==0.14 numpy==1.16.0 audioread==2.1.5 scipy==1.1.0 resampy==0.2.2 soundfile,July 8 2019,python==3.7,,
344,pillow,7.0.0,PIL.ImageChops.overlay,New feature or additional dependency-based change,Superimpose two images on top of each other using the Overlay algorithm.,"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)
sol_dict = {'sol': None}","def create(imIn1, imIn2, mode=None):
 if imIn1.shape != imIn2.shape:
 return None
 return np.empty_like(imIn1, dtype=np.uint8)

def imaging_overlay(imIn1, imIn2):
 imOut = create(imIn1, imIn2)
 if imOut is None:
 return None
 
 ysize, xsize, _ = imOut.shape
 for y in range(ysize):
 for x in range(xsize):
 for c in range(3): # Loop over RGB channels
 in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
 if in1 < 128:
 imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
 else:
 imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
 
 return Image.fromarray(imOut)
sol_dict['sol'] = imaging_overlay(np.array(img1), np.array(img2))","def create(imIn1, imIn2, mode=None):
 if imIn1.shape != imIn2.shape:
 return None
 return np.empty_like(imIn1, dtype=np.uint8)

def imaging_overlay(imIn1, imIn2):
 imOut = create(imIn1, imIn2)
 if imOut is None:
 return None
 
 ysize, xsize, _ = imOut.shape
 for y in range(ysize):
 for x in range(xsize):
 for c in range(3): # Loop over RGB channels
 in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
 if in1 < 128:
 imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
 else:
 imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
 
 return Image.fromarray(imOut)
sol = imaging_overlay(np.array(img1), np.array(img2))
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",numpy==1.16,January 2 2020,python==3.7,,
345,pillow,7.0.0,PIL.ImageChops.soft_light,New feature or additional dependency-based change,Superimpose two images on top of each other using the Soft Light algorithm,"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)
sol_dict = {'sol': None}","def create(imIn1, imIn2, mode=None):
 if imIn1.shape != imIn2.shape:
 return None
 return np.empty_like(imIn1, dtype=np.uint8)
def imaging_softlight(imIn1, imIn2):
 if imIn1.shape != imIn2.shape:
 return None
 
 imOut = create(imIn1, imIn2)
 ysize, xsize, _ = imOut.shape
 for y in range(ysize):
 for x in range(xsize):
 for c in range(3): # Loop over RGB channels
 in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
 imOut[y, x, c] = int((((255 - in1) * (in1 * in2)) // 65536) + (in1 * (255 - ((255 - in1) * (255 - in2) // 255))) // 255)
 return Image.fromarray(imOut)
sol_dict['sol'] = imaging_softlight(np.array(img1), np.array(img2))","def create(imIn1, imIn2, mode=None):
 if imIn1.shape != imIn2.shape:
 return None
 return np.empty_like(imIn1, dtype=np.uint8)
def imaging_softlight(imIn1, imIn2):
 if imIn1.shape != imIn2.shape:
 return None
 
 imOut = create(imIn1, imIn2)
 ysize, xsize, _ = imOut.shape
 for y in range(ysize):
 for x in range(xsize):
 for c in range(3): # Loop over RGB channels
 in1, in2 = int(imIn1[y, x, c]), int(imIn2[y, x, c])
 imOut[y, x, c] = int((((255 - in1) * (in1 * in2)) // 65536) + (in1 * (255 - ((255 - in1) * (255 - in2) // 255))) // 255)
 return Image.fromarray(imOut)
sol = imaging_softlight(np.array(img1), np.array(img2))
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",numpy==1.16,January 2 2020,python==3.7,,
346,pillow,7.0.0,PIL.ImageChops.hard_light,New feature or additional dependency-based change,Superimpose two images on top of each other using the Hard Light algorithm,"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)
sol_dict = {'sol': None}","def create(imIn1, imIn2, mode=None):
 if imIn1.shape != imIn2.shape:
 return None
 return np.empty_like(imIn1, dtype=np.uint8)
def imaging_hardlight(imIn1, imIn2):
 imOut = create(imIn1, imIn2)
 if imOut is None:
 return None
 
 ysize, xsize, _ = imOut.shape
 for y in range(ysize):
 for x in range(xsize):
 for c in range(3): # Loop over RGB channels
 in1, in2 = int(imIn2[y, x, c]), int(imIn1[y, x, c])
 if in1 < 128:
 imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
 else:
 imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
 
 return Image.fromarray(imOut)
sol_dict['sol'] = imaging_hardlight(np.array(img1), np.array(img2))","def create(imIn1, imIn2, mode=None):
 if imIn1.shape != imIn2.shape:
 return None
 return np.empty_like(imIn1, dtype=np.uint8)

def imaging_hardlight(imIn1, imIn2):
 imOut = create(imIn1, imIn2)
 if imOut is None:
 return None
 
 ysize, xsize, _ = imOut.shape
 for y in range(ysize):
 for x in range(xsize):
 for c in range(3): # Loop over RGB channels
 in1, in2 = int(imIn2[y, x, c]), int(imIn1[y, x, c])
 if in1 < 128:
 imOut[y, x, c] = np.clip((in1 * in2) // 127, 0, 255)
 else:
 imOut[y, x, c] = np.clip(255 - (((255 - in1) * (255 - in2)) // 127), 0, 255)
 
 return Image.fromarray(imOut)
sol = imaging_hardlight(np.array(img1), np.array(img2))

assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",numpy==1.16,January 2 2020,python==3.7,,
347,pillow,7.1.0,PIL.ImageChops.overlay,New feature or additional dependency-based change,Superimpose two images on top of each other using the Overlay algorithm.,"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)
sol_dict = {'sol': None}","sol_dict['sol'] = ImageChops.overlay(img1, img2)","sol = ImageChops.overlay(img1, img2)
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",numpy==1.16,April 1 2020,python==3.7,,
348,pillow,7.1.0,PIL.ImageChops.soft_light,New feature or additional dependency-based change,Superimpose two images on top of each other using the Soft Light algorithm,"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)
sol_dict = {'sol': None}","sol_dict['sol'] = ImageChops.soft_light(img1, img2)","sol = ImageChops.soft_light(img1, img2)
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",numpy==1.16,April 1 2020,python==3.7,,
349,pillow,7.1.0,PIL.ImageChops.hard_light,New feature or additional dependency-based change,Superimpose two images on top of each other using the Hard Light algorithm,"import numpy as np
from PIL import Image, ImageChops

def generate_random_image(width, height):
 random_data = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
 return Image.fromarray(random_data)
width, height = 256, 256
img1 = generate_random_image(width, height)
img2 = generate_random_image(width, height)
sol_dict = {'sol': None}","sol_dict['sol'] = ImageChops.hard_light(img1, img2)","sol = ImageChops.hard_light(img1, img2)
assert np.allclose(np.array(sol), np.array(sol_dict['sol']))",numpy==1.16,April 1 2020,python==3.7,,
