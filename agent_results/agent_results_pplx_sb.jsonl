{"example_id": "0", "output": "import torch\nimport math\nfrom torch.distributions import Normal\ndef log_ndtr(input_tensor: torch.Tensor) -> torch.Tensor:\n    std_normal = Normal(loc=0.0, scale=1.0)\n    z = (input_tensor - std_normal.loc) * std_normal.scale.reciprocal() / math.sqrt(2)\n    log_cdf = torch.log(0.5 * (1 + torch.erf(z)))\n    return log_cdf"}
{"example_id": "1", "output": "import torch\ndef gamma_ln(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.lgamma(input_tensor)"}
{"example_id": "2", "output": "import torch\ndef erf(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.erf(input_tensor)"}
{"example_id": "3", "output": "import torch\ndef erfc(input_tensor: torch.Tensor) -> torch.Tensor:\n    return 1 - torch.erf(input_tensor)"}
{"example_id": "4", "output": "import torch\ndef bessel_i0(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.i0(input_tensor)"}
{"example_id": "5", "output": "import torch\nfrom scipy.special import iv\n\nclass ModifiedBesselI1(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        if input.is_cuda:\n            # CPU fallback for SciPy compatibility\n            return torch.tensor(iv(1, input.cpu().detach().numpy()), \n                              device=input.device)\n        return torch.from_numpy(iv(1, input.detach().numpy())).to(input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        # Derivative formula: I\u2081' = (I\u2080 + I\u2082)/2 \u2248 (I\u2080 + I\u2081)/2 using approximation\n        # For strict correctness, we'd need I\u2080 and I\u2082 implementations here\n        grad_input = grad_output * 0.5 * (iv(0, input.numpy()) + iv(2, input.numpy()))\n        return torch.tensor(grad_input, dtype=grad_output.dtype)\n\ndef bessel_i1(input_tensor: torch.Tensor) -> torch.Tensor:\n    return ModifiedBesselI1.apply(input_tensor)"}
{"example_id": "6", "output": "import torch\ndef gamma_ln(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.gammaln(input_tensor)"}
{"example_id": "7", "output": "import torch\ndef erf(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.erf(input_tensor)"}
{"example_id": "8", "output": "import torch\ndef erfc(input_tensor: torch.Tensor) -> torch.Tensor:\n    return 1 - torch.erf(input_tensor)\nx = torch.tensor([1.0, 2.0, 3.0])\nprint(erfc(x))"}
{"example_id": "9", "output": "import torch\ndef bessel_i0(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.i0(input_tensor)"}
{"example_id": "10", "output": "import torch\ndef bessel_i1(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.special.i1(input_tensor)"}
{"example_id": "11", "output": "import torch\ndef invert_mask(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.BoolTensor:\n    mask = ~torch.lt(tensor1, tensor2)\n    return mask"}
{"example_id": "12", "output": "import torch\nimport math\n\ndef log_ndtr(input_tensor: torch.Tensor) -> torch.Tensor:\n    return torch.log(0.5 * (1.0 + torch.erf(input_tensor / math.sqrt(2.0))))"}
{"example_id": "13", "output": "import torch\ndef invert_mask(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.BoolTensor:\n    mask = tensor1 < tensor2\n    return ~mask"}
{"example_id": "14", "output": "import torch\ndef stft(audio_signal: torch.Tensor, n_fft: int) -> torch.Tensor:\n    win_length = n_fft\n    hop_length = n_fft // 2\n    window = torch.hann_window(win_length)\n    complex_stft = torch.stft(input=audio_signal, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, return_complex=False)\n    real_part = complex_stft.real\n    imag_part = complex_stft.imag\n    real_tensor = torch.cat((real_part.unsqueeze(-1), imag_part.unsqueeze(-1)), dim=-1)\n    return real_tensor"}
{"example_id": "15", "output": "import torch\ndef stft(audio_signal: torch.Tensor, n_fft: int) -> torch.Tensor:\n    \"\"\"Compute STFT magnitudes from a 1D audio tensor.\n    \n    Args:\n        audio_signal: Input tensor of shape (L,)\n        n_fft: FFT window size\n    \n    Returns:\n        Tensor of shape (2, freq_bins, time_frames) containing real/imaginary components\n    \"\"\"\n    # Ensure audio is 1D\n    if audio_signal.dim() != 1:\n        raise ValueError(\"Input must be 1D\")\n    \n    # Compute STFT (returns complex tensor [freq_bins, time_frames])\n    stft_complex = torch.stft(\n        audio_signal,\n        n_fft=n_fft,\n        win_length=n_fft,\n        window=torch.hann_window(n_fft),\n        center=True,\n        return_complex=True\n    )\n    \n    # Split complex into real/imaginary and stack\n    return torch.stack((stft_complex.real, stft_complex.imag), dim=0)"}
{"example_id": "16", "output": "import torch\ndef istft(spectrogram: torch.Tensor, n_fft: int, hop_length: int, win_length: int, normalized=False) -> torch.Tensor:\n    # Split into real and imaginary parts\n    real_part = spectrogram[:, :, :spectrogram.shape[2]//2]\n    imag_part = spectrogram[:, :, spectrogram.shape[2]//2:]\n\n    # Form a complex tensor\n    complex_spectrogram = torch.complex(real=real_part, imag=imag_part)\n\n    # Transpose to fit the expected format of torch.istft\n    complex_spectrogram = complex_spectrogram.transpose(1, 2)  # Now (freqs, frames)\n\n    # Assuming window is correctly defined from previous STFT computation\n    window = torch.hann_window(win_length)\n    time_domain_signal = torch.istft(\n        complex_spectrogram,\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=window, \n        center=True, \n        normalized=normalized\n    )\n    return time_domain_signal"}
{"example_id": "17", "output": "import torch\ndef istft(spectrogram: torch.Tensor, signal: torch.Tensor, n_fft: int, hop_length: int, win_length: int, normalized=False) -> torch.Tensor:\n    \"\"\"Compute the Inverse Short-Time Fourier Transform (ISTFT) of a spectrogram.\n\n    Args:\n        spectrogram (torch.Tensor): The input spectrogram (65, 33, 2).\n        n_fft (int): The size of the FFT.\n        hop_length (int): The hop length.\n        win_length (int): The window length.\n        normalized (bool, optional): Whether the spectrogram is normalized. Defaults to False.\n\n    Returns:\n        torch.Tensor: The ISTFT of the spectrogram.\n    \"\"\"\n    spectrogram_complex = torch.view_as_complex(spectrogram)\n    window = torch.hann_window(win_length)\n    signal = torch.istft(\n        spectrogram_complex,\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=window,\n        normalized=normalized\n    )\n    return signal\n\n# Example usage\nspectrogram = torch.randn(65, 33, 2)\nsignal = torch.zeros(512)  # Provide an initial signal\nn_fft = 128\nhop_length = 32\nwin_length = 64\nresult = istft(spectrogram, signal, n_fft, hop_length, win_length)\nprint(result.shape)"}
{"example_id": "18", "output": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_join(gdf1 : gpd.GeoDataFrame, gdf2 : gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    return gpd.sjoin(gdf1, gdf2, how='inner', predicate='intersects')"}
{"example_id": "19", "output": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_join(gdf1 : gpd.GeoDataFrame, gdf2 : gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    return gpd.sjoin(gdf1, gdf2, how=\"inner\", op=\"intersects\")"}
{"example_id": "20", "output": "import geopandas as gpd\nfrom shapely.geometry import box\n\ndef perform_union(gdf : gpd.GeoDataFrame) -> gpd.GeoSeries:\n    return gdf.geometry.unary_union"}
{"example_id": "21", "output": "import geopandas as gpd\nfrom shapely.geometry import box\n\ndef perform_union(gdf: gpd.GeoDataFrame) -> gpd.GeoSeries:\n    return gdf.geometry.unary_union"}
{"example_id": "22", "output": "import geopandas as gpd\nfrom shapely.geometry import Point\ndef create_geoseries(x: list[int], y: list[int]) -> gpd.GeoSeries:\n    points = [Point(x_coord, y_coord) for x_coord, y_coord in zip(x, y)]\n    geoseries = gpd.GeoSeries(points)\n    return geoseries"}
{"example_id": "23", "output": "import geopandas as gpd\nfrom shapely.geometry import Point\ndef create_geoseries(x:list[int], y:list[int]) -> gpd.GeoSeries:\n    points = [Point(xi, yi) for xi, yi in zip(x, y)]\n    gs = gpd.GeoSeries(points, crs=\"EPSG:4326\")\n    return gs"}
{"example_id": "24", "output": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon, box\n\ndef spatial_query(gdf:gpd.GeoDataFrame, other:gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n    combined_geometry = other.unary_union\n    sindex = gdf.sindex\n    possible_matches_index = list(sindex.intersection(combined_geometry.bounds))\n    precise_matches = gdf.iloc[possible_matches_index][gdf.intersects(combined_geometry)]\n    return precise_matches"}
{"example_id": "25", "output": "import geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\ndef spatial_query(gdf:gpd.GeoDataFrame, other:gpd.GeoSeries) -> gpd.GeoDataFrame:\n    return gpd.sjoin(gdf, other, how='inner', op='within')"}
{"example_id": "26", "output": "import nltk\nimport io\nimport contextlib\n\ndef show_usage(obj:object) -> str:\n    with io.StringIO() as buf, contextlib.redirect_stdout(buf):\n        help(obj)\n    return buf.getvalue()"}
{"example_id": "27", "output": "import networkx as nx\ndef modularity_communities(G:nx.Graph) -> list:\n    return list(nx.community.greedy_modularity_communities(G))"}
{"example_id": "28", "output": "import networkx as nx\ndef modularity_communities(G:nx.Graph) -> list:\n    return list(nx.community.greedy_modularity_communities(G))"}
{"example_id": "29", "output": "import networkx as nx\ndef bounding_distance(G:nx.Graph) -> int:\n    return nx.diameter(G)"}
{"example_id": "30", "output": "import networkx as nx\ndef bounding_distance(G:nx.Graph) -> int:\n    return nx.algorithms.distance_measures.diameter(G)"}
{"example_id": "31", "output": "import networkx as nx\nfrom networkx.algorithms.community.modularity_max import _naive_greedy_modularity_communities\ndef naive_modularity_communities(G:nx.Graph) -> list:\n    return list(_naive_greedy_modularity_communities(G))"}
{"example_id": "32", "output": "import networkx as nx\nfrom networkx.algorithms.community.modularity_max import _naive_greedy_modularity_communities\ndef naive_modularity_communities(G:nx.Graph) -> list:\n    return list(_naive_greedy_modularity_communities(G))"}
{"example_id": "33", "output": "import networkx as nx\ndef get_nodes(G:nx.Graph) -> list:\n    return list(G)"}
{"example_id": "34", "output": "import networkx as nx\ndef get_first_edge(G:nx.Graph) -> tuple :\n    edges = list(G.edges())\n    if edges:\n        return edges[0]\n    return None"}
{"example_id": "35", "output": "import networkx as nx\ndef shortest_path(G:nx.Graph, source:int) -> list:\n    return nx.single_source_bellman_ford(G, source)"}
{"example_id": "36", "output": "import gradio as gr\n\ndef render_quadratic_formula():\n    formula = \"$$x = \\frac{{-b \\pm \\sqrt{{b^2 - 4ac}}}}{{2a}}$$\"\n    return formula\n\ninterface = gr.Interface(fn=render_quadratic_formula, inputs=[], outputs = \"text\")\n\ninterface.launch()"}
{"example_id": "37", "output": "import gradio as gr\n\ndef render_quadratic_formula():\n    formula = \"$$x = \\frac{{-b \\pm \\sqrt{{b^2 - 4ac}}}}{2a}$$\"\n    return formula\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def respond(message, history):\n        history = history or []\n        if message:\n            history.append([message, render_quadratic_formula()])\n        return history\n    \n    msg.submit(respond, [msg, chatbot], chatbot, queue=False).then()\n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.launch()"}
{"example_id": "38", "output": "import gradio as gr\n\ndef display_image():\n    return \"https://gradio.app/static/img/logo.svg\"\n\niface = gr.Interface(fn=display_image, outputs=gr.Image(interactive=False),)\niface.launch()"}
{"example_id": "39", "output": "import gradio as gr\n\ndef display_image(img_path):\n    return img_path\n\ndemo = gr.Interface(\n    fn=display_image,\n    inputs=gr.Image(type=\"filepath\"),\n    outputs=gr.Image()\n)\ndemo.launch()"}
{"example_id": "40", "output": "import gradio as gr\n\ndef process_image(image):\n    # Assume 'image' is a filepath string here\n    text_output = f\"Processed image saved at: {image}\"\n    return text_output\n\ndemo = gr.Interface(\n    fn=process_image,\n    inputs=gr.Image(type=\"filepath\"),  # Returns filepath instead of numpy array\n    outputs=\"text\"\n)\n\ndemo.launch()"}
{"example_id": "41", "output": "import gradio as gr\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Model\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\nmodel.eval()\n\n# Preprocessing\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Prediction function\ndef predict(img):\n    img = Image.fromarray(img.astype('uint8'), 'RGB')\n    inputs = preprocess(img).unsqueeze(0)\n    with torch.no_grad():\n        outputs = model(inputs)\n    probs = torch.nn.functional.softmax(outputs[0], dim=0)\n    return {str(i): float(probs[i]) for i in range(1000)}\n\n# Interface\ngraiface = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(label=\"Input Image\"),\n    outputs=gr.Label(num_top_classes=3),\n    title=\"Image Classification with ResNet18\"\n)"}
{"example_id": "42", "output": "import gradio as gr\n\ndef get_selected_options(options):\n    return f\"Selected options: {options}\"\n\nselection_options = [\"angola\", \"pakistan\", \"canada\"]\n\niface = gr.Interface(get_selected_options, inputs = gr.Dropdown(choices=selection_options, multiselect=True), outputs=\"text\")\n\niface.launch()"}
{"example_id": "43", "output": "from sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\ndef get_n_features(clf: GradientBoostingClassifier) -> int:\n    X, y = make_classification(n_samples=1000, n_features=5, n_informative=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    clf.fit(X_train, y_train)\n    return clf.n_features_in_\n\nclf = GradientBoostingClassifier()\nprint(get_n_features(clf))"}
{"example_id": "44", "output": "from sklearn.ensemble import GradientBoostingClassifier\n# Initialize the classifier\ndef init_clf() -> GradientBoostingClassifier:\n    classifier = GradientBoostingClassifier(loss=\"log_loss\")\n    return classifier"}
{"example_id": "45", "output": "from sklearn.cross_decomposition import CCA\nimport numpy as np\n\ndef get_coef_shape(cca_model: CCA, X: np.ndarray, Y: np.ndarray) -> tuple:\n    cca_model.fit(X, Y)\n    return cca_model.x_rotations_.shape"}
{"example_id": "46", "output": "from sklearn.cross_decomposition import CCA\nimport numpy as np\ndef get_coef_shape(cca_model: CCA, X: np.ndarray, Y: np.ndarray) -> tuple:\n    cca_model.fit(X, Y)\n    return cca_model.coef_.shape"}
{"example_id": "47", "output": "from sklearn.datasets import make_sparse_coded_signal\nfrom sklearn.decomposition import SparseCoder\nimport numpy as np\ndef get_signal(n_samples: int, n_features: int, n_components: int, n_nonzero_coefs: int) -> tuple:\n    # Generate random dictionary\n    dictionary = np.random.rand(n_components, n_features)\n    # Generate signal\n    y, X, w = make_sparse_coded_signal(\n        n_samples=n_samples,\n        n_components=n_components,\n        n_features=n_features,\n        n_nonzero_coefs=n_nonzero_coefs,\n        random_state=0,\n    )\n    # Transpose y\n    y = y.T\n    coder = SparseCoder(dictionary=dictionary.T, transform_algorithm='lasso_lars', transform_alpha=0.1)\n    sparse_code = coder.transform(y.T)\n    return sparse_code, dictionary"}
{"example_id": "48", "output": "from sklearn.datasets import load_digits\nfrom sklearn.utils import Bunch\nfrom sklearn.decomposition import FastICA\n\ndef apply_fast_ica(data: Bunch, n_components: int) -> FastICA:\n    ica = FastICA(n_components=n_components, whiten='arbitrary-variance')\n    transformed_data = ica.fit_transform(data.data)\n    return transformed_data\n\ndigits = load_digits()\nX = digits.data\n\ntransformed_data = apply_fast_ica(digits, n_components=64) # Example usage\nprint(transformed_data)"}
{"example_id": "49", "output": "from sklearn.datasets import load_digits\nfrom sklearn.decomposition import FastICA\nfrom sklearn.utils import Bunch\n\ndef apply_fast_ica(data: Bunch, n_components: int) -> FastICA:\n    ica = FastICA(n_components=n_components, whiten='unit-variance', random_state=0)\n    transformed_data = ica.fit_transform(data.data)\n    return transformed_data\n\ndigits = load_digits()\nX = digits.data\n\ntransformed_data = apply_fast_ica(digits, n_components=64)"}
{"example_id": "50", "output": "from sklearn.impute import SimpleImputer\nimport numpy as np\ndef get_imputer(data: np.ndarray) -> SimpleImputer:\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    imp.fit(data)\n    return imp"}
{"example_id": "51", "output": "from sklearn import metrics\ndef get_scorer_names() -> list:\n    return metrics.get_scorer_names()"}
{"example_id": "52", "output": "from sklearn import metrics\ndef get_scorer_names() -> list:\n    return metrics.get_scorer_names()"}
{"example_id": "53", "output": "from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\ndef get_pairwise_dist(X: np.ndarray,Y: np.ndarray) -> np.ndarray:\n    distances = manhattan_distances(X, Y, sum_over_features=False)\n    return distances"}
{"example_id": "54", "output": "from sklearn.metrics.pairwise import manhattan_distances\nimport numpy as np\ndef get_pairwise_dist(X: np.ndarray,Y: np.ndarray) -> np.ndarray:\n    return manhattan_distances(X, Y)"}
{"example_id": "55", "output": "from matplotlib.colors import *\nimport numpy as np\ncmap = {\n    \"blue\": [[1, 2, 2], [2, 2, 1]],\n    \"red\": [[0, 0, 0], [1, 0, 0]],\n    \"green\": [[0, 0, 0], [1, 0, 0]]\n}\n\ncmap_reversed = {}\nfor color, mapping in cmap.items():\n    cmap_reversed[color] = np.flip(mapping, axis=0).tolist()\nprint(cmap_reversed)"}
{"example_id": "56", "output": "import pandas as pd\n\ndef get_grouped_df(df: pd.DataFrame) -> pd.DataFrame:\n    all_categories = df['cat'].unique().tolist()\n    df['cat'] = pd.Categorical(df['cat'], categories=all_categories)\n    return df.groupby('cat')['value'].sum()"}
{"example_id": "57", "output": "import pandas as pd\ndef get_grouped_df(df: pd.DataFrame) -> pd.DataFrame:\n    return df.groupby(by=list(df.columns), dropna=False, observed=False).sum()"}
{"example_id": "58", "output": "import pandas as pd\nimport numpy as np\ndef get_expected_value(df: pd.DataFrame) -> pd.Series:\n    df_copy = df.copy()\n    view = df_copy.iloc[:, 0]\n    view[:] = np.array([98, 99])\n    return view"}
{"example_id": "59", "output": "import pandas as pd\nimport numpy as np\ndef get_expected_value(df: pd.DataFrame) -> pd.Series:\n    df_copy = df.copy()\n    df_copy.iloc[0] = [5, 6]\n    return df_copy.iloc[0]"}
{"example_id": "60", "output": "import pandas as pd\nimport numpy as np\ndef get_slice(ser: pd.Series, start: int, end: int) -> pd.Series:\n    return ser.iloc[start:end]"}
{"example_id": "61", "output": "import pandas as pd\nimport numpy as np\ndef get_slice(ser: pd.Series, start: int, end: int) -> pd.Series:\n    return ser.iloc[start:end]"}
{"example_id": "62", "output": "import pandas as pd\ndef correct_type(index: pd.Index) -> str:\n    return str(index.dtype)"}
{"example_id": "63", "output": "import pandas as pd\ndef combined(df1: pd.DataFrame, df2: pd.DataFrame, series1: pd.Series, series2: pd.Series) -> tuple:\n    combined_df = pd.concat([df1, df2])\n    combined_series = pd.concat([series1, series2])\n    return (combined_df, combined_series)"}
{"example_id": "64", "output": "import pandas as pd\ndef correct_type(index: pd.Index) -> str:\n    return index.dtype"}
{"example_id": "65", "output": "import pandas as pd\ndef combined(df1: pd.DataFrame, df2: pd.DataFrame, series1: pd.Series, series2: pd.Series) -> tuple:\n    combined_df = pd.concat([df1, df2], ignore_index=True)\n    combined_series = pd.concat([series1, series2], ignore_index=True)\n    return combined_df, combined_series"}
{"example_id": "66", "output": "import numpy as np\n\ndef apply_convolution_full(arr1 : np.ndarray, arr2 : np.ndarray) -> np.ndarray:\n    return np.convolve(arr1, arr2, mode='full')"}
{"example_id": "67", "output": "import numpy as np\n\ndef apply_convolution_valid(arr1 : np.ndarray , arr2 : np.ndarray) -> np.ndarray:\n    return np.convolve(arr1, arr2, mode='valid')"}
{"example_id": "68", "output": "import numpy as np\n\ndef apply_correlate_full(arr1 : np.ndarray, arr2 : np.ndarray) -> np.ndarray:\n    return np.correlate(arr1, arr2, mode='full')"}
{"example_id": "69", "output": "import numpy as np\n\ndef find_common_type(arr1:np.ndarray, arr2:np.ndarray) -> np.dtype:\n    if arr1.dtype == arr2.dtype:\n        return arr1.dtype\n    else:\n        return None"}
{"example_id": "70", "output": "import numpy as np\n\ndef find_common_type(arr1:np.ndarray, arr2:np.ndarray) -> np.dtype:\n    return np.promote_types(arr1.dtype, arr2.dtype)"}
{"example_id": "71", "output": "import numpy as np\n\ndef custom_round(arr:np.ndarray) -> np.ndarray:\n    return np.round(arr)"}
{"example_id": "72", "output": "import numpy as np\n\ndef custom_product(arr:np.ndarray) -> np.ndarray:\n    return np.prod(arr)"}
{"example_id": "73", "output": "import numpy as np\n\ndef custom_cumproduct(arr:np.ndarray) -> np.ndarray:\n    return np.cumprod(arr)"}
{"example_id": "74", "output": "import numpy as np\n\ndef custom_sometrue(arr:np.ndarray) -> np.ndarray:\n    return np.any(arr)"}
{"example_id": "75", "output": "import numpy as np\n\ndef custom_alltrue(arr:np.ndarray) -> np.ndarray:\n    return np.all(arr)"}
{"example_id": "76", "output": "import numpy as np\n\ndef custom_round(arr: np.ndarray) -> np.ndarray:\n    return np.round(arr)"}
{"example_id": "77", "output": "import numpy as np\n\ndef custom_product(arr:np.ndarray) -> np.ndarray:\n    return np.prod(arr)"}
{"example_id": "78", "output": "import numpy as np\n\ndef custom_cumproduct(arr:np.ndarray) -> np.ndarray:\n    return arr.cumprod()"}
{"example_id": "79", "output": "import numpy as np\n\ndef custom_anytrue(arr:np.ndarray) -> np.ndarray:\n    return np.any(arr)"}
{"example_id": "80", "output": "import numpy as np\n\ndef custom_alltrue(arr:np.ndarray) -> np.ndarray:\n    return np.all(arr)"}
{"example_id": "81", "output": "import numpy as np\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\ndef predict_start(model: LGBMClassifier, start_iter: int) -> np.ndarray:\n    num_iter = model.booster_.current_iteration()\n    if start_iter < 0 or start_iter > num_iter:\n        raise ValueError(\"start_iter must be between 0 and the number of iterations.\")\n    return model.predict(model.booster_.data, start_iteration=start_iter)"}
{"example_id": "82", "output": "import numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import make_classification\n\nNUM_SAMPLES = 500\nNUM_FEATURES = 20\nINFORMATIVE_FEATURES = 2\nREDUNDANT_FEATURES = 10\nRANDOM_STATE = 42\nNUM_BOOST_ROUND = 100\nNFOLD = 5\nLEARNING_RATE = 0.05\nEARLY_STOPPING_ROUNDS = 10\nX, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)\ntrain_data = lgb.Dataset(X, label=y)\n\nparams = {{\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': LEARNING_RATE,\n    'verbose': -1\n}}\n\ncv_results = lgb.cv(\n    params=params,\n    train_set=train_data,\n    num_boost_round=NUM_BOOST_ROUND,\n    nfold=NFOLD,\n    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n    return_cvbooster=True\n)\n\nprint(cv_results)"}
{"example_id": "83", "output": "import lightgbm.compat as compat\ndef decode_string(string: bytes) -> str:\n    return string.decode()"}
{"example_id": "84", "output": "import numpy as np\nimport lightgbm as lgb\nfrom sklearn.datasets import make_classification\n\nNUM_SAMPLES = 500\nNUM_FEATURES = 20\nINFORMATIVE_FEATURES = 2\nREDUNDANT_FEATURES = 10\nRANDOM_STATE = 42\nNUM_BOOST_ROUND = 100\nNFOLD = 5\nLEARNING_RATE = 0.05\nEARLY_STOPPING_ROUNDS = 10\nX, y = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_informative=INFORMATIVE_FEATURES, n_redundant=REDUNDANT_FEATURES, random_state=RANDOM_STATE)\ntrain_data = lgb.Dataset(X, label=y)\n\nparams = {{\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': LEARNING_RATE,\n    'verbose': -1\n}}\n\ncv_results = lgb.cv(\n    params=params,\n    train_set=train_data,\n    num_boost_round=NUM_BOOST_ROUND,\n    nfold=NFOLD,\n    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n    verbose_eval=True  # display evaluation metric\n)"}
{"example_id": "85", "output": "import lightgbm as lgb\nimport numpy as np\nimport ctypes\n\ndef convert_cint32_array_to_numpy(c_pointer: ctypes.POINTER, length: int) -> np.ndarray:\n    \"\"\"\n    Convert a ctypes pointer to a numpy array.\n    \n    Args:\n        c_pointer (c_array_type): A ctypes pointer to an array of integers.\n        length (int): The length of the array.\n        \n    Returns:\n        np.ndarray: A numpy array containing the elements of the ctypes array.\n    \"\"\"\n    # Convert and copy to ensure memory safety\n    return np.ctypeslib.as_array(c_pointer, shape=(length,)).copy()"}
{"example_id": "86", "output": "import lightgbm as lgb\nimport numpy as np\n\ndef get_params(dataset: lgb.Dataset) -> dict:\n    \"\"\"\n    Get the parameters of the dataset.\n    \n    Args:\n        dataset (lgb.Dataset): The dataset to get the parameters from.\n        \n    Returns:\n        dict: The parameters of the dataset.\n    \"\"\"\n    params = vars(dataset)\n    return params"}
{"example_id": "87", "output": "import numpy as np\nimport json\nfrom lightgbm.compat import json_default_with_numpy\n\ndef dump_json(data: any) -> str:\n    \"\"\"\n    Dump data to JSON format.\n    \n    Args:\n        data (any): The data to dump.\n        \n    Returns:\n        str: The JSON representation of the data.\n    \"\"\"\n    return json.dumps(data, default=json_default_with_numpy)"}
{"example_id": "88", "output": "import ctypes\nimport lightgbm.basic as basic\n\ndef create_c_array(values: list, ctype: type) -> ctypes.Array:\n    \"\"\"\n    Create a ctypes array from a list of values.\n    Args:\n        values (list): A list of values to be converted to a ctypes array.\n        ctype (type): The ctypes type of the array elements.\n    Returns:\n        ctypes.Array: A ctypes array containing the values.\n    \"\"\"\n    return (ctype * len(values))(*values)"}
{"example_id": "89", "output": "import lightgbm as lgb\nimport ctypes\n\ndef c_str(python_string: str) -> ctypes.c_char_p:\n    \"\"\"\n    Convert a Python string to a ctypes c_char_p.\n    \n    Args:\n        python_string (str): The Python string to convert.\n        \n    Returns:\n        ctypes.c_char_p: The converted ctypes c_char_p.\n    \"\"\"\n    return ctypes.c_char_p(python_string.encode('utf-8'))"}
{"example_id": "90", "output": "import lightgbm as lgb\nimport numpy as np\n\ndef convert_from_sliced_object(sliced_data: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert a sliced object to a fixed object.\n    \n    Args:\n        sliced_data (np.ndarray): The sliced object to convert.\n        \n    Returns:\n        np.ndarray: The converted fixed object.\n    \"\"\"\n    return sliced_data.copy()"}
{"example_id": "91", "output": "import spacy\nfrom spacy.pipeline.span_ruler import SpanRuler\n\nnlp = spacy.blank(\"en\")\nruler = nlp.add_pipe(\"span_ruler\")\nruler.add_patterns([{\"label\": \"PERSON\", \"pattern\": \"John Doe\"}, {\"label\": \"ORG\", \"pattern\": \"Google\"}, {\"label\": \"PERSON\", \"pattern\": \"Jane Doe\"}])\n\ndef get_labels(ruler: SpanRuler) -> tuple:\n    \"\"\"\n    Get the labels of the SpanRuler.\n    \n    Args:\n        ruler (SpanRuler): The SpanRuler to get the labels from.\n        \n    Returns:\n        tuple: The labels of the SpanRuler.\n    \"\"\"\n    labels = set()\n    for pattern in ruler.patterns:\n        labels.add(pattern[\"label\"])\n    return tuple(labels)\n\nprint(get_labels(ruler))"}
{"example_id": "92", "output": "import spacy\nfrom spacy.training import Example\nfrom spacy.training import augment\n\ndef create_whitespace_variant(nlp: spacy.Language, example: Example, whitespace: str, position: int) -> Example:\n    \"\"\"\n    Create a whitespace variant of the given example.\n    \n    Args:\n        nlp (Language): The spaCy language model.\n        example (Example): The example to augment.\n        whitespace (str): The whitespace to insert.\n        position (int): The position to insert the whitespace.\n        \n    Returns:\n        Example: The augmented example.\n    \"\"\"\n    doc = example.reference\n    words = [t.text for t in doc]\n    spaces = [bool(t.whitespace_) for t in doc]\n    if position >= 0 and position < len(words):\n        words[position] += whitespace\n    elif position == -1:\n         words[-1] += whitespace   \n    else:\n        raise ValueError(\"Position must be within the range of words or -1 for the last word.\")\n    \n    doc_whitespace_variant = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n\n    return Example(doc_whitespace_variant, example.y)"}
{"example_id": "93", "output": "import spacy\nfrom spacy.pipeline.span_ruler import SpanRuler\n\n\ndef remove_pattern_by_id(ruler: SpanRuler, pattern_id: str) -> None:\n    \"\"\"\n    Remove a pattern from the SpanRuler by its ID.\n    \n    Args:\n        ruler (SpanRuler): The SpanRuler to remove the pattern from.\n        pattern_id (str): The ID of the pattern to remove.\n        \n    Returns:\n        None\n    \"\"\"\n    ruler.remove_by_id(pattern_id)"}
{"example_id": "94", "output": "import nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import wordnet\n\ndef align_words_func(hypothesis, reference):\n    \"\"\"\n    Align words between hypothesis and reference sentences.\n    \n    Args:\n        hypothesis (list): List of words in the hypothesis sentence.\n        reference (list): List of words in the reference sentence.\n        \n    Returns:\n        tuple: A tuple containing the aligned matches, unmatched hypothesis, and unmatched reference.\n    \"\"\"\n    matched_pairs, unmatched_hyp, unmatched_ref = align_words(\n        hypothesis=hypothesis,\n        reference=reference,\n        stemmer=PorterStemmer(),\n        wordnet=wordnet\n    )\n    return matched_pairs, unmatched_hyp, unmatched_ref"}
{"example_id": "95", "output": "import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.corpus import wordnet\n\ndef get_synset_examples(synset: str) -> list:\n    \"\"\"\n    Get examples for a given synset.\n    \n    Args:\n        synset (str): The synset to get examples for.\n        \n    Returns:\n        list: A list of examples for the synset.\n    \"\"\"\n    return wordnet.synset(synset).examples()"}
{"example_id": "96", "output": "import nltk\nnltk.download('sinica_treebank')\nfrom nltk.tree import Tree\nfrom nltk.corpus import sinica_treebank\n\ndef parse_sinica_treebank_sentence(sentence: str) -> Tree:\n    \"\"\"\n    Parse a sentence from the Sinica Treebank.\n    \n    Args:\n        sentence (str): The sentence to parse.\n        \n    Returns:\n        Tree: The parsed tree.\n    \"\"\"\n    return Tree.fromstring(sentence)"}
{"example_id": "97", "output": "from nltk.lm.api import accumulate\nimport operator\n\ndef accumulate_functional(iterable, func):\n    \"\"\"\n    Accumulate the results of applying a function to an iterable.\n    \n    Args:\n        iterable (iterable): An iterable to accumulate.\n        func (function): A function to apply to the elements of the iterable.\n        \n    Returns:\n        list: A list of accumulated results.\n    \"\"\"\n    return list(accumulate(iterable, func))"}
{"example_id": "98", "output": "import nltk.tokenize.destructive\n\n\ndef tokenize_sentence(sentence: str) -> list:\n    \"\"\"\n    Tokenize a sentence into words.\n    \n    Args:\n        sentence (str): The sentence to tokenize.\n        \n    Returns:\n        list: A list of tokens.\n    \"\"\"\n    return nltk.tokenize.destructive.word_tokenize(sentence)"}
{"example_id": "99", "output": "import django\nfrom django.conf import settings\nfrom django.utils import timezone\nimport datetime as dt\n\nsettings.configure()\n\ndef get_time_in_utc(year: int, month: int, day: int) -> timezone.datetime:\n    naive_datetime = dt.datetime(year, month, day)\n    utc_aware_datetime = naive_datetime.replace(tzinfo=dt.timezone.utc)\n    return utc_aware_datetime"}
{"example_id": "100", "output": "import django\nfrom django.conf import settings\nfrom django.utils import timezone\nfrom datetime import datetime\n\nsettings.configure()\n\ndef get_time_in_utc(year: int, month: int, day: int) -> timezone.datetime:\n    naive_dt = datetime(year, month, day)\n    aware_dt = timezone.make_aware(naive_dt, timezone.get_current_timezone())\n    utc_dt = aware_dt.astimezone(timezone.utc)\n    return utc_dt"}
{"example_id": "101", "output": "from django.conf import settings\nfrom django.forms.models import BaseModelFormSet\nfrom django.forms import Form\n\nsettings.configure()\ndef save_existing(formset: BaseModelFormSet, form : Form, obj:str) -> None:\n    instances = formset.save(commit=False)\n    for instance in instances:\n        instance.save()"}
{"example_id": "102", "output": "from django.conf import settings\nfrom django.forms.models import BaseModelFormSet\nfrom django.forms import Form\nfrom django.db import models\n\nsettings.configure()\n\nclass Author(models.Model):\n    name = models.CharField(max_length=100)\n    title = models.CharField(max_length=100)\n\ndef save_existing(formset: BaseModelFormSet, form: Form, instance: Author) -> None:\n    if formset.is_valid():\n        instances = formset.save(commit=False)\n        for inst in instances:\n            inst.save()\n        for obj in formset.deleted_objects:\n            obj.delete()\n        formset.save_m2m()"}
{"example_id": "103", "output": "import django\nfrom django.conf import settings\nfrom django import forms\nfrom django.template import Template, Context\n\nsettings.configure(\n      TEMPLATES=[\n          {{\n              'BACKEND': 'django.template.backends.django.DjangoTemplates',\n          }},\n      ],\n  )\ndjango.setup()\n\ndef render_output(template_string):\n  form = SampleForm()\n  template = Template(template_string)\n  context = Context({'form': form})\n  rendered_output = template.render(context)\n  return rendered_output\n\n# target for html string\n# <form>\n#   <div>\n#     <label for='id_name'>Name:</label>\n\n# <div class='helptext' id='id_name_helptext'>Enter your name</div>\n\n# <input type='text' name='name' required aria-describedby='id_name_helptext' id='id_name'>\n#   </div>\n# </form>\n\nclass SampleForm(forms.Form):\n    name = forms.CharField(label='Name', help_text='Enter your name')\ndef get_template_string()->str:\n  return '''<form>\n  <div>\n    {{ form.name.as_field_group }}\n  </div>\n</form>'''"}
{"example_id": "104", "output": "import django\nfrom django.conf import settings\nfrom django import forms\nfrom django.template import Template, Context\n\nsettings.configure(\n      TEMPLATES=[\n          {\n              'BACKEND': 'django.template.backends.django.DjangoTemplates',\n          },\n      ],\n  )\ndjango.setup()\n\ndef render_output(template_string):\n  form = SampleForm()\n  template = Template(template_string)\n  context = Context({'form': form})\n  rendered_output = template.render(context)\n  return rendered_output\n\n# target for html string\n# <form>\n#   <div>\n#     <label for='id_name'>Name:</label>\n\n# <div class='helptext' id='id_name_helptext'>Enter your name</div>\n\n# <input type='text' name='name' required aria-describedby='id_name_helptext' id='id_name'>\n#   </div>\n# </form>\n\nclass SampleForm(forms.Form):\n    name = forms.CharField(label='Name', help_text='Enter your name')\ndef get_template_string()->str:\n    return '''<form>\n  <div>\n    {{ form.name.label_tag }}\n\n    <div class='helptext' id='{{ form.name.id_for_label }}_helptext'>{{ form.name.help_text }}</div>\n\n    {{ form.name }}\n  </div>\n</form>'''"}
{"example_id": "105", "output": "import django\nfrom django.conf import settings\nfrom django.db import models, connection\nfrom django.db.models import F\n\nsettings.configure(\n    DATABASES={{'default': {{'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}}}},\n)\ndjango.setup()\n\n\ndef display_side_and_area(square):\n    return square.side, square.area\n\ndef create_square(side):\n    square = Square.objects.create(side=side)\n    square.refresh_from_db()\n    return square\n\nclass Square(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    side = models.IntegerField()\n    area = models.IntegerField(editable=False)\n\n    def save(self, *args, **kwargs):\n        self.area = self.side**2\n        super().save(*args, **kwargs)"}
{"example_id": "106", "output": "import django\nfrom django.conf import settings\nfrom django.db import models, connection\nfrom django.db.models import F\n\nsettings.configure(\n    DATABASES={{'default': {{'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}}}},\n)\ndjango.setup()\n\n\ndef display_side_and_area(square):\n    return square.side, square.area\n\ndef create_square(side):\n    square = Square.objects.create(side=side)\n    square.refresh_from_db()\n    return square\n\nclass Square(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    side = models.IntegerField()\n    area = models.IntegerField(null=True, blank=True)\n\n    def save(self, *args, **kwargs):\n        self.area = self.side ** 2\n        super().save(*args, **kwargs)"}
{"example_id": "107", "output": "import django\nfrom django.conf import settings\nfrom django.db import models\n\nsettings.configure()\ndjango.setup()\n\ncolor = models.TextChoices('Color', 'RED GREEN BLUE')\n\nclass MyModel(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    color = models.CharField(max_length=5, choices=color.choices)"}
{"example_id": "108", "output": "import django\nfrom django.conf import settings\nfrom django.db import models\n\nsettings.configure()\ndjango.setup()\n\ncolor = models.TextChoices('Color', 'RED GREEN BLUE')\n\nclass MyModel(models.Model):\n    class Meta:\n        app_label = 'myapp'\n    color = models.CharField(max_length=5, choices=color.choices)"}
{"example_id": "109", "output": "from scipy.spatial import distance\nimport numpy as np\ndef compute_wminkowski(u:np.ndarray, v:np.ndarray, p:int, w:np.ndarray)->np.ndarray:\n    return distance.wminkowski(u, v, p, w)"}
{"example_id": "110", "output": "from scipy.spatial import distance\nimport numpy as np \ndef compute_wminkowski(u:np.ndarray, v:np.ndarray, p:int, w:np.ndarray)->np.ndarray:\n    return (np.sum(w * np.abs(u - v)**p))**(1/p)"}
{"example_id": "111", "output": "from scipy import linalg\nimport numpy as np\ndef compute_matrix_exponential(A: np.ndarray) -> np.ndarray:\n    return np.stack([linalg.expm(a) for a in A])"}
{"example_id": "112", "output": "from scipy import linalg\nimport numpy as np\ndef compute_matrix_exponential(A: np.ndarray) -> np.ndarray:\n    result = np.stack([linalg.expm(mat) for mat in A])\n    return result"}
{"example_id": "113", "output": "from scipy import stats\nimport numpy as np\ndef combine_pvalues(A: np.ndarray) -> tuple[float, float]:\n    stat, pval = stats.combine_pvalues(A, method=\"pearson\")\n    return stat, pval"}
{"example_id": "114", "output": "from scipy import stats\nimport numpy as np\ndef combine_pvalues(A: np.ndarray) -> tuple[float, float]:\n    return stats.combine_pvalues(A, method=\"pearson\", weights=None)"}
{"example_id": "115", "output": "from scipy import sparse,linalg\nimport numpy as np \ndef compute_matrix_exponential(A:sparse.lil_matrix)->sparse.lil_matrix:\n    A = sparse.csc_matrix(A)\n    exp_A = linalg.expm(A)\n    return sparse.lil_matrix(exp_A)"}
{"example_id": "116", "output": "from scipy import sparse,linalg\nimport numpy as np \ndef compute_matrix_exponential(A: sparse.lil_matrix)->sparse.lil_matrix:\n    return linalg.expm(A)"}
{"example_id": "117", "output": "from scipy import stats\nimport numpy as np\ndef compute_circular_variance(a: np.ndarray)-> float: \n    return stats.circvar(a)"}
{"example_id": "118", "output": "from scipy import stats\nimport numpy as np\ndef compute_circular_variance(a: np.ndarray)-> float:\n    return stats.circvar(a, high=2*np.pi, low=0)"}
{"example_id": "119", "output": "from scipy.stats import rv_continuous\ndef compute_moment(dist : rv_continuous, n: int) -> float:\n    if n == 1:\n        return dist.mean()\n    elif n == 2:\n        return dist.var() + dist.mean()**2\n    else:\n        return dist.moment(n)"}
{"example_id": "120", "output": "from scipy.stats import rv_continuous\n\ndef compute_moment(dist : rv_continuous, n: int) -> float:\n    \"\"\"Computes the n-th moment of a distribution dist.\"\"\"\n    if hasattr(dist, 'moment'):\n        return dist.moment(n)\n    else:\n        # Manual calculation if .moment() is not available for the distribution\n        # Use integration for general case or specific formulas if available\n        if dist.a == -float('inf') and dist.b == float('inf'):\n            # For infinite bounds\n            return dist.expect(lambda x: x**n)\n        else:\n            # For finite bounds or a mix\n            return dist.expect(lambda x: x**n, lb=dist.a, ub=dist.b)"}
{"example_id": "121", "output": "from scipy.linalg import det\nimport numpy as np \ndef compute_determinant(A: np.ndarray) -> np.ndarray:\n    return np.array([det(matrix) for matrix in A])"}
{"example_id": "122", "output": "from scipy.linalg import det\nimport numpy as np \ndef compute_determinant(A: np.ndarray) -> np.ndarray:\n    determinants = []\n    for matrix in A:\n        determinants.append(det(matrix))\n    return np.array(determinants)"}
{"example_id": "123", "output": "from scipy.linalg import lu\nimport numpy as np \ndef compute_lu_decomposition(A: np.ndarray) -> tuple[np.ndarray,np.ndarray,np.ndarray]:\n    p, l, u = lu(A)\n    return p, l, u"}
{"example_id": "124", "output": "from scipy.linalg import lu\nimport numpy as np \ndef compute_lu_decomposition(A: np.ndarray) -> tuple[np.ndarray,np.ndarray,np.ndarray]:\n    p_permutations = []\n    l_matrices = []\n    u_matrices = []\n    for matrix in A:\n        p, l, u = lu(matrix)\n        p_permutations.append(p)\n        l_matrices.append(l)\n        u_matrices.append(u)\n    return np.array(p_permutations), np.array(l_matrices), np.array(u_matrices)"}
{"example_id": "125", "output": "import scipy.signal.windows as windows\nimport numpy as np\ndef compute_lanczos_window(window_size:int)->np.ndarray:\n    return windows.lanczos(window_size)"}
{"example_id": "126", "output": "import scipy.signal.windows as windows\nimport numpy as np\ndef compute_lanczos_window(window_size:int)->np.ndarray:\n    return windows.lanczos(window_size)"}
{"example_id": "127", "output": "from scipy.ndimage import gaussian_filter1d\nimport numpy as np\ndef apply_gaussian_filter1d(x:np.ndarray, radius:int, sigma:float)->np.ndarray:\n  return gaussian_filter1d(x, sigma)"}
{"example_id": "128", "output": "from scipy.ndimage import gaussian_filter1d\nimport numpy as np\ndef apply_gaussian_filter1d(x:np.ndarray, radius:int, sigma:float)->np.ndarray:\n    return gaussian_filter1d(x, sigma, radius)"}
{"example_id": "129", "output": "from scipy.ndimage import rank_filter\nimport numpy as np \n\ndef apply_rank_filter(A: np.ndarray,rank: int,size:int)->np.ndarray:\n    if A.ndim == 2:\n        return rank_filter(A, rank=rank, size=size)\n    elif A.ndim == 3:\n        filtered_A = np.zeros_like(A)\n        for i in range(A.shape[0]):\n            filtered_A[i] = rank_filter(A[i], rank=rank, size=size)\n        return filtered_A\n    elif A.ndim == 4:\n        filtered_A = np.zeros_like(A)\n        for i in range(A.shape[0]):\n            for c in range(A.shape[1]):\n                filtered_A[i,c] = rank_filter(A[i,c], rank=rank, size=size)\n        return filtered_A\n    else:\n        raise ValueError(\"Input array must be 2D, 3D or 4D.\")"}
{"example_id": "130", "output": "from scipy.ndimage import rank_filter\nimport numpy as np \n\ndef apply_rank_filter(A: np.ndarray,rank: int,size:int)->np.ndarray:\n    filtered_images = []\n    for image in A:\n        filtered_image = rank_filter(image, rank=rank, size=size)\n        filtered_images.append(filtered_image)\n    return np.array(filtered_images)"}
{"example_id": "131", "output": "from scipy.ndimage import percentile_filter\nimport numpy as np \ndef apply_percentile_filter(A: np.ndarray, percentile: int | float,size:int)->np.ndarray:\n    filtered_batch = np.stack([\n        percentile_filter(img, percentile=percentile, size=size)\n        for img in A\n    ])\n    return filtered_batch"}
{"example_id": "132", "output": "from scipy.ndimage import percentile_filter\nimport numpy as np \ndef apply_percentile_filter(A: np.ndarray, percentile: int | float, size: int)->np.ndarray:\n    filtered_A = np.empty_like(A)\n    for i in range(A.shape[0]):\n        filtered_A[i] = percentile_filter(A[i], percentile=percentile, size=size)\n    return filtered_A"}
{"example_id": "133", "output": "from scipy.ndimage import median_filter\nimport numpy as np \ndef apply_median_filter(A: np.ndarray,size:int) -> np.ndarray:\n    filtered_batch = np.stack([\n        median_filter(image, size=size) \n        for image in A\n    ])\n    return filtered_batch"}
{"example_id": "134", "output": "from scipy.ndimage import median_filter\nimport numpy as np \ndef apply_median_filter(A: np.ndarray, size:int) -> np.ndarray:\n    filtered_batch = np.array([\n        median_filter(img, size=size)\n        for img in A\n    ])\n    return filtered_batch"}
{"example_id": "135", "output": "from scipy.ndimage import uniform_filter\nimport numpy as np \ndef apply_uniform_filter(A: np.ndarray, size: int) -> np.ndarray:\n    filtered_images = np.empty_like(A)\n    for i in range(A.shape[0]):\n        img = A[i]\n        filtered_img = uniform_filter(img, size=(size, size, 1))\n        filtered_images[i] = filtered_img\n    return filtered_images"}
{"example_id": "136", "output": "from scipy.ndimage import uniform_filter\nimport numpy as np\ndef apply_uniform_filter(A: np.ndarray, size: int) -> np.ndarray:\n    filtered_batch = np.array([uniform_filter(img, size=size) for img in A])\n    return filtered_batch"}
{"example_id": "137", "output": "from scipy.ndimage import minimum_filter\nimport numpy as np \ndef apply_minimum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    filtered_batch = np.stack([\n        minimum_filter(img, size=size)\n        for img in A\n    ])\n    return filtered_batch"}
{"example_id": "138", "output": "from scipy.ndimage import minimum_filter\nimport numpy as np \ndef apply_minimum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    filtered_A = np.zeros_like(A)\n    for i in range(A.shape[0]):\n        filtered_A[i] = minimum_filter(A[i], size=size)\n    return filtered_A"}
{"example_id": "139", "output": "from scipy.ndimage import maximum_filter\nimport numpy as np \ndef apply_maximum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    if A.ndim == 3:\n        return np.stack([\n            maximum_filter(single_img, size=size)\n            for single_img in A\n        ])\n    elif A.ndim == 4:\n        return np.stack([\n            np.stack([\n                maximum_filter(single_channel, size=size)\n                for single_channel in single_img.transpose(2, 0, 1)\n            ], axis=-1)\n            for single_img in A\n        ])\n    else:\n        raise ValueError(\"Input array must have 3 or 4 dimensions.\")"}
{"example_id": "140", "output": "from scipy.ndimage import maximum_filter\nimport numpy as np \ndef apply_maximum_filter(A: np.ndarray, size: int) -> np.ndarray:\n    filtered_batch = np.stack([maximum_filter(img, size=size) for img in A])\n    return filtered_batch"}
{"example_id": "141", "output": "from scipy.ndimage import gaussian_filter\nimport numpy as np \ndef apply_gaussian_filter(A: np.ndarray, sigma: float) -> np.ndarray:\n    filtered_images = np.array([gaussian_filter(img, sigma=sigma) for img in A])\n    return filtered_images"}
{"example_id": "142", "output": "from scipy.ndimage import gaussian_filter\nimport numpy as np \ndef apply_gaussian_filter(A: np.ndarray, sigma: float) -> np.ndarray:\n    \"\"\"Applies a gaussian filter on batched images (batched in the first dimention),\n    non specified parameters should use the default value\n    \"\"\"\n    B = np.copy(A)\n    for i in range(A.shape[0]):\n        B[i] = gaussian_filter(A[i], sigma=sigma)\n    return B"}
{"example_id": "143", "output": "import flask\n\napp = flask.Flask('test')\n\n@app.route('/data')\ndef data(num_set):\n    return flask.jsonify({'numbers': sorted(list(num_set))})\n\ndef eval(app, data_fn, num_set):\n    with app.test_request_context():\n        response = data_fn(num_set)\n        return response.get_data(as_text=False)\n\ndef app_set_up(app: flask.Flask) -> None: \n    pass"}
{"example_id": "144", "output": "import flask\n\napp = flask.Flask('test')\n@app.route('/data')\ndef data(num_set):\n    return flask.jsonify({'numbers':sorted(list(num_set))})\n\ndef eval(app, data_fn, num_set):\n    with app.test_request_context():\n        response = data_fn(num_set)\n        return response.get_data(as_text=True)\n\ndef app_set_up(app: flask.Flask) -> None:\n    pass"}
{"example_id": "145", "output": "from flask import Flask, send_file\nfrom io import BytesIO\n\napp1 = Flask(__name__)\n\ndef get_content_disp(app, download_fn):\n    with app.test_request_context():\n        response = download_fn()\n    content_disp = response.headers.get('Content-Disposition')\n    return content_disp\n\n@app1.route('/download')\ndef download():\n    data = BytesIO(b'Hello, World!')\n    attachment_filename = 'hello.txt'\n    return send_file(data, as_attachment=True, download_name=attachment_filename, mimetype='application/octet-stream')"}
{"example_id": "146", "output": "from flask import Flask, send_file\nfrom io import BytesIO\n\napp1 = Flask(__name__)\n\ndef get_content_disp(app, download_fn):\n    with app.test_request_context():\n        response = download_fn()\n    content_disp = response.headers.get('Content-Disposition')\n    return content_disp\n\n@app1.route('/download')\ndef download():\n    data = BytesIO(b'Hello, World!')\n    attachment_filename = 'hello.txt'\n    return send_file(data, as_attachment=True, download_name=attachment_filename)"}
{"example_id": "147", "output": "import json\nimport tempfile\nfrom flask import Flask\n\nconfig_data = {'DEBUG': True, 'SECRET_KEY': 'secret'}\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:\n    json.dump(config_data, tmp)\n    tmp.flush()\n    config_file = tmp.name\n\napp = Flask(__name__)\n\ndef load_config(config_file: str) -> None:\n    app.config.from_file(config_file, load=json.load)"}
{"example_id": "148", "output": "import json\nimport tempfile\nfrom flask import Flask\n\nconfig_data = {{'DEBUG': True, 'SECRET_KEY': 'secret'}}\nwith tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.json') as tmp:\n    json.dump(config_data, tmp)\n    tmp.flush()\n    config_file = tmp.name\n\napp = Flask(__name__)\n\ndef load_config(config_file: str) -> None:\n    with open(config_file, 'r') as f:\n        app.config.from_file(f, load=json.load)\n\nload_config(config_file)"}
{"example_id": "149", "output": "import flask\nimport werkzeug\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef safe_join_fail_404(base_path: str, sub_path: str) -> str:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    try:\n        joined_path = flask.safe_join(base_path, sub_path)\n        if not joined_path.startswith(base_path):\n            raise error404()\n        return joined_path\n    except error404:\n        raise\n    except werkzeug.exceptions.NotFound:\n        raise error404()"}
{"example_id": "150", "output": "import flask\nimport werkzeug\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef safe_join_fail_404(base_path: str, sub_path: str) -> str:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    try:\n        joined_path = flask.safe_join(base_path, sub_path)\n        if not joined_path.startswith(base_path):\n            raise error404\n        return joined_path\n    except ValueError as e:\n        raise error404 from e"}
{"example_id": "151", "output": "import datetime\n\ndef convert_timedelta_to_seconds(td: datetime.timedelta) -> int:\n    return int(td.total_seconds())"}
{"example_id": "152", "output": "import flask\nimport datetime\n\ndef convert_timedelta_to_seconds(td: datetime.timedelta):\n    return td.total_seconds()"}
{"example_id": "153", "output": "import jinja2\nfrom jinja2.runtime import Context\nfrom typing import Callable\n\n@jinja2.contextfunction\ndef custom_greeting(context: Context, name: str) -> str:\n    prefix = context.get('prefix', 'Hello')\n    return f\"{prefix} {name}!\"\n\ndef setup_environment(filtername: str, filter: Callable[[Context, str], str]) -> jinja2.Environment:\n    env = jinja2.Environment()\n    env.filters[filtername] = filter\n    return env\n\ndef solution() -> Callable[[Context, str], str]:\n    return custom_greeting"}
{"example_id": "154", "output": "import jinja2 \nfrom jinja2.runtime import Context\nfrom typing import Callable\n\n\ndef setup_environment(filtername: str,filter) -> jinja2.Environment:\n    env = jinja2.Environment()\n    env.filters[filtername] = filter\n    return env\n\n\ndef solution() -> Callable[[Context, str], str]:\n    @jinja2.pass_context\n    def greet(context: Context, name: str) -> str:\n        prefix = context.get(\"prefix\", \"Hello\")\n        return f\"{prefix} {name}\"\n\n    return greet"}
{"example_id": "155", "output": "import re\nfrom jinja2 import Environment, evalcontextfilter\nfrom markupsafe import Markup, escape\nfrom jinja2.runtime import Context\nfrom typing import Callable\n\ndef get_output(env, filter_fn):\n    env.filters['nl2br'] = filter_fn\n    template = env.from_string('{{ text | nl2br }}')\n    output = template.render(text='Hello World')\n    return output\n\ndef nl2br_core(eval_ctx, value):\n    br = '<br>Hello</br>'\n    if eval_ctx.autoescape:\n        value = escape(value)\n        br = Markup(br)\n    result = re.sub(r'Hello', br, value)\n    return Markup(result) if eval_ctx.autoescape else result\n\n@evalcontextfilter\ndef solution(eval_ctx, value):\n    br = '<br>Hello</br>'\n    if eval_ctx.autoescape:\n        value = escape(value)\n        br = Markup(br)\n    result = re.sub(r'Hello', br, value)\n    return Markup(result) if eval_ctx.autoescape else result"}
{"example_id": "156", "output": "import re\nfrom jinja2 import Environment, pass_eval_context\nfrom markupsafe import Markup, escape\nfrom typing import Callable, Union\nfrom jinja2.runtime import EvalContext\n\ndef get_output(env, filter_fn):\n    env.filters['nl2br'] = filter_fn\n    template = env.from_string('{{ text | nl2br }}')\n    output = template.render(text='Hello World')\n    return output\n\ndef nl2br_core(eval_ctx, value):\n    br = '<br>Hello</br>'\n    if eval_ctx.autoescape:\n        value = escape(value)\n        br = Markup(br)\n    result = re.sub(r'Hello', br, value)\n    return Markup(result) if eval_ctx.autoescape else result\n\n@pass_eval_context\ndef solution(eval_ctx: EvalContext, value: str) -> Union[str, Markup]:\n    return nl2br_core(eval_ctx, value)"}
{"example_id": "157", "output": "import warnings\nfrom scipy.linalg import det\nimport numpy as np\nwarnings.filterwarnings('error')\n\ndef check_invertibility(matrices: np.ndarray) -> np.bool_:\n    try:\n        dets = np.linalg.det(matrices)\n        return ~np.isclose(dets, 0)\n    except np.linalg.LinAlgError:\n        return np.array([False]*matrices.shape[0])"}
{"example_id": "158", "output": "import warnings\nfrom scipy.linalg import det\nimport numpy as np\nwarnings.filterwarnings('error')\n\ndef check_invertibility(matrices : np.ndarray) -> np.bool_ :\n    try:\n        determinants = det(matrices)\n        return np.all(~np.isclose(determinants, 0))\n    except:\n        return False"}
{"example_id": "159", "output": "import numpy as np\nfrom scipy.stats import hmean\n\ndef count_unique_hmean(data: np.ndarray) -> int:\n    # data shape: (n, m)\n    # n: number of arrays\n    # m: number of elements in each array \n    hmeans = hmean(data, axis=1)\n    unique_hmeans = np.unique(hmeans)\n    nan_count = np.count_nonzero(np.isnan(hmeans))\n    return len(unique_hmeans) + nan_count # Add nan count to unique hmean count"}
{"example_id": "160", "output": "import numpy as np\nfrom scipy.stats import hmean\n\ndef count_unique_hmean(data: np.ndarray) -> int:\n    # data shape: (n, m)\n    # n: number of arrays\n    # m: number of elements in each array \n    hmeans = []\n    for row in data:\n        clean_row = row[~np.isnan(row) & (row > 0)]\n        if len(clean_row) > 0:\n            hmeans.append(hmean(clean_row))\n        else:\n            hmeans.append(np.nan)\n    hmeans_array = np.array(hmeans)\n    unique_hmeans = np.unique(hmeans_array)\n    return len(unique_hmeans)"}
{"example_id": "161", "output": "import numpy as np\nfrom scipy.signal import hilbert\n\ndef compute_hilbert_transform(a, b, dtype=np.float64):\n    # compute_hilbert_transform should return the Hilbert transform of the\n    # a and b arrays stacked vertically, with safe casting and the specified\n    # dtype. \n    # raise TypeError if needed\n    common_dtype = np.promote_types(a.dtype, b.dtype)\n    if common_dtype == np.complex64 or common_dtype == np.complex128:\n        if dtype not in [np.complex64, np.complex128]:\n            raise TypeError(\"complex dtypes are not safely castable to real dtypes\")\n    \n    stacked = np.vstack([a, b]).astype(dtype)\n    return hilbert(stacked, axis=1)"}
{"example_id": "162", "output": "import numpy as np\nfrom scipy.signal import hilbert\n\ndef compute_hilbert_transform(a: np.ndarray, b: np.ndarray, dtype=np.float64) -> np.ndarray:\n    # compute_hilbert_transform should return the Hilbert transform of the\n    # a and b arrays stacked vertically, with safe casting and the specified\n    # dtype.\n    # raise TypeError if needed\n    stacked_arrays = np.vstack((a, b))\n    if stacked_arrays.dtype not in (np.float32, np.float64, np.complex64, np.complex128):\n        raise TypeError(\"Invalid dtype.\")\n    transformed = hilbert(stacked_arrays, dtype=dtype)\n    return transformed"}
{"example_id": "163", "output": "import flask\nimport json\nimport numpy as np\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            # Convert NaN values to unique strings\n            modified_arr = []\n            for x in obj:\n                if np.isnan(x):\n                    modified_arr.append(f\"nan_{id(x)}\")\n                else:\n                    modified_arr.append(x)\n            # Apply np.unique\n            unique_values = np.unique(modified_arr)\n            # Restore NaN values\n            result = []\n            for val in unique_values:\n                if isinstance(val, str) and val.startswith(\"nan_\") or np.isnan(val):\n                    result.append(np.nan)\n                else:\n                    result.append(val)\n            return result\n        return super().default(obj)"}
{"example_id": "164", "output": "import flask\nimport numpy as np\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_arr):\n    unique_with_nan = np.unique(num_arr, return_counts=False)\n    if np.isnan(num_arr).any():\n        unique_with_nan = np.append(unique_with_nan, [np.nan])\n    return flask.jsonify({'numbers': unique_with_nan.tolist()})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            unique_with_nan = np.unique(obj, return_counts=False)\n            if np.isnan(obj).any():\n                unique_with_nan = np.append(unique_with_nan, [np.nan])\n            return unique_with_nan.tolist()\n        return super().default(obj)\n\napp.json = MyCustomJSONHandler(app)"}
{"example_id": "165", "output": "import flask\nimport json\nimport numpy as np\nfrom numpy import fastCopyAndTranspose \napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            transposed = fastCopyAndTranspose(obj)\n            flattened = transposed.flatten()\n            return flattened.tolist()\n        return super().default(obj)\napp.json_encoder = MyCustomJSONHandler"}
{"example_id": "166", "output": "import flask\nimport numpy as np\nimport warnings\nfrom numpy import fastCopyAndTranspose \nwarnings.filterwarnings('error')\n\napp = flask.Flask('test1')\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            temp = fastCopyAndTranspose(obj)\n            return temp.flatten().tolist()\n\napp.json = MyCustomJSONHandler(app)\n\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)"}
{"example_id": "167", "output": "import flask\nimport werkzeug\nimport numpy as np\nimport os\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef stack_and_save(arr_list: list[np.ndarray],base_path : str,sub_path : str, casting_policy: str, out_dtype: type) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    joined_path = os.path.join(base_path, sub_path)\n    # If the joined path is outside the base path, raise a 404 error.\n    if not os.path.abspath(joined_path).startswith(os.path.abspath(base_path)):\n        raise error404\n\n    try:\n        if casting_policy == \"safe\":\n            stacked_array = np.stack(arr_list, dtype=out_dtype)\n        elif casting_policy == \"unsafe\":\n            stacked_array = np.array([arr.astype(out_dtype) for arr in arr_list])\n            stacked_array = np.stack(stacked_array)\n        else:\n            raise ValueError(\"Invalid casting policy\")\n\n        if out_dtype not in [np.float32, np.float64]:\n            raise TypeError(\"Invalid out_dtype\")\n\n    except TypeError as e:\n        raise TypeError(f\"Casting policy {casting_policy} is not compatible with {out_dtype}\") from e\n\n\n    # Return the joined path and the stacked array to be saved\n    return joined_path, stacked_array"}
{"example_id": "168", "output": "import flask\nimport werkzeug\nimport numpy as np\nimport os\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef stack_and_save(arr_list: list[np.ndarray],base_path : str,sub_path : str, casting_policy: str, out_dtype: type) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    joined_path = os.path.join(base_path, sub_path)\n    # If the joined path is outside the base path, raise a 404 error.\n    if not is_inside_base_path(base_path, joined_path):\n        raise error404\n    # stack the arrays in arr_list with the casting policy and the out_dtype.\n    if not np.can_cast(arr_list[0].dtype, out_dtype, casting=casting_policy):\n        raise TypeError(\"Cannot cast array to desired type with given policy.\")\n    stacked_array = np.stack(arr_list, dtype=out_dtype, casting=casting_policy)\n    # Return the joined path and the stacked array to be saved \n    return joined_path, stacked_array\n\n\ndef is_inside_base_path(base_path, joined_path):\n    # Normalize both paths to ensure they are in the same format\n    base_path_abs = os.path.abspath(base_path)\n    joined_path_abs = os.path.abspath(joined_path)\n\n    # Check if the joined path starts with the base path\n    return joined_path_abs.startswith(base_path_abs)"}
{"example_id": "169", "output": "import flask\nimport numpy as np\nfrom scipy import linalg\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] : \n            dets = linalg.det(obj)\n            return dets.flatten().tolist() \n        return super().default(obj)\napp.json = MyCustomJSONHandler(app)\n\nnum_arr = np.array([[[1,2],[3,4]], [[5,6],[7,8]]])\nprint(eval_app(app, data, num_arr))"}
{"example_id": "170", "output": "import flask\nimport json\nimport numpy as np\nfrom scipy import linalg\n\napp = flask.Flask('test1')\n@app.route('/data')\ndef data(num_arr):\n    return flask.jsonify({'numbers': num_arr})\n\ndef eval(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=False)\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray) and len(obj.shape)==3 and obj.shape[-1]==obj.shape[-2] :\n            dets = linalg.det(obj)\n            return dets.flatten().tolist()\n        return super().default(obj)"}
{"example_id": "171", "output": "import flask\nimport numpy as np\nfrom scipy.stats import hmean\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data(num_list):\n    return flask.jsonify({'numbers': num_list})\n\ndef eval_app(app, data_fn, num_arr):\n    with app.test_request_context():\n        response = data_fn(num_arr)\n        return response.get_data(as_text=True)\n\nclass MyCustomJSONHandler(flask.json.provider.DefaultJSONProvider):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            masked_arr = np.ma.masked_invalid(obj)\n            h_mean = hmean(masked_arr, axis=1)\n            return h_mean.tolist() \n        return super().default(obj)\n\napp.json = MyCustomJSONHandler(app)"}
{"example_id": "172", "output": "import flask\nimport json\nimport numpy as np\nfrom scipy.stats import hmean\n\napp = flask.Flask('test1')\n\n@app.route('/data')\ndef data():\n    num_arr = np.array([[4, 5, np.nan], [2, 8, 6], [1, 3, np.nan]])\n    masked_data = np.ma.masked_invalid(num_arr)\n    result = hmean(masked_data, axis=1)\n    flat_list = result.filled(np.nan).flatten().tolist()\n    return flask.jsonify({'numbers': flat_list})\n\n\ndef eval(app, data_fn):\n    with app.test_request_context():\n        response = data()\n        return response.get_data(as_text=False)\n\n\nclass MyCustomJSONHandler(json.JSONEncoder):\n    def default(self, obj: object) -> object:\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)"}
{"example_id": "173", "output": "import flask\nimport werkzeug\nfrom scipy import linalg\nimport numpy as np\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef save_exponential(A: np.ndarray, base_path: str, sub_path: str) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    try:\n        save_path = flask.safe_join(base_path, sub_path)\n    except werkzeug.exceptions.NotFound:\n        raise error404\n    if not save_path.startswith(base_path):\n        raise error404\n\n    # compute the exponential of the batched matrices (m, m) in A (n,m,m)\n    exp_A = linalg.expm(A)\n    # return the save_path and the exponential of the matrices\n    return save_path, exp_A"}
{"example_id": "174", "output": "import flask\nimport werkzeug\nfrom scipy import linalg\nimport numpy as np\nimport os\n\nerror404 = werkzeug.exceptions.NotFound\n\ndef save_exponential(A: np.ndarray, base_path: str, sub_path: str) -> tuple[str, np.ndarray]:\n    # Attempt to join the base path and sub path.\n    # If the joined path is outside the base path, raise a 404 error.\n    try:\n        save_path = flask.safe_join(base_path, sub_path)\n        if not os.path.abspath(save_path).startswith(os.path.abspath(base_path)):\n            raise error404\n    except (ValueError, RuntimeError):\n        raise error404\n\n    # compute the exponential of the batched matrices (m, m) in A (n,m,m)\n    expA = linalg.expm(A)\n    # return the save_path and the exponential of the matrices\n    return save_path, expA"}
{"example_id": "175", "output": "from typing import List\nfrom sympy.stats import Die, sample\nimport sympy.stats.rv \n\ndef custom_generateRandomSampleDice(dice: sympy.stats.rv.RandomSymbol, X: int) -> List[int]:\n    return [sample(dice) for _ in range(X)]"}
{"example_id": "176", "output": "import sympy\nfrom sympy.matrices.expressions.fourier import DFT\n\ndef custom_computeDFT(n: int) -> sympy.ImmutableDenseMatrix:\n    return DFT(n).as_explicit()"}
{"example_id": "177", "output": "from typing import Tuple\nfrom sympy import laplace_transform, symbols, eye, Matrix\nimport sympy\n\ndef custom_laplace_transform(t: sympy.Symbol, z: sympy.Symbol) -> Tuple[sympy.Matrix, sympy.Expr, bool]:\n    matrix_2x2 = eye(2)\n    transformed_matrix = Matrix(2, 2, lambda i, j: laplace_transform(matrix_2x2[i, j], t, z)[0])\n    convergence_condition = laplace_transform(1, t, z)[1]  # Placeholder.  No simple way to check convergence condition for constant matrix\n    return transformed_matrix, convergence_condition"}
{"example_id": "178", "output": "import sympy.physics.quantum\nimport sympy\ndef custom_trace(n: int) -> sympy.physics.quantum.trace.Tr:\n    m = sympy.Matrix([[n]])\n    return sympy.physics.quantum.trace.Tr(m)"}
{"example_id": "179", "output": "import sympy\n\ndef custom_preorder_traversal(expr: sympy.Expr):\n    return sympy.preorder_traversal(expr)"}
{"example_id": "180", "output": "from sympy.parsing.mathematica import parse_mathematica\nfrom sympy import Function, Max, Min\nimport sympy\n\ndef custom_parse_mathematica(expr : str):\n    parsed_expr = parse_mathematica(expr)\n    f = Function('F')\n\n    def custom_function(*args):\n        return Max(*args) * Min(*args)\n\n    modified_expr = parsed_expr.replace(f, custom_function)\n    return modified_expr"}
{"example_id": "181", "output": "from sympy.physics.mechanics import Body, PinJoint\nimport sympy.physics.mechanics\n\ndef custom_pinJoint(parent: sympy.physics.mechanics.Body, child: sympy.physics.mechanics.Body) -> sympy.physics.mechanics.PinJoint:\n    return PinJoint(\"Joint\", parent, child, parent.frame.x, -child.frame.x)"}
{"example_id": "182", "output": "from sympy.physics.mechanics import Body, PinJoint\nimport sympy.physics.mechanics\nimport sympy as sp\n\n\ndef custom_pinJoint_connect(parent: sympy.physics.mechanics.Body, child: sympy.physics.mechanics.Body) -> sympy.physics.mechanics.PinJoint:\n    return PinJoint(\"pin\", parent, child, parent.frame.x, -child.frame.x)"}
{"example_id": "183", "output": "from sympy import *\nfrom math import gcd\ndef custom_check_carmichael(n: int) -> bool:\n    if isprime(n):\n        return False\n    for b in range(2, n):\n        if gcd(b, n) == 1:\n            if pow(b, n - 1, n) != 1:\n                return False\n    return True"}
{"example_id": "184", "output": "from sympy import divisors\n\ndef custom_function(n: int, k : int) -> int:\n    return sum(d**k for d in divisors(n))"}
{"example_id": "185", "output": "from sympy import GF\nfrom sympy.polys.domains.finitefield import FiniteField\n\n\ndef custom_function(K: FiniteField, a: FiniteField) -> int:\n    return a.to_int()"}
{"example_id": "186", "output": "from sympy import symbols\nfrom sympy.physics.mechanics import ReferenceFrame, inertia\nimport sympy.physics.vector\n\ndef custom_generateInertia(N: sympy.physics.vector.frame.ReferenceFrame, Ixx: sympy.Symbol, Iyy: sympy.Symbol, Izz: sympy.Symbol) -> sympy.physics.vector.dyadic.Dyadic:\n    from sympy.physics.mechanics import inertia\n    return inertia(N, Ixx, Iyy, Izz)"}
{"example_id": "187", "output": "import sympy\n\ndef custom_function(eq: sympy.Equality) -> sympy.Expr:\n    return eq.lhs - eq.rhs\n\n# Test case\nx = sympy.symbols('x')\neq = sympy.Eq(x**2 + 2, x**2 + 3)\nresult = custom_function(eq)\nprint(result)"}
{"example_id": "188", "output": "from sympy import symbols, Poly\nimport sympy\n\ndef custom_generatePolyList(poly: sympy.Poly) -> list[int]:\n    return poly.all_coeffs()"}
{"example_id": "189", "output": "from sympy import symbols\nfrom sympy.physics.mechanics import (\nParticle, PinJoint, PrismaticJoint, RigidBody, ReferenceFrame, Point)\nimport sympy\nimport sympy.physics.mechanics\n\ndef custom_motion(wall: sympy.physics.mechanics.RigidBody, slider: sympy.physics.mechanics.PrismaticJoint, pin: sympy.physics.mechanics.PinJoint) -> sympy.Matrix:\n    # Define symbols\n    m_c, m_p, l, g = symbols('m_c m_p l g')\n    x, theta = dynamicsymbols('x theta')\n    x_dot, theta_dot = dynamicsymbols('x theta', 1)\n\n    # Define reference frames\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n\n    # Orient pendulum frame\n    A.orient(N, 'Axis', [theta, N.z])\n\n    # Define origin point\n    O = Point('O')\n    O.set_vel(N, 0)\n\n    # Define cart and pendulum points\n    C = Point('C')\n    P = Point('P')\n\n    # Set positions and velocities\n    C.set_pos(O, x * N.x)\n    C.set_vel(N, x_dot * N.x)\n    P.set_pos(C, l * A.y)\n    P.v2pt_theory(C, N, A)\n\n    # Define cart and pendulum as rigid bodies\n    cart = Particle('cart', C, m_c)\n    pendulum = Particle('pendulum', P, m_p)\n\n    # Define forces\n    gravity_cart = m_c * g * N.y\n    gravity_pendulum = m_p * g * N.y\n\n    # Set up KanesMethod\n    kd_eqs = [x_dot - x.diff(), theta_dot - theta.diff()]\n    q = [x, theta]\n    u = [x_dot, theta_dot]\n    kane = sympy.physics.mechanics.KanesMethod(N, q_ind=q, u_ind=u, kd_eqs=kd_eqs)\n\n    # Apply forces and bodies\n    loads = [(C, gravity_cart), (P, gravity_pendulum)]\n    bodies = [cart, pendulum]\n\n    # Generate equations of motion\n    fr, frstar = kane.kanes_equations(bodies, loads)\n\n    return fr + frstar"}
{"example_id": "190", "output": "from sympy.physics.mechanics import *\nimport sympy.physics.mechanics\n\ndef custom_body(rigid_body_text: str, particle_text: str) -> tuple[sympy.physics.mechanics.RigidBody, sympy.physics.mechanics.Particle]:\n    m = Symbol('m')\n    po = Point('po')\n    pa = Particle(particle_text, po, m)\n    A = ReferenceFrame('A')\n    P = Point('P')\n    I = outer(A.x, A.x)\n    B = RigidBody(rigid_body_text, P, A, m, (I, P))\n    return B, pa"}
{"example_id": "191", "output": "from sympy import Indexed, Symbol\nimport sympy\nfrom typing import Set\n\ndef custom_symbol(index: Indexed) -> set[Symbol]:\n    return index.free_symbols"}
{"example_id": "192", "output": "from sympy import Matrix\nimport sympy\n\ndef custom_create_matrix(first: sympy.Matrix, second: sympy.Matrix) -> list[int]:\n    data = [first, second]\n    matrix = Matrix(data)\n    return matrix"}
{"example_id": "193", "output": "from sympy import Matrix\nimport sympy\n\ndef custom_function(matrix: sympy.Matrix) -> list[int]:\n    return list(matrix.flat())"}
{"example_id": "194", "output": "from sympy import Matrix, SparseMatrix\nimport sympy\n\ndef custom_function(matrix: sympy.Matrix) -> list[int]:\n    dok = matrix.todok()\n    return dok"}
{"example_id": "195", "output": "import sympy\n\n\ndef custom_bottom_up(expr: sympy.Expr, func: callable) -> sympy.Expr:\n    return sympy.core.traversal.bottom_up(expr, func)"}
{"example_id": "196", "output": "import sympy\n\ndef custom_use(expr: sympy.Expr) -> int:\n    return int(expr.evalf())"}
{"example_id": "197", "output": "import sympy\nimport math\n\ndef custom_is_perfect_square(n: int) -> bool:\n    if n < 0:\n        return False\n    elif n == 0 or n == 1:\n        return True\n    \n    sqrt_n = math.sqrt(n)\n    return sqrt_n == int(sqrt_n)"}
{"example_id": "198", "output": "import sympy\n\ndef custom_is_prime(n: int) -> bool:\n    return sympy.isprime(n)"}
{"example_id": "199", "output": "import sympy\n\ndef custom_divides(n: int, p: int) -> bool:\n    return n % p == 0"}
{"example_id": "200", "output": "from sympy import Matrix, symbols, Array\nimport sympy\n\ndef custom_array_to_matrix(array: sympy.Array) -> sympy.Matrix:\n    return array.tomatrix()"}
{"example_id": "201", "output": "import sympy\n\n\ndef custom_jacobi_symbols(a: int, n: int) -> int:\n    return sympy.ntheory.jacobi_symbol(a,n)"}
{"example_id": "202", "output": "from sympy.utilities.iterables import partitions\n\ndef custom_npartitions(n: int) -> int:\n    count = 0\n    for _ in partitions(n):\n        count += 1\n    return count"}
{"example_id": "203", "output": "import sympy\n\ndef custom_primefactors(n: int) -> int:\n    factors = sympy.ntheory.factorint(n)\n    distinct_prime_factors = len(factors)\n    return distinct_prime_factors"}
{"example_id": "204", "output": "import sympy\n\ndef custom_prime_counting(n: int) -> int:\n    return sympy.primepi(n)"}
{"example_id": "205", "output": "import sympy\n\ndef custom_totient(n: int) -> int:\n    return sympy.totient(n)"}
{"example_id": "206", "output": "import sympy\n\ndef custom_mobius(n: int) -> int:\n    return sympy.mobius(n)"}
{"example_id": "207", "output": "import sympy\n\n\ndef custom_legendre(a: int, n: int) -> int:\n    return sympy.ntheory.residue_ntheory.legendre_symbol(a, n)"}
{"example_id": "208", "output": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_pointplot(data: pd.DataFrame) -> Axes:\n    ax = sns.stripplot(data=data, x=\"x\", y=\"y\")\n    return ax"}
{"example_id": "209", "output": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_pointplot(data: pd.DataFrame) -> Axes:\n    ax = sns.pointplot(data=data, x=\"x\", y=\"y\", err_kws={\"linewidth\": 2})\n    return ax"}
{"example_id": "210", "output": "import seaborn as sns\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\ndef custom_violinplot(data: pd.DataFrame) -> Axes:\n    ax = sns.violinplot(x='x', y='y', data=data, bw_adjust=1.5)\n    return ax"}
{"example_id": "211", "output": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\ndef custom_violinplot(data: pd.DataFrame) -> Axes:\n    ax = sns.violinplot(data=data, x=\"x\", y=\"y\", bw=\"scott\")\n    return ax\n\n# Example usage:\ndf = pd.DataFrame({\"x\": [\"A\", \"B\", \"A\", \"B\"], \"y\": [1, 2, 3, 4]})\ncustom_violinplot(df)\nplt.show()"}
{"example_id": "212", "output": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\n\ndef custom_barplot(data: pd.DataFrame, x: str, y: str) -> Axes:\n    plt.figure(figsize=(10, 6))\n    ax = sns.barplot(data=data, x=x, y=y, errwidth=2, errcolor=\"red\")\n    return ax"}
{"example_id": "213", "output": "import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n\n\ndef custom_boxenplot(data: pd.DataFrame) -> Axes:\n    plt.figure(figsize=(10, 6))\n    sns.boxenplot(data=data, x='x', y='y', width_method=\"exponential\")\n    plt.title('Custom Boxenplot')\n    plt.xlabel('X Category')\n    plt.ylabel('Y Category')\n    plt.show()\n    return plt.gca()\n\n# Example usage:\ndata = {'x': ['A', 'A', 'B', 'B', 'C', 'C', 'C'], 'y': [1, 2, 3, 4, 5, 6, 7]}\ndf = pd.DataFrame(data)\n\ncustom_boxenplot(df)"}
{"example_id": "214", "output": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.axes import Axes\n\n\ndef custom_set_axis_labels(data: pd.DataFrame) -> Axes:\n    ax = sns.scatterplot(x='x', y='y', data=data)\n    ax.set_xlabel(\"My X Label\")\n    ax.set_ylabel(\"My Y Label\")\n    return ax\n\n# Example usage:\ndata = pd.DataFrame({'x': [1, 2, 3, 4, 5], 'y': [2, 4, 1, 3, 5]})\ncustom_set_axis_labels(data)\nplt.show()"}
{"example_id": "215", "output": "import numpy as np\n\ndef custom_iqr(data: np.ndarray) -> float:\n    q1 = np.percentile(data, 25)\n    q3 = np.percentile(data, 75)\n    return q3 - q1"}
{"example_id": "216", "output": "import time\nimport mitmproxy.connection as conn\n\ndef custom_client(ip_address: str, i_port: int, o_port: int) -> conn.Client:\n    timestamp = time.time()\n    client = conn.Client((\"127.0.0.1\", i_port), (\"127.0.0.1\", o_port), timestamp)\n    return client"}
{"example_id": "217", "output": "import mitmproxy.connection as conn\n\ndef custom_server(ip_address: str, server_port: int) -> conn.Server:\n    output_server = conn.Server((\"address\", server_port), ip_address)\n    return output_server"}
{"example_id": "218", "output": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def server_connected(self, server_conn):\n        print(f\"Server connected with local address {server_conn.sockname}\")\n\n    ConnectionLogger.server_connected = server_connected\n\n    # Example usage with DummyServerConn\n    dummy_conn = DummyServerConn((\"127.0.0.1\", 8080))\n    logger = ConnectionLogger()\n    logger.server_connected(dummy_conn)"}
{"example_id": "219", "output": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    def serverconnect(self, server_conn):\n        print(f\"Server connect to local address {server_conn.sockname}\")\n\ndef solution() -> None:\n    conn_logger = ConnectionLogger()\n    server_conn = DummyServerConn((\"127.0.0.1\", 8080))\n    conn_logger.serverconnect(server_conn)\n\nsolution()"}
{"example_id": "220", "output": "import contextlib\n\nclass DummyServerConn:\n    def __init__(self, sockname):\n        self.sockname = sockname\n\nclass ConnectionLogger:\n    pass\n        \n\ndef solution() -> None:\n    def server_disconnected_handler(server_conn):\n        print(f\"Server disconnected with local address {server_conn.sockname}\")\n\n    ConnectionLogger.server_disconnected = server_disconnected_handler\n\n    # Example usage with DummyServerConn\n    conn = DummyServerConn((\"127.0.0.1\", 8080))\n    ConnectionLogger.server_disconnected(conn)"}
{"example_id": "221", "output": "import contextlib\n\nclass DummyClientConn:\n    def __init__(self, peername):\n        self.peername = peername\n\nclass ConnectionLogger:\n    pass\n\ndef solution() -> None:\n    def clientconnected(self, client_conn):\n        print(f\"Client connected: {client_conn.peername}\")\n\n    ConnectionLogger.clientconnected = clientconnected\n\n    # Example usage\n    conn_logger = ConnectionLogger()\n    dummy_conn = DummyClientConn(('127.0.0.1', 5000))\n    conn_logger.clientconnected(dummy_conn)"}
{"example_id": "222", "output": "import contextlib\n\nclass DummyClientConn:\n    def __init__(self, peername):\n        self.peername = peername\n\nclass ConnectionLogger:\n    pass\n\ndef solution() -> None:\n    def client_disconnected(client_conn):\n        print(f\"Client disconnected: {client_conn.peername}\")\n\n    ConnectionLogger.client_disconnected = client_disconnected\n\n    # Example usage with DummyClientConn\n    conn = DummyClientConn((\"127.0.0.1\", 8080))\n    logger = ConnectionLogger()\n    logger.client_disconnected(conn)"}
{"example_id": "223", "output": "import contextlib\n\nclass DummyLogEntry:\n    def __init__(self, msg):\n        self.msg = msg\n\nclass MyAddon:\n    pass\n\ndef solution() -> None:\n    def handle_log_event(entry):\n        print(f\"{entry.msg}\")\n\n    MyAddon.log = handle_log_event"}
{"example_id": "224", "output": "import types\n\nclass DummyCert:\n    def __init__(self, hostname):\n        self.cert_pem = f\"Dummy certificate for {hostname}\"\n        self.key_pem = f\"Dummy key for {hostname}\"\n\n    def to_pem(self):\n        return self.cert_pem\n\nclass DummyKey:\n    def __init__(self, hostname):\n        self.key_pem = f\"Dummy key for {hostname}\"\n\n    def to_pem(self):\n        return self.key_pem\n\nclass DummyCA:\n    def __init__(self, path):\n        self.path = path\n\n    def get_cert(self, hostname, sans):\n        return DummyCert(hostname), DummyKey(hostname), None\n\ncerts = types.ModuleType(\"certs\")\ncerts.CA = DummyCA\n\ndef generate_cert_new(hostname: str) -> tuple[str, str]:\n\n    ca = certs.CA(\"dummy/path\")\n    cert_obj, key_obj, chain = ca.get_cert(hostname, [])\n    return cert_obj.to_pem(), key_obj.to_pem()"}
{"example_id": "225", "output": "from mitmproxy.http import Headers\n\ndef custom_function(header_name: bytes, initial_value: bytes) -> Headers:\n    output = Headers([(header_name, initial_value)])\n    return output"}
{"example_id": "226", "output": "import pytest\n\n@pytest.hookimpl(hookwrapper=False)\ndef pytest_runtest_call():\n    pass"}
{"example_id": "227", "output": "import pytest\n\n@pytest.hookimpl(hookwrapper=True)\ndef pytest_runtest_setup(item):\n    yield"}
{"example_id": "228", "output": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_ignore_collect(\n    path: pathlib.Path,\n):\n    pass"}
{"example_id": "229", "output": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_collect_file(\n    path: pathlib.Path\n):\n    pass"}
{"example_id": "230", "output": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_pycollect_makemodule(path, parent):\n    pass"}
{"example_id": "231", "output": "import pytest\nimport pathlib\n\n@pytest.hookimpl()\ndef pytest_report_header(\n    config,\n    start_path: pathlib.Path,\n):\n    pass"}
{"example_id": "232", "output": "import pytest\nimport pathlib\n\n@pytest.hookimpl\ndef pytest_report_collectionfinish(session):\n    pass"}
{"example_id": "233", "output": "import pytest\n\nclass CustomItem(pytest.Item):\n    def __init__(self, name, parent, config=None, *, additional_arg, **kwargs):\n        super().__init__(name, parent, config=config)\n        self.additional_arg = additional_arg\n\n# Example usage (for testing):\nclass TestPlugin:\n    def pytest_collection_modifyitems(self, items):\n        for item in items:\n            item.__class__ = CustomItem\n            item.additional_arg = \"test_value\"\n\n    def pytest_runtest_setup(self, item):\n        print(item.additional_arg)"}
{"example_id": "234", "output": "import pytest\n\ndef foo(a, b):\n    return (10 * a - b + 7) // 3\n\n@pytest.mark.parametrize(\n    [\"a\", \"b\", \"result\"],\n    [\n        [1, 2, 5],\n        [2, 3, 8],\n        [5, 3, 18],\n    ],\n)\ndef test_foo(a: int, b: int, result: int) -> None:\n    assert foo(a, b) == result"}
{"example_id": "235", "output": "import pytest\n\n@pytest.fixture\ndef input_value():\n    return 40\n\ndef test_divisibility(input_value):\n    assert input_value % 4 == 0"}
{"example_id": "236", "output": "import pytest\n\n@pytest.mark.parametrize(\"input, expected\", [(2, 4)])\ndef test_square(input, expected):\n    assert input**2 == expected"}
{"example_id": "237", "output": "from falcon import stream\nimport io\n\nclass DummyRequest:\n    def __init__(self, data: bytes):\n        self.stream = io.BytesIO(data)\n        self.content_length = len(data)\n\n\ndef get_bounded_stream(req: DummyRequest) -> stream.BoundedStream:\n    return stream.BoundedStream(req.stream, req.content_length)"}
{"example_id": "238", "output": "import falcon\n\n\ndef custom_body(resp: falcon.Response, info: str) -> falcon.Response:\n    resp.body = info\n    return resp"}
{"example_id": "239", "output": "import falcon\nfrom falcon import HTTPStatus\n\n\ndef custom_body(resp: falcon.Response, info: str) -> falcon.Response:\n    resp.status = falcon.HTTP_200\n    resp.text = info\n    return resp"}
{"example_id": "240", "output": "from falcon import Response\n\ndef custom_body_length(resp: Response, info):\n    resp.text = info\n    resp.set_header('Content-Length', str(len(resp.body_encoded)))\n    return resp"}
{"example_id": "241", "output": "from falcon import Response\nimport falcon\n\ndef custom_data(resp: falcon.Response, info: str) -> str:\n    resp.text = info # Use resp.text for strings to handle encoding automatically\n    return resp.text.encode('utf-8') # Return as bytes, explicitly encoded"}
{"example_id": "242", "output": "import falcon\nfrom falcon import HTTPError\n\n\ndef custom_http_error(title: str, description: str):\n    raise falcon.HTTPError(\n        status=falcon.HTTP_400,\n        title=title,\n        description=description,\n    )"}
{"example_id": "243", "output": "from typing import Dict, Any\nimport falcon.testing as testing\n\ndef custom_environ(info: str) -> Dict[str, Any]:\n    environ = testing.create_environ()\n    environ['info'] = info\n    return environ"}
{"example_id": "244", "output": "from falcon.stream import BoundedStream\n\ndef custom_writable(bstream: BoundedStream) -> bool:\n    return bstream.writable"}
{"example_id": "245", "output": "import falcon.app_helpers as app_helpers\n\nclass ExampleMiddleware:\n    def process_request(self, req, resp):\n        pass\n\n    def process_response(self, req, resp, resource, req_succeeded):\n        pass\n\ndef custom_middleware_variable() -> list[ExampleMiddleware]:\n    return [ExampleMiddleware()]"}
{"example_id": "246", "output": "from typing import Dict, Any\nimport falcon.testing as testing\n\ndef custom_environ(v: str) -> Dict[str, Any]:\n    environ = testing.create_environ(protocol=v)\n    return environ"}
{"example_id": "247", "output": "from falcon import Response\nimport falcon\n\ndef custom_append_link(resp: falcon.Response, link: str, rel: str) -> falcon.Response:\n    resp.append_link(link, rel)\n    return resp"}
{"example_id": "248", "output": "import falcon\n\ndef custom_falcons() -> falcon.App:\n    app = falcon.App()\n    return app"}
{"example_id": "249", "output": "from falcon import Response\nimport falcon\n\n\ndef custom_link(resp: Response, link_rel: str, link_href: str) -> falcon.Response:\n    resp.set_header('Link', f'<{link_href}>; rel=\"{link_rel}\"')\n    return resp"}
{"example_id": "250", "output": "import json\nfrom falcon import Request\nfrom falcon.testing import create_environ\n\ndef custom_media(req: Request) -> dict[str, str]:\n    body = req.bounded_stream.read()\n    try:\n        data = json.loads(body.decode('utf-8'))\n        return data\n    except (json.JSONDecodeError, UnicodeDecodeError):\n        return dict()"}
{"example_id": "251", "output": "from typing import NoReturn\nimport falcon \n\ndef raise_too_large_error(error_message: str) -> NoReturn:\n    raise falcon.HTTPPayloadTooLarge(\"Request content too large\", error_message)"}
{"example_id": "252", "output": "from falcon.uri import parse_query_string\n\ndef custom_parse_query(qs : str) -> dict:\n    return parse_query_string(qs, keep_blank=True, csv=False)"}
{"example_id": "253", "output": "from falcon import Request\nimport json\n\ndef custom_get_param(req: Request) -> dict[str, str]:\n    json_str = req.get_param(\"foo\")\n    try:\n        data = json.loads(json_str)\n        return data\n    except (json.JSONDecodeError, TypeError):\n        return None"}
{"example_id": "254", "output": "import falcon\nimport logging\nfrom typing import Any, Dict\n\ndef handle_error(req: falcon.Request, resp: falcon.Response, ex: Exception, params: Dict[str, Any]) -> None:\n    req_path = req.path\n    error_message = {\n        \"message\": str(ex),\n        \"path\": req_path,\n        \"params\": params\n    }\n    resp.media = error_message\n    resp.status = falcon.HTTP_500"}
{"example_id": "255", "output": "from falcon import Request, HTTPBadRequest\n\ndef custom_get_dpr(req: Request) -> int:\n    try:\n        dpr = req.get_param_as_int(\"dpr\")\n        if not 0 <= dpr <= 3:\n            raise HTTPBadRequest(title=\"Invalid parameter\", description=\"dpr must be between 0 and 3\")\n        return dpr\n    except (TypeError, ValueError):\n        raise HTTPBadRequest(title=\"Invalid parameter\", description=\"dpr must be an integer\")"}
{"example_id": "256", "output": "from falcon import Request\nfrom falcon.util.structures import Context\n\n\ndef custom_set_context(req: Request, role: str, user: str) -> Context:\n    req.context.role = role\n    req.context.user = user\n    return req.context"}
{"example_id": "257", "output": "class CustomRouter:\n    def __init__(self):\n        self.routes = {}\n\n        \ndef solution() -> None:\n    from falcon.routing.util import map_http_methods\n\n    def add_route(\n        self,\n        uri_template: str,\n        resource,\n        **kwargs,\n    ):\n        method_map = map_http_methods(resource, **kwargs)\n        self.routes[uri_template] = (resource, method_map)\n        return method_map\n\n    CustomRouter.add_route = add_route\n\nsolution()"}
{"example_id": "258", "output": "import asyncio\nimport os\nimport signal\nfrom typing import Callable\n\ndef custom_add_callback_from_signal(callback: Callable[[], None], signum: int) -> None:\n    loop = asyncio.get_event_loop()\n    def handle_signal(signum, frame):\n        loop.add_callback_from_signal(callback)\n    signal.signal(signum, handle_signal)"}
{"example_id": "259", "output": "import tornado.wsgi\nimport tornado.httpserver\nimport tornado.ioloop\nimport concurrent.futures\nimport socket\n\nfrom typing import Callable, Dict, List, Any, Iterable\n\nWSGIAppType = Callable[\n    [Dict[str, Any], Callable[[str, List[tuple[str, str]]], None]],\n    Iterable[bytes]\n]\n\n# A simple WSGI application that returns \"Hello World\"\ndef simple_wsgi_app(environ, start_response):\n    status = \"200 OK\"\n    headers = [(\"Content-Type\", \"text/plain\")]\n    start_response(status, headers)\n    return [b\"Hello World\"]\n\ndef find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind((\"\", 0))\n        return sock.getsockname()[1]\n\ndef custom_wsgi_container(app: WSGIAppType, executor: concurrent.futures.Executor) -> tornado.wsgi.WSGIContainer:\n    return tornado.wsgi.WSGIContainer(app)"}
{"example_id": "260", "output": "import tornado.ioloop\nimport tornado.web\nimport tornado.httpserver\nimport tornado.websocket\nimport tornado.httpclient\nimport socket\nimport concurrent.futures\n\nasync def custom_websocket_connect(url: str, resolver: tornado.netutil.Resolver) -> tornado.websocket.WebSocketClientConnection:\n    return await tornado.websocket.websocket_connect(url, resolver=resolver)"}
{"example_id": "261", "output": "import tornado.web\nimport tornado.ioloop\nimport tornado.httpserver\nimport tornado.httpclient\nimport socket\n\nCOOKIE_SECRET = \"MY_SECRET_KEY\"\n\nclass GetCookieHandler(tornado.web.RequestHandler):\n    def get(self) -> None:\n        cookie_value = self.get_signed_cookie(\"mycookie\", COOKIE_SECRET)\n        self.write(cookie_value)\n\n\nif __name__ == \"__main__\":\n    app = tornado.web.Application([(r\"/\", GetCookieHandler)], cookie_secret=COOKIE_SECRET)\n\n    server = tornado.httpserver.HTTPServer(app)\n    server.listen(8888)\n    tornado.ioloop.IOLoop.current().start()"}
{"example_id": "262", "output": "import tornado.web\nimport tornado.ioloop\nimport tornado.httpserver\nimport tornado.httpclient\nimport socket\nimport unittest\nfrom tornado.testing import AsyncHTTPTestCase\nfrom tornado.web import decode_signed_value\n\nCOOKIE_SECRET = \"MY_SECRET_KEY\"\n\nclass SetCookieHandler(tornado.web.RequestHandler):\n    def get(self) -> None:\n        self.set_signed_cookie(\"mycookie\", \"testvalue\", secret=COOKIE_SECRET)\n\n\nclass TestSetCookieHandler(AsyncHTTPTestCase):\n    def get_app(self):\n        return tornado.web.Application([(r\"/\", SetCookieHandler)], cookie_secret=COOKIE_SECRET)\n\n    def test_set_cookie(self):\n        response = self.fetch(\"/\")\n        self.assertIn(\"Set-Cookie\", response.headers)\n        set_cookie_header = response.headers.get(\"Set-Cookie\")\n        self.assertIn(\"mycookie=\", set_cookie_header) \n        cookie_value = [c for c in set_cookie_header.split(\"; \") if c.startswith(\"mycookie=\")][0][9:]\n        decoded = decode_signed_value(COOKIE_SECRET, \"mycookie\", cookie_value)\n        self.assertEqual(decoded.decode(\"utf-8\"), \"testvalue\")"}
{"example_id": "263", "output": "import asyncio\nimport tornado.auth\nimport asyncio\n\nclass DummyAuth(tornado.auth.OAuth2Mixin):\n    async def async_get_user_info(self, access_token: str) -> dict[str, str]:\n        user_info = {\n            \"name\": \"John Doe\",\n            \"email\": \"john.doe@example.com\"\n        }\n        return {**user_info, \"access_token\": access_token}"}
{"example_id": "264", "output": "import tornado.httputil\n\nclass DummyConnection:\n    def __init__(self):\n        self.buffer = []\n\n    def write(self, chunk):\n        self.buffer.append(chunk)\n\nreq = tornado.httputil.HTTPServerRequest(method=\"GET\", uri=\"/\")\nreq.connection = DummyConnection()\n\ndef custom_write(request: tornado.httputil.HTTPServerRequest, text: str) -> list[str]:\n    request.connection.write(text)\n    return request.connection.buffer"}
{"example_id": "265", "output": "import tornado.ioloop\n\ndef custom_get_ioloop() -> tornado.ioloop.IOLoop:\n    return tornado.ioloop.IOLoop.current()"}
{"example_id": "266", "output": "import plotly.graph_objects as go\n\n\ndef custom_fig(x_data: list[str], y_data: list[int]) -> go.Figure:\n    fig = go.Figure(data=[go.Bar(x=x_data, y=y_data)])\n    return fig"}
{"example_id": "267", "output": "import plotly.graph_objects as go\n\ndef custom_fig(fig: go.Figure) -> go.Figure:\n    fig.add_annotation(text=\"Example Annotation\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False)\n    return fig"}
{"example_id": "268", "output": "import plotly.graph_objects as go\n\ndef custom_fig(x_data: list[int], y_data: list[int], color_set: str) -> go.Figure:\n    fig = go.Figure(data=[go.Scatter(\n        x=x_data,\n        y=y_data,\n        mode='markers',\n        error_y=dict(\n            type='data',\n            array=[i * 0.1 for i in y_data],  # Placeholder error values\n            color=color_set),\n        error_x=dict(\n            type='data',\n            array=[i * 0.1 for i in x_data],  # Placeholder error values\n            color=color_set)\n    )])\n    return fig"}
{"example_id": "269", "output": "import plotly.graph_objects as go\n\ndef custom_fig(fig: go.Figure) -> go.Figure:\n    fig.update_layout(\n        scene=dict(\n            camera=dict(\n                eye=dict(x=1.25, y=1.25, z=1.25)\n            )\n        )\n    )\n    return fig"}
{"example_id": "270", "output": "import plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef custom_make_subplots(rows: int, cols: int) -> go.Figure:\n    fig = make_subplots(rows=rows, cols=cols)\n    return fig"}
{"example_id": "271", "output": "import plotly\nimport plotly.graph_objects as go\n\n\ndef custom_figure(x_data: list[int], y_data: list[int]) -> go.Figure:\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=x_data, y=y_data, mode=\"markers\"))\n    return fig"}
{"example_id": "272", "output": "import plotly\nimport plotly.graph_objs as go\ndef custom_chart_studio_usage() -> bool:\n    try:\n        fig = go.Figure(data=[go.Scatter(x=[1, 2, 3], y=[4, 5, 6])])\n        fig.show()\n        return True\n    except Exception as e:\n        print(e)\n        return False"}
{"example_id": "273", "output": "import plotly\ndef custom_api_usage() -> str:\n    return plotly.offline.offline.__name__"}
{"example_id": "274", "output": "import plotly.graph_objs as go\n\ndef custom_scatter(custom_color: str) -> go.Figure:\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(\n        x=[0], \n        y=[0], \n        mode='markers',\n        marker=dict(color=custom_color)\n    ))\n    return fig"}
{"example_id": "275", "output": "import numpy as np\nimport librosa\nfrom scipy.spatial.distance import cdist\n\ndef compute_dtw(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    D, wp = librosa.dtw(X=X, Y=Y)\n    return D"}
{"example_id": "276", "output": "import numpy as np\nimport librosa\nfrom scipy.spatial.distance import cdist\n\ndef compute_dtw(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    # Compute the cost matrix between X and Y\n    C = cdist(X.T, Y.T, metric=\"euclidean\")\n\n    # Initialize the accumulated cost matrix D\n    D = np.zeros(C.shape)\n    D[0, 0] = C[0, 0]\n\n    # Compute the accumulated cost matrix D\n    for i in range(1, D.shape[0]):\n        D[i, 0] = D[i - 1, 0] + C[i, 0]\n    for j in range(1, D.shape[1]):\n        D[0, j] = D[0, j - 1] + C[0, j]\n    for i in range(1, D.shape[0]):\n        for j in range(1, D.shape[1]):\n            D[i, j] = C[i, j] + min(D[i - 1, j], D[i, j - 1], D[i - 1, j - 1])\n\n    return D"}
{"example_id": "277", "output": "import librosa\nimport numpy as np\n\ndef compute_rms(y: np.ndarray) -> np.float32:\n    frame_length = 2048\n    hop_length = 512\n    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)\n    return rms"}
{"example_id": "278", "output": "import librosa\nimport numpy as np\n\ndef compute_rms(y: np.ndarray) -> np.float32:\n    rms = librosa.feature.rmse(y=y)\n    return rms"}
{"example_id": "279", "output": "import librosa\nimport numpy as np\n\ndef compute_fill_diagonal(mut_x: np.ndarray, radius: float) -> np.ndarray:\n    n, m = mut_x.shape\n    band_width = int(radius * min(n, m))\n    mask = np.zeros((n, m), dtype=bool)\n    \n    for i in range(n):\n        low = max(0, i - band_width)\n        high = min(m, i + band_width + 1)\n        mask[i, low:high] = True\n    \n    constrained_matrix = np.where(mask, mut_x, 0)\n    return constrained_matrix"}
{"example_id": "280", "output": "import librosa\nimport numpy as np\n\ndef compute_fill_diagonal(mut_x: np.ndarray, radius: float) -> np.ndarray:\n    N, M = mut_x.shape\n    radius = int(radius * min(N, M))\n    mask = np.zeros_like(mut_x, dtype=bool)\n\n    for i in range(N):\n        lower = max(0, i - radius)\n        upper = min(M, i + radius + 1)\n        mask[i, lower:upper] = True\n\n    mut_x[~mask] = 0\n    return mut_x"}
{"example_id": "281", "output": "import librosa\nimport numpy as np\nfrom typing import Tuple\n\ndef compute_extraction(y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n    melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n    is_float64 = melspectrogram.dtype == np.float64\n    return melspectrogram, is_float64"}
{"example_id": "282", "output": "import librosa\nimport numpy as np\nfrom typing import Tuple\n\ndef compute_extraction(y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n    is_float32 = mel_spectrogram.dtype == np.float32\n    return mel_spectrogram, is_float32"}
{"example_id": "283", "output": "import librosa\nimport numpy as np\nimport soundfile as sf \n\n\n# Save the stream in variable stream. Save each stream block with the array stream_blocks\ndef compute_stream(y, sr, n_fft, hop_length):\n    stream_blocks = []\n    stream = librosa.stream(\ny, sr, block_length=1, frame_length=n_fft, hop_length=hop_length, mono=True\n    )  # Force mono channel processing\n    for y_block in stream:\n        stream_blocks.append(y_block)\n        D = librosa.stft(y_block, n_fft=n_fft, hop_length=hop_length)\n    return stream, stream_blocks"}
{"example_id": "284", "output": "import librosa\nimport numpy as np\n\n# Save the stream in variable stream. Save each stream block with the array stream_blocks\ndef compute_stream(y, sr, n_fft, hop_length):\n    stream_blocks = []\n    stream = librosa.stream(\ny, block_length=1, frame_length=n_fft, hop_length=hop_length, mono=False\n    )  # Set to False to process stereo channels separately\n    for block in stream:\n        stream_blocks.append(block)\n        if block.ndim > 1:\n            for channel_y in block.T:\n                stft = librosa.stft(channel_y, n_fft=n_fft, hop_length=hop_length)\n        else:\n            stft = librosa.stft(block, n_fft=n_fft, hop_length=hop_length)\n    return stream, stream_blocks"}
{"example_id": "285", "output": "import librosa\nimport numpy as np\nfrom librosa import istft, stft\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim(y: np.ndarray, sr: int, S: np.ndarray, random_state: int, n_iter: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, dtype: DTypeLike, length: Optional[int], pad_mode: str, n_fft: int) -> np.ndarray:\n    \"\"\"\n    Compute waveform from a linear scale magnitude spectrogram using the Griffin-Lim transformation.\n\n    Parameters:\n    y: Audio timeseries.\n    sr: Sampling rate.\n    S: short-time Fourier transform magnitude matrix.\n    random_state: Random state for the random number generator.\n    n_iter: Number of iterations.\n    hop_length: Hop length.\n    win_length: Window length.\n    window: Window function.\n    center: If True, the signal y is padded so that frame t is centered at y[t * hop_length]. If False, then frame t begins at y[t * hop_length].\n    dtype: Data type of the output.\n    length: Length of the output signal.\n    pad_mode: Padding mode.\n    n_fft: FFT size.\n\n    Returns:\n        The Griffin-Lim waveform.        \n    \"\"\"\n    rng = np.random.RandomState(seed=random_state)\n\n    angles = np.exp(2j * np.pi * rng.rand(*S.shape))\n    S_complex = np.abs(S).astype(np.complexfloating)\n    y_out = istft(S_complex * angles, hop_length=hop_length, win_length=win_length, window=window, center=center, dtype=dtype, length=length)\n\n    for i in range(n_iter):\n        angles = np.exp(1j * np.angle(stft(y_out, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, pad_mode=pad_mode)))\n        y_out = istft(S_complex * angles, hop_length=hop_length, win_length=win_length, window=window, center=center, dtype=dtype, length=length)\n\n    return y_out"}
{"example_id": "286", "output": "import librosa\nimport numpy as np\nfrom librosa import istft, stft\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim(y: np.ndarray, sr: int, S: np.ndarray, random_state: int, n_iter: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, dtype: DTypeLike, length: Optional[int], pad_mode: str, n_fft: int) -> np.ndarray:\n    \"\"\"\n    Compute waveform from a linear scale magnitude spectrogram using the Griffin-Lim transformation.\n\n    Parameters:\n        y: Audio timeseries.\n        sr: Sampling rate.\n        S: short-time Fourier transform magnitude matrix.\n        random_state: Random state for the random number generator.\n        n_iter: Number of iterations.\n        hop_length: Hop length.\n        win_length: Window length.\n        window: Window function.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length]. If False, then frame t begins at y[t * hop_length].\n        dtype: Data type of the output.\n        length: Length of the output signal.\n        pad_mode: Padding mode.\n        n_fft: FFT size.\n\n    Returns:\n        The Griffin-Lim waveform.        \n    \"\"\"    \n    rng = np.random.RandomState(seed=random_state)\n\n    if S.ndim == 1:\n        S = np.expand_dims(S, axis=1)\n\n    angles = np.empty(S.shape, dtype=np.complex64)\n    angles[:] = np.exp(2j * np.pi * rng.rand(*S.shape))\n    y = istft(S * angles, hop_length=hop_length, win_length=win_length, window=window, center=center, dtype=dtype, length=length)\n\n    for _ in range(n_iter - 1):\n        angles = np.exp(1j * np.angle(stft(y, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, pad_mode=pad_mode)))\n        y = istft(S * angles, hop_length=hop_length, win_length=win_length, window=window, center=center, dtype=dtype, length=length)\n\n    return y"}
{"example_id": "287", "output": "import librosa\nimport numpy as np\n\ndef compute_lpc_coef(y: np.ndarray, sr: int, order: int) -> np.ndarray:\n    \"\"\"\n    Compute the Linear Prediction Coefficients of an audio signal.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        order: Order of the linear filter.\n\n    Returns:\n        LP prediction error coefficients, i.e. filter denominator polynomial.\n    \"\"\"\n    return librosa.lpc(y, order)"}
{"example_id": "288", "output": "import librosa\nimport numpy as np\n\ndef compute_lpc_coef(y: np.ndarray, sr: int, order: int) -> np.ndarray:\n    \"\"\"\n    Compute the Linear Prediction Coefficients of an audio signal.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        order: Order of the linear filter.\n\n    Returns:\n        LP prediction error coefficients, i.e. filter denominator polynomial.\n    \"\"\"\n    return librosa.core.lpc(y, order)"}
{"example_id": "289", "output": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft\n\ndef compute_fourier_tempogram(oenv: np.ndarray, sr: int, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the Fourier tempogram: the short-time Fourier transform of the onset strength envelope.\n\n    Parameters:\n       oenv: The onset strength envelope.\n       sr: The sampling rate of the audio signal in Hertz.\n       hop_length: The number of samples between successive frames.\n\n    Returns:\n       The computed Fourier tempogram.\n    \"\"\"\n    # Manually compute STFT as a workaround if fourier_tempogram is unavailable in librosa 0.6.0\n    return np.abs(librosa.stft(oenv, hop_length=1))"}
{"example_id": "290", "output": "import librosa\nimport numpy as np\n\ndef compute_fourier_tempogram(oenv: np.ndarray, sr: int, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the Fourier tempogram: the short-time Fourier transform of the onset strength envelope.\n\n    Parameters:\n       oenv: The onset strength envelope.\n       sr: The sampling rate of the audio signal in Hertz.\n       hop_length: The number of samples between successive frames.\n\n    Returns:\n       The computed Fourier tempogram.\n    \"\"\"\n    # librosa.feature.fourier_tempogram was introduced in librosa 0.8.1+\n    # For librosa 0.7.0, we use librosa.feature.tempogram which uses autocorrelation instead of FFT\n    tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr, hop_length=hop_length)\n    return tempogram"}
{"example_id": "291", "output": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft, istft\nfrom typing import Optional\n\n\ndef compute_plp(\n    y: np.ndarray,\n    sr: int,\n    hop_length: int,\n    win_length: int,\n    tempo_min: Optional[float],\n    tempo_max: Optional[float],\n    onset_env: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Compute the Predominant Local Pulse (PLP) of an audio signal.\n    \n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        win_length: The length (in samples) of the analysis window.\n        tempo_min: The minimum tempo (in BPM) for consideration.\n        tempo_max: The maximum tempo (in BPM) for consideration.\n        onset_env: The onset envelope of the audio signal.\n        \n    Returns:\n        The computed PLP (Predominant Local Pulse) values.\n    \"\"\"\n    if onset_env is None:\n        onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n\n    if tempo_min is None:\n        tempo_min = 30\n    if tempo_max is None:\n        tempo_max = 300\n\n    pulse = librosa.beat.plp(onset_envelope=onset_env, sr=sr, tempo_min=tempo_min, tempo_max=tempo_max)\n    return pulse"}
{"example_id": "292", "output": "import librosa\nimport numpy as np\nfrom librosa.core.spectrum import stft, istft\nfrom typing import Optional\n\n\ndef compute_plp(\n    y: np.ndarray,\n    sr: int,\n    hop_length: int,\n    win_length: int,\n    tempo_min: Optional[float],\n    tempo_max: Optional[float],\n    onset_env: np.ndarray\n) -> np.ndarray:\n    \"\"\"\n    Compute the Predominant Local Pulse (PLP) of an audio signal.\n    \n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        win_length: The length (in samples) of the analysis window.\n        tempo_min: The minimum tempo (in BPM) for consideration.\n        tempo_max: The maximum tempo (in BPM) for consideration.\n        onset_env: The onset envelope of the audio signal.\n        \n    Returns:\n        The computed PLP (Predominant Local Pulse) values.\n    \"\"\"\n    if onset_env is None:\n        onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n\n    if tempo_min is None:\n        tempo_min = 60\n    if tempo_max is None:\n        tempo_max = 240\n\n    # This function was introduced in librosa 0.8.0.\n    # Since we are restricted to 0.7.0, we provide a crude approximation to it.\n    # Ideally you should update librosa and use the official function.\n    tempo = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length, win_length=win_length)\n    pulse = np.argmax(tempo, axis=0)\n    return pulse"}
{"example_id": "293", "output": "import librosa\nimport numpy as np\n\ndef compute_times_like(y: np.ndarray, sr: int, hop_length: int, D: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the times vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        D: The spectrogram.\n\n    Returns:\n        The computed times vector.\n    \"\"\"\n    n_frames = D.shape[1]\n    times = np.arange(0, n_frames) * hop_length / sr\n    return times"}
{"example_id": "294", "output": "import librosa\nimport numpy as np\n\ndef compute_times_like(y: np.ndarray, sr: int, hop_length: int, D: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the times vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        hop_length: The number of samples between successive frames.\n        D: The spectrogram.\n\n    Returns:\n        The computed times vector.\n    \"\"\"\n    n_frames = D.shape[1]\n    times = librosa.frames_to_time(frames=range(n_frames), sr=sr, hop_length=hop_length)\n    return times"}
{"example_id": "295", "output": "import librosa\nimport numpy as np\n\ndef compute_samples_like(y: np.ndarray, sr: int, D: np.ndarray, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the samples vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        D: The spectrogram.\n    \n    Returns:\n        The computed samples vector.\n    \"\"\"\n    n_frames = D.shape[1]\n    samples = librosa.time_to_samples(np.arange(n_frames) * hop_length / sr)\n\n    return samples"}
{"example_id": "296", "output": "import librosa\nimport numpy as np\n\ndef compute_samples_like(y: np.ndarray, sr: int, D: np.ndarray, hop_length: int) -> np.ndarray:\n    \"\"\"\n    Compute the samples vector of a spectrogram.\n\n    Parameters:\n        y: The audio signal.\n        sr: The sampling rate of the audio signal in Hertz.\n        D: The spectrogram.\n    \n    Returns:\n        The computed samples vector.\n    \"\"\"\n    n_fft = 2048 # Assuming a default n_fft value\n    num_frames = D.shape[1]\n    frames = np.arange(num_frames)\n    samples = frames * hop_length + n_fft // 2\n    return samples"}
{"example_id": "297", "output": "import librosa\nimport numpy as np\n\ndef compute_tone(frequency: int, sr: int, length: int) -> np.ndarray:\n    \"\"\"\n    Constructs a pure tone (cosine) signal at a given frequency.\n\n    Parameters:\n        frequency: The frequency of the tone in Hz.\n        sr: The sampling rate of the signal in Hz.\n        length: The length of the signal in samples.\n\n    Returns:\n        np.ndarray: The pure tone signal.\n    \"\"\"\n    time = np.arange(length) / sr\n    tone_signal = np.cos(2 * np.pi * frequency * time)\n    return tone_signal"}
{"example_id": "298", "output": "import librosa\nimport numpy as np\n\ndef compute_tone(frequency: int, sr: int, length: int) -> np.ndarray:\n    \"\"\"\n    Constructs a pure tone (cosine) signal at a given frequency.\n\n    Parameters:\n        frequency: The frequency of the tone in Hz.\n        sr: The sampling rate of the signal in Hz.\n        length: The length of the signal in samples.\n\n    Returns:\n        np.ndarray: The pure tone signal.\n    \"\"\"\n    return librosa.tone(frequency, sr=sr, length=length)"}
{"example_id": "299", "output": "import librosa\nimport numpy as np\n\ndef compute_chirp(fmin: int, fmax: int, duration: int, sr: int, linear: bool) -> np.ndarray:\n    \"\"\"\n    Constructs a \u201cchirp\u201d or \u201csine-sweep\u201d signal. The chirp sweeps from frequency fmin to fmax (in Hz).\n\n    Parameters:\n        fmin: The minimum frequency of the chirp in Hz.\n        fmax: The maximum frequency of the chirp in Hz.\n        duration: The duration of the chirp in seconds.\n        sr: The sampling rate of the signal in Hz.\n\n    Returns:\n        np.ndarray: The chirp signal.\n    \"\"\"\n    if linear:\n        return librosa.chirp(fmin, fmax, sr=sr, duration=duration, linear=True)\n    return librosa.chirp(fmin, fmax, sr=sr, duration=duration)"}
{"example_id": "300", "output": "import librosa\nimport numpy as np\nfrom scipy.signal import chirp\n\ndef compute_chirp(fmin: int, fmax: int, duration: int, sr: int, linear: bool) -> np.ndarray:\n    \"\"\"\n    Constructs a \u201cchirp\u201d or \u201csine-sweep\u201d signal. The chirp sweeps from frequency fmin to fmax (in Hz).\n\n    Parameters:\n        fmin: The minimum frequency of the chirp in Hz.\n        fmax: The maximum frequency of the chirp in Hz.\n        duration: The duration of the chirp in seconds.\n        sr: The sampling rate of the signal in Hz.\n        linear: Whether to use linear or exponential frequency sweep.\n\n    Returns:\n        np.ndarray: The chirp signal.\n    \"\"\"\n    t = np.linspace(0, duration, int(sr * duration))\n    if linear:\n        return chirp(t, f0=fmin, f1=fmax, t1=duration, method='linear')\n    else:\n        return chirp(t, f0=fmin, f1=fmax, t1=duration, method='logarithmic')"}
{"example_id": "301", "output": "import librosa\nimport numpy as np\n\ndef compute_shear(E: np.ndarray, factor: int, axis: int) -> np.ndarray:\n    return librosa.effects.time_stretch(E, factor)"}
{"example_id": "302", "output": "import librosa\nimport numpy as np\n\ndef compute_shear(E: np.ndarray, factor: int, axis: int) -> np.ndarray:\n    return librosa.util.shear(E, factor=factor, axis=axis)"}
{"example_id": "303", "output": "import librosa\nimport numpy as np\n\ndef compute_localmin(x: np.ndarray, axis: int) -> np.ndarray:\n    return librosa.util.localmin(x, axis=axis)"}
{"example_id": "304", "output": "import librosa\nimport numpy as np\nfrom scipy.signal import argrelmin\n\ndef compute_localmin(x: np.ndarray, axis: int) -> np.ndarray:\n    return argrelmin(x, axis=axis)[0]"}
{"example_id": "305", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Optional\n\ndef compute_yin(sr: int, fmin: int, fmax: int, duration: float, period: float, phi: float, method: str, y: np.ndarray, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], trough_threshold: float) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency (F0) estimation using the YIN algorithm.\n\n    Parameters:\n        sr: The sampling rate of the audio signal in Hertz.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        duration: The duration of the audio signal in seconds.\n        period: The period of the fundamental frequency in seconds.\n        phi: The phase of the fundamental frequency in radians.\n        method: Interpolation method.\n        y: The audio signal.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        trough_threshold: Absolute threshold for peak estimation.\n\n    Returns:\n        The estimated fundamental frequency in Hz.\n    \"\"\"\n    f0 = librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)\n    return f0"}
{"example_id": "306", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Optional\n\ndef compute_yin(sr: int, fmin: int, fmax: int, duration: float, period: float, phi: float, method: str, y: np.ndarray, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], trough_threshold: float) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency (F0) estimation using the YIN algorithm.\n\n    Parameters:\n        sr: The sampling rate of the audio signal in Hertz.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        duration: The duration of the audio signal in seconds.\n        period: The period of the fundamental frequency in seconds.\n        phi: The phase of the fundamental frequency in radians.\n        method: Interpolation method.\n        y: The audio signal.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        trough_threshold: Absolute threshold for peak estimation.\n\n    Returns:\n        The estimated fundamental frequency in Hz.\n    \"\"\"\n    f0 = librosa.yin(y=y, fmin=fmin, fmax=fmax, sr=sr, frame_length=frame_length, center=center, pad_mode=pad_mode, win_length=win_length, hop_length=hop_length, trough_threshold=trough_threshold)\n    return f0"}
{"example_id": "307", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional, Tuple\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_pyin(freq: int, sr: int, y: np.ndarray, fmin: int, fmax: int, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], n_thresholds: int, beta_parameters: Tuple[int], boltzmann_parameter: int, resolution: float, max_transition_rate: float, switch_prob: float, no_trough_prob: float, fill_na: DTypeLike) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency estimation using probabilistic YIN.\n\n    Parameters:\n        freq: The frequency of the fundamental frequency in Hz.\n        sr: The sampling rate of the audio signal in Hertz.\n        y: The audio signal.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        n_thresholds: Number of thresholds.\n        beta_parameters: Beta parameters.\n        boltzmann_parameter: Boltzmann parameter.\n        resolution: Resolution.\n        max_transition_rate: Maximum transition rate.\n        switch_prob: Switch probability.\n        no_trough_prob: No trough probability.\n        fill_na: Fill NA value.\n\n    Returns:\n        Time series of fundamental frequencies in Hertz.\n    \"\"\"\n    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=fmin, fmax=fmax, sr=sr)\n    return f0"}
{"example_id": "308", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional, Tuple\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_pyin(freq: int, sr: int, y: int, fmin: int, fmax: int, frame_length: int, center: bool, pad_mode: str, win_length: Optional[int], hop_length: Optional[int], n_thresholds: int, beta_parameters: Tuple[int], boltzmann_parameter: int, resolution: float, max_transition_rate: float, switch_prob: float, no_trough_prob: float, fill_na: DTypeLike) -> np.ndarray:\n    \"\"\"\n    Calculates the fundamental frequency estimation using probabilistic YIN.\n\n    Parameters:\n        freq: The frequency of the fundamental frequency in Hz.\n        sr: The sampling rate of the audio signal in Hertz.\n        y: The audio signal.\n        fmin: The minimum frequency to consider in Hz.\n        fmax: The maximum frequency to consider in Hz.\n        frame_length: The length of the frame in samples.\n        center: If True, the signal y is padded so that frame t is centered at y[t * hop_length].\n        pad_mode: Padding mode.\n        win_length: Window length.\n        hop_length: Hop length.\n        n_thresholds: Number of thresholds.\n        beta_parameters: Beta parameters.\n        boltzmann_parameter: Boltzmann parameter.\n        resolution: Resolution.\n        max_transition_rate: Maximum transition rate.\n        switch_prob: Switch probability.\n        no_trough_prob: No trough probability.\n        fill_na: Fill NA value.\n\n    Returns:\n        Time series of fundamental frequencies in Hertz.\n    \"\"\"\n    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=fmin, fmax=fmax, sr=sr, frame_length=frame_length, center=center, pad_mode=pad_mode, win_length=win_length, hop_length=hop_length, n_thresholds=n_thresholds, beta_parameters=beta_parameters, boltzmann_parameter=boltzmann_parameter, resolution=resolution, max_transition_rate=max_transition_rate, switch_prob=switch_prob, no_trough_prob=no_trough_prob, fill_na=fill_na)\n    return f0"}
{"example_id": "309", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_vqt(y: np.ndarray, sr: int, hop_length: int, fmin: int, n_bins: int, gamma: int, bins_per_octave: int, tuning: float, filter_scale: int, norm: 1, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike) -> np.ndarray:\n    # librosa.vqt is not available in librosa==0.7.0\n    # Use librosa.cqt with adjusted bins_per_octave for a similar effect\n    C = librosa.cqt(y, sr=sr, hop_length=hop_length, fmin=fmin, n_bins=n_bins, bins_per_octave=bins_per_octave, tuning=tuning, filter_scale=filter_scale, norm=norm, sparsity=sparsity, window=window, scale=scale, pad_mode=pad_mode, res_type=res_type, dtype=dtype)\n    return C"}
{"example_id": "310", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_vqt(y: np.ndarray, sr: int) -> np.ndarray:\n    \"\"\"Compute the variable-Q transform of an audio signal.\n\n    Args:\n        y: Audio time series.\n        sr: Sampling rate of y.\n\n    Returns:\n        Variable-Q transform of y.\n    \"\"\"\n    # Use default parameters for all arguments except y and sr\n    return np.abs(librosa.vqt(y, sr=sr))"}
{"example_id": "311", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim_cqt(y: np.ndarray, sr: int, C, n_iter: int, hop_length: int, fmin: int, bins_per_octave: int, tuning: float, filter_scale: 1, norm: int, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike, length: Optional[int], momentum: float, init: Optional[str]) -> np.ndarray:\n    rng = np.random.RandomState(seed=0)\n    angles = np.exp(2j * np.pi * rng.rand(*C.shape))\n    y_hat = librosa.istft(C * angles, hop_length=hop_length, win_length=int(filter_scale*2**(bins_per_octave/2)) + 1, window=window, center=True, dtype=dtype, length=length)\n    for i in range(n_iter):\n        angles = np.exp(1j * np.angle(librosa.cqt(y=y_hat, sr=sr, hop_length=hop_length, fmin=fmin, n_bins=C.shape[0], bins_per_octave=bins_per_octave, tuning=tuning, filter_scale=filter_scale, norm=norm, sparsity=sparsity, window=window, scale=scale, pad_mode=pad_mode, res_type=res_type)))\n        y_hat = librosa.istft(C * angles, hop_length=hop_length, win_length=int(filter_scale*2**(bins_per_octave/2)) + 1, window=window, center=True, dtype=dtype, length=length)\n\n    return y_hat"}
{"example_id": "312", "output": "import librosa\nimport numpy as np\nimport scipy\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\ndef compute_griffinlim_cqt(y: np.ndarray, sr: int, C, n_iter: int, hop_length: int, fmin: int, bins_per_octave: int, tuning: float, filter_scale: 1, norm: int, sparsity: float, window: str, scale: bool, pad_mode: str, res_type: str, dtype: DTypeLike, length: Optional[int], momentum: float, init: Optional[str]) -> np.ndarray:\n    rng = np.random.RandomState(seed=0)\n    return librosa.griffinlim_cqt(C, sr=sr, hop_length=hop_length, n_iter=n_iter, bins_per_octave=bins_per_octave, momentum=momentum, init=init)"}
{"example_id": "313", "output": "import librosa\nimport numpy as np\nimport scipy\nimport scipy.optimize\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_mel_to_audio(y: np.ndarray, sr: int, S: np.ndarray, M: np.ndarray, n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, pad_mode: str, power: float, n_iter: int, length: Optional[int], dtype: DTypeLike) -> np.ndarray:\n    np.random.seed(seed=0)\n    n_mels = M.shape[0]\n    # Invert mel spectrogram to power spectrogram\n    inv_S = librosa.feature.inverse_mel(M, sr=sr, n_fft=n_fft)\n\n    # Invert power spectrogram to waveform using Griffin-Lim\n    y_inv = librosa.core.griffinlim(inv_S, n_iter=n_iter, hop_length=hop_length, win_length=win_length, window=window, center=center, pad_mode=pad_mode, power=power, length=length, dtype=dtype)\n\n    return y_inv"}
{"example_id": "314", "output": "import librosa\nimport numpy as np\nimport scipy\nimport scipy.optimize\nfrom typing import Union, Optional\n\nDTypeLike = Union[np.dtype, type]\n\n\ndef compute_mel_to_audio(y: np.ndarray, sr: int, S: np.ndarray, M: np.ndarray, n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: str, center: bool, pad_mode: str, power: float, n_iter: int, length: Optional[int], dtype: DTypeLike) -> np.ndarray:\n    np.random.seed(seed=0)\n    n_mel = M.shape[0]\n    # Invert mel basis\n    Minv = np.linalg.pinv(M)\n\n    # Reconstruct magnitude spectrogram\n    S_reconstructed = Minv @ S\n\n    # Invert the power\n    S_reconstructed = np.power(S_reconstructed, 1.0 / power)\n\n    # Convert to complex\n    S_complex = librosa.core.stft_to_audio(S_reconstructed, n_fft=n_fft, hop_length=hop_length)\n\n    # Apply Griffin Lim\n    audio = librosa.griffinlim(np.abs(S_complex), hop_length=hop_length, n_iter=n_iter)\n\n    if length is not None:\n        audio = librosa.util.fix_length(audio, size=length)\n\n    return audio"}
{"example_id": "315", "output": "import librosa\nimport numpy as np\nimport scipy\n\ndef compute_mfcc_to_mel(mfcc: np.ndarray, n_mels: int=128, dct_type: int=2, norm: str='ortho', ref: float=1.0) -> np.ndarray:\n    \"\"\"\n    Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.\n\n    Parameters:\n        mfcc (np.ndarray): Mel-frequency cepstral coefficients.\n        n_mels (int): Number of Mel bands to generate.\n        dct_type (int): Type of DCT to use.\n        norm (str): Normalization to use.\n        ref: Reference power for (inverse) decibel calculation\n\n    Returns:\n        An approximate Mel power spectrum recovered from mfcc.        \n    \"\"\"\n    return librosa.feature.inverse.mfcc_to_mel(mfcc, n_mels=n_mels, dct_type=dct_type, norm=norm, ref=ref)"}
{"example_id": "316", "output": "import librosa\nimport numpy as np\nimport scipy\n\ndef compute_mfcc_to_mel(mfcc: np.ndarray, n_mels: int=128, dct_type: int=2, norm: str='ortho', ref: float=1.0) -> np.ndarray:\n    \"\"\"\n    Invert Mel-frequency cepstral coefficients to approximate a Mel power spectrogram.\n\n    Parameters:\n        mfcc (np.ndarray): Mel-frequency cepstral coefficients.\n        n_mels (int): Number of Mel bands to generate.\n        dct_type (int): Type of DCT to use.\n        norm (str): Normalization to use.\n        ref: Reference power for (inverse) decibel calculation\n\n    Returns:\n        An approximate Mel power spectrum recovered from mfcc.        \n    \"\"\"    \n    n_mfcc = mfcc.shape[-2]  # Number of MFCC coefficients provided\n\n    # Pad or truncate MFCCs to match n_mels (required for DCT inversion)\n    mfcc_full = np.pad(mfcc, ((0, 0), (0, n_mels - n_mfcc)), mode='constant')\n\n    # Inverse DCT (type 2 orthonormal by default)\n    log_mel = librosa.core.dct(\n        x=mfcc_full,\n        type=dct_type,\n        norm=norm,\n        axis=-2\n    )\n    mel_power = librosa.db_to_power(log_mel, ref=ref)\n    return mel_power"}
{"example_id": "317", "output": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    img1 = img1.convert(\"RGBA\")\n    img2 = img2.convert(\"RGBA\")\n    img2 = img2.resize(img1.size)\n    img = Image.alpha_composite(img1, img2)\n    return img"}
{"example_id": "318", "output": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    if img1.mode != img2.mode:\n        img1 = img1.convert(img2.mode)\n    if img1.size != img2.size:\n        img1 = img1.resize(img2.size)\n    result = ImageChops.soft_light(img1, img2)\n    return result"}
{"example_id": "319", "output": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    img1 = img1.convert('RGBA')\n    img2 = img2.convert('RGBA')\n\n    img2 = img2.resize(img1.size)\n\n    img1_pixels = img1.load()\n    img2_pixels = img2.load()\n    result = Image.new('RGBA', img1.size)\n    result_pixels = result.load()\n\n    for x in range(img1.width):\n        for y in range(img1.height):\n            r1, g1, b1, a1 = img1_pixels[x, y]\n            r2, g2, b2, a2 = img2_pixels[x, y]\n\n            r1, g1, b1 = r1/255, g1/255, b1/255\n            r2, g2, b2 = r2/255, g2/255, b2/255\n\n            r = 2 * r2 * r1 if r2 <= 0.5 else 1 - 2 * (1 - r2) * (1 - r1)\n            g = 2 * g2 * g1 if g2 <= 0.5 else 1 - 2 * (1 - g2) * (1 - g1)\n            b = 2 * b2 * b1 if b2 <= 0.5 else 1 - 2 * (1 - b2) * (1 - b1)\n\n            r, g, b = int(r * 255), int(g * 255), int(b * 255)\n            a = min(a1, a2)\n            result_pixels[x, y] = (r, g, b, a)\n\n    return result"}
{"example_id": "320", "output": "import numpy as np\nfrom PIL import Image, ImageChops\n\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    img1 = img1.convert(\"RGBA\")\n    img2 = img2.convert(\"RGBA\")\n    img1.paste(img2, (0, 0), img2)\n    return img1"}
{"example_id": "321", "output": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    result = ImageChops.soft_light(img1, img2)\n    return result"}
{"example_id": "322", "output": "import numpy as np\nfrom PIL import Image, ImageChops\n\ndef imaging(img1: Image, img2: Image) -> Image:\n    result = ImageChops.hard_light(img1, img2)\n    return result"}
{"example_id": "323", "output": "from tqdm import tqdm\n\ndef infinite():\n    i = 0\n    while True:\n        yield i\n        i += 1\n        if i == 1000:\n          return\n\n# Define the total in sol_dict['total'] and use it.\nsol_dict = {{\"total\":0}}\n\nwith tqdm(total=None) as pbar:\n    for i in infinite():\n        pbar.update(1)"}
{"example_id": "324", "output": "from tqdm import tqdm\n\ndef infinite():\n    i = 0\n    while True:\n        yield i\n        i += 1\n        if i == 1000:\n          return\n\n# Define the total in sol_dict['total'] and use it.\nsol_dict = {{\"total\":1000}}\nfor i in tqdm(infinite(), total=sol_dict['total']):\n    pass"}
{"example_id": "325", "output": "import kymatio\nimport torch\nfrom kymatio import Scattering2D\nfrom kymatio.scattering2d.frontend.torch_frontend import ScatteringTorch2D\nfrom typing import Tuple\n\ndef compute_scattering(a: torch.Tensor) -> Tuple[torch.Tensor, ScatteringTorch2D]:\n    J = 2\n    L = 8\n    shape = a.shape[-2:]\n    scattering = ScatteringTorch2D(J=J, shape=shape, L=L)\n    return scattering(a), scattering"}
{"example_id": "326", "output": "import matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n    ax.tick_params(bottom=False, top=False, left=False, right=False)"}
{"example_id": "327", "output": "import matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n    ax.tick_params(\n        axis='both',\n        which='both',\n        bottom=False,\n        top=False,\n        left=False,\n        right=False,\n        labelbottom=False,\n        labeltop=False,\n        labelleft=False,\n        labelright=False\n    )"}
{"example_id": "328", "output": "import matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes import Axes\n\ndef modify(fig: Figure, ax: Axes) -> None:\n    ax.tick_params(axis='both', which='both', labelbottom=False, labelleft=False, bottom=False, left=False)"}
{"example_id": "329", "output": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef use_seaborn() -> None:\n    sns.set_theme()\n\n    # Example data\n    import pandas as pd\n    import numpy as np\n\n    df = pd.DataFrame({\n        'x_axis': range(1, 101),\n        'y_axis': np.random.randn(100) * 15 + range(1, 101)\n    })\n\n    plt.plot('x_axis', 'y_axis', data=df, marker='o', color='mediumvioletred')\n    plt.show()"}
{"example_id": "330", "output": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef use_seaborn() -> None:\n    sns.set_theme()\n    plt.plot([0, 1, 2], [2, 5, 3])\n    # plt.show()\n\nuse_seaborn()"}
